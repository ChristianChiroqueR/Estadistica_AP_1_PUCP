[["index.html", "Estadística para el Análisis Político I Sesión 1 Introducción 1.1 Los pilares de la Estadística PRUEBA 1.2 Los objetos: elementos básicos en el R 1.3 Las Estructuras en el R 1.4 Nuestras herramientas: Paquetes y funciones 1.5 Abrir una base de datos 1.6 Manipulación de datos con Tidyverse", " Estadística para el Análisis Político I Docente: Christian Chiroque Ruiz 2023-09-05 Sesión 1 Introducción 1.1 Los pilares de la Estadística PRUEBA 1.1.1 Definición La estadística es una rama de las matemáticas que se enfoca en el análisis, interpretación y presentación de datos. Los estadísticos se centran en la teoría estadística y en el desarrollo de métodos para analizar datos. Los estadísticos también pueden diseñar estudios experimentales para recopilar datos de una manera rigurosa y controlada. En términos generales, la estadística se divide en dos áreas principales: estadística descriptiva y estadística inferencial. La estadística descriptiva se centra en la organización y presentación de datos, mientras que la estadística inferencial se ocupa de la inferencia y la predicción a partir de datos. 1.1.2 Unidad de análisis En estadística, una unidad de análisis se refiere a la entidad o elemento individual que se estudia en una investigación o estudio estadístico. Esta entidad puede ser una persona, un animal, un objeto, una organización, un evento, una muestra, etc. Dependiendo del objetivo del estudio y de la pregunta que se pretenda responder, la unidad de análisis puede variar. Es importante tener en cuenta que la elección de la unidad de análisis puede tener un impacto significativo en los resultados de un estudio estadístico. Por lo tanto, es fundamental definir claramente la unidad de análisis y asegurarse de que sea coherente con los objetivos y la hipótesis de la investigación. 1.1.3 Tipos de variable En estadística, una variable es cualquier característica, propiedad o atributo que puede tomar diferentes valores y que se puede medir o observar en los elementos de una población o muestra. Distinguimos los tipos de variables porque diferentes métodos estadísticos pueden ser aplicados a cada tipo. Variables numéricas: Se expresan en términos numéricos y se pueden medir con precisión. Los valores representan diferentes magnitudes (cantidad) de la variable. Ejemplos: ingreso anual, número de sobrinos, edad y número de años de estudio. También denominadas cuantitativas. Variables categóricas: No se expresan en términos numéricos y no se pueden medir con precisión. Se miden utilizando un conjunto de categorías o etiquetas. Ejemplo: estado civil, departamento de residencia, tipo de música favorita. También denominadas cualitativas. 1.1.4 Escalas de medición Identifica cómo es medida la variable. Para las variables numéricas: Se utiliza la escala de intervalo: Existe una distancia específica entre cada par de valores y es comparable. Ej: Existe la misma distancia entre 1500 y 1000 soles que entre 1000 soles y 500 soles. Para las variables categóricas: Se utiliza: Escala nominal: Si las categorías no están ordenadas (no hay un alto o bajo, o comparación de intensidad). Las distintas categorías son llamadas “niveles”. Escala ordinal: Es un caso particular. No son nominales porque tienen un orden natural y no son numéricas porque usan etiquetas (niveles). Ej: Nivel de satisfacción con el Poder Ejecutivo. 1.2 Los objetos: elementos básicos en el R Vamos a examinar la clase de algunos de los elementos más básicos en R: los números, los caracteres y los elementos lógicos. class(1.5) ## [1] &quot;numeric&quot; # Para escribir valores character siempre entre comillas class(&quot;rojo&quot;) ## [1] &quot;character&quot; # Para escribir valores booleanos siempre usar mayúscula. class(TRUE) ## [1] &quot;logical&quot; En R, los datos pueden ser coercionados, es decir, forzados, para transformarlos de un tipo a otro. as.numeric(&quot;5&quot;) ## [1] 5 as.integer(5.1) ## [1] 5 as.character(5) ## [1] &quot;5&quot; Podemos asignarles etiquetas (nombres) a esos elementos. x &lt;- 5.5 class(x) ## [1] &quot;numeric&quot; y &lt;- &quot;perro&quot; class(y) ## [1] &quot;character&quot; z &lt;- TRUE class(z) ## [1] &quot;logical&quot; Considerar que también se puede usar el signo “=”. Sin embargo, tiene algunas diferencias en cuanto a su uso en el programa. Por ejemplo, uno puede escribir esta sentencia X &lt;-5+5 y 5+5-&gt;X. Sin embargo, el sistema no acepta lo siguiente: 5+5 = X 1.3 Las Estructuras en el R 1.3.1 Los vectores Un vector es una colección de uno o más datos del mismo tipo. Tipo. Un vector tiene el mismo tipo que los datos que contiene. Si tenemos un vector que contiene datos de tipo numérico, el vector será también de tipo numérico. Los vectores son atómicos, pues sólo pueden contener datos de un sólo tipo, no es posible mezclar datos de tipos diferentes dentro de ellos. Largo. Es el número de elementos que contiene un vector. El largo es la única dimensión que tiene esta estructura de datos. NO TIENE DIMENSIÓN (dim) Ejemplo: Vamos a crear tres vectores: uno numérico, uno de caracter y uno lógico. Podemos utilizar la función length() para medir el largo de estos (cuántos elementos contiene). vector_numerico &lt;- c(1, 2, 3, 4, 5) length(vector_numerico) ## [1] 5 vector_caracter &lt;- c(&quot;arbol&quot;, &quot;casa&quot;, &quot;persona&quot;) length(vector_caracter) ## [1] 3 vector_logico&lt;- c(TRUE, TRUE, FALSE, FALSE, TRUE) length(vector_logico) ## [1] 5 También podemos utilizar la función class() para corroborar que cada vector tiene la misma clase de los elementos que contiene. class(vector_numerico) ## [1] &quot;numeric&quot; class(vector_caracter) ## [1] &quot;character&quot; class(vector_logico) ## [1] &quot;logical&quot; Tener en cuenta que los vectores también pueden tener valores perdidos (NA). vector_con_NA &lt;- c(1,2,3,NA,5) length(vector_con_NA) ## [1] 5 class(vector_con_NA) ## [1] &quot;numeric&quot; 1.3.2 Un vector especial: los factores Un factor es un tipo de datos específico en R. Puede ser descrito como un dato numérico representado por una etiqueta. Supongamos que tenemos un conjunto de datos que representan el género de personas encuestadas por teléfono, pero estos se encuentran capturados con los números 1 y 2. genero &lt;- c(1,1,1,2,2,1,2) El número 1 corresponde a Mujer y el 2 a Hombre. A diferencia del carácter, el factor tiene NIVELES (levels). Podemos crear un vector de tipo factor con la función factor(). genero_en_factor=factor(genero, levels= 1:2, labels=c(&quot;Mujer&quot;, &quot;Hombre&quot;)) genero_en_factor ## [1] Mujer Mujer Mujer Hombre ## [5] Hombre Mujer Hombre ## Levels: Mujer Hombre En la práctica, muchas veces vamos a ver las variables de tipo factor en nuestro análisis. Por ello, debes ser muy cuidadoso en la preparación previa que debes realizar a la base de datos antes de aplicar las funciones. Asimismo, un factor puede estar ordenado o no ordenado. Esto nos sirve, por ejemplo, para crear variables de tipo ordinal. Podemos indicarlo ello, con el argumento ordered=. Veamos: confianza=c(1, 1, 3, 2) confianza_en_factor=factor(confianza, levels= 1:3, labels=c(&quot;Bajo&quot;, &quot;Medio&quot;, &quot;Alto&quot;), ordered = TRUE) confianza_en_factor ## [1] Bajo Bajo Alto Medio ## 3 Levels: Bajo &lt; ... &lt; Alto Vemos que nos indica los niveles, pero en este caso están ordenados de menor a mayor. 1.3.3 Data frames Los data frames son estructuras de datos de dos dimensiones (rectangulares) que pueden contener datos de diferentes tipos, por lo tanto, son heterogéneas. Compuesto por vectores. Estructura más usada para ciencia de datos. Mientras que en una matriz todas las celdas deben contener datos del mismo tipo, los renglones de un data frame admiten datos de distintos tipos, pero sus columnas conservan la restricción de contener datos de un sólo tipo. En términos generales, los renglones en un data frame representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables. mi_df &lt;- data.frame( &quot;variable1&quot; = 1:3, &quot;variable2&quot; = c(1.2, 3.4, 4.5), &quot;variable3&quot; = as.character(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)), &quot;variable4&quot; = as.factor(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)) ) #Para crear un DT los vectores de insumo deben ser del mismo largo mi_df ## variable1 variable2 variable3 ## 1 1 1.2 a ## 2 2 3.4 b ## 3 3 4.5 c ## variable4 ## 1 1 ## 2 2 ## 3 3 str(mi_df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ variable1: int 1 2 3 ## $ variable2: num 1.2 3.4 4.5 ## $ variable3: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; ## $ variable4: Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 2 3 1.3.4 Propiedades Un data.frame tiene: Dimensión: un número de filas y un número de columnas. dim(mi_df) #FILAS Y COLUMNAS ## [1] 3 4 Largo: número de casos length(mi_df) ## [1] 4 Nombre de columnas: Podemos consultar el nombre de las columnas (variables) con la función names(). names(mi_df) ## [1] &quot;variable1&quot; &quot;variable2&quot; ## [3] &quot;variable3&quot; &quot;variable4&quot; 1.3.5 Índices Usar índices para obtener subconjuntos es el procedimiento más universal en R, pues funciona para todas las estructuras de datos. Un índice en R representa una posición. Cuando usamos índices le pedimos a R que extraiga de una estructura los datos que se encuentran en una o varias posiciones específicas dentro de ella. Ejemplos: Seleccionar la columna 2: mi_df [,2] ## [1] 1.2 3.4 4.5 Para seleccionar una columna, también podemos usar el símbolo de $. Es bastante usado en varias funciones. mi_df$variable2 ## [1] 1.2 3.4 4.5 Seleccionar el caso (fila) 2: mi_df [2,] ## variable1 variable2 variable3 ## 2 2 3.4 b ## variable4 ## 2 2 Seleccionar el elemento que se encuentra en la fila 2 y la columna 2: mi_df [2,2] ## [1] 3.4 1.4 Nuestras herramientas: Paquetes y funciones 1.4.1 Paquetes En R, un paquete es un conjunto de herramientas y funciones predefinidas que permiten a los usuarios realizar tareas específicas, como análisis de datos o visualización de gráficos. Los paquetes pueden ser instalados desde los repositorios de CRAN u otros lugares (como repositorios). Para instalar un paquete necesitas escribir install.packages(\"nombre_del_paquete\"). Luego de instalarlo, para comenzar a utilizarlo debes abrirlo con el siguiente comando library(nombre_del_paquete). 1.4.2 Funciones Las funciones son bloques de código que realizan una tarea específica. Pueden ser definidas por el usuario o pueden ser proporcionadas por un paquete (esto es lo más común). Las funciones toman argumentos, que son valores que se pasan a la función para que los utilice en su tarea. Los argumentos de una función son variables o valores que se pasan a la función para que sean utilizados en la tarea que se está realizando. Algunos argumentos son obligatorios, lo que significa que deben ser proporcionados para que la función pueda realizar su tarea, mientras que otros son opcionales y tienen un valor predeterminado si no se especifican. Para ver qué argumentos tiene una función puedes entrar a la documentación de la misma. Por ejemplo, el paquete “dplyr” es un conjunto de herramientas que se utiliza para manipular y transformar datos en R. Una de las funciones de “dplyr” es “filter”, que se utiliza para filtrar filas en un conjunto de datos. Un argumento obligatorio para la función “filter” es el conjunto de datos que se va a filtrar, mientras que un argumento opcional es la condición que se utilizará para filtrar los datos. 1.5 Abrir una base de datos En la práctica tenemos el reto de manipular bases de datos que se encuentran en distintos tipos de archivo. Algunas veces se encuentran en formato .xlsx (Excel regular), pero otras veces las encontramos en formato .csv (separado con comas o puntos y comas), .sav (archivos desde el SPSS), entre otros. Para ello, tenemos dos opciones. La más sencilla es utilizar el paquete rio y utilizamos la función import(). Este paquete es una navaja suiza porque te permite abrir distintos formatos con la misma función. library(rio) ## The following rio suggested packages are not installed: &#39;arrow&#39;, &#39;feather&#39;, &#39;fst&#39;, &#39;hexView&#39;, &#39;pzfx&#39;, &#39;readODS&#39;, &#39;rmatio&#39; ## Use &#39;install_formats()&#39; to install them elecciones_2011&lt;-import(&quot;RESULTADOS_EG_PRESIDENCIAL_2011.xls&quot;) head(elecciones_2011,3) ## UBIGEO DEPARTAMENTO PROVINCIA ## 1 010101 AMAZONAS CHACHAPOYAS ## 2 010101 AMAZONAS CHACHAPOYAS ## 3 010101 AMAZONAS CHACHAPOYAS ## DISTRITO ## 1 CHACHAPOYAS ## 2 CHACHAPOYAS ## 3 CHACHAPOYAS ## ORGANIZACIÓN_POLITICA ## 1 FUERZA NACIONAL ## 2 PARTIDO POLITICO ADELANTE ## 3 DESPERTAR NACIONAL ## VOTOS_VALIDOS_OP VOTOS_VALIDOS ## 1 6 11148 ## 2 24 11148 ## 3 35 11148 ## VOTOS_BLANCO VOTOS_NULOS ## 1 1034 340 ## 2 1034 340 ## 3 1034 340 ## NUMERO_ELECTORES VOTOS_EMITIDOS ## 1 15748 12522 ## 2 15748 12522 ## 3 15748 12522 O también podemos utilizar el paquete readxl() que pertenece al universo de Tidyverse. Lo bueno de esta función es que cuando visualizas la data se realiza de forma más ordenada. library(readxl) elecciones_2011&lt;-read_excel(&quot;RESULTADOS_EG_PRESIDENCIAL_2011.xls&quot;) head(elecciones_2011,3) ## # A tibble: 3 × 11 ## UBIGEO DEPARTAMENTO PROVINCIA ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 010101 AMAZONAS CHACHAPOYAS ## 2 010101 AMAZONAS CHACHAPOYAS ## 3 010101 AMAZONAS CHACHAPOYAS ## # ℹ 8 more variables: ## # DISTRITO &lt;chr&gt;, ## # ORGANIZACIÓN_POLITICA &lt;chr&gt;, ## # VOTOS_VALIDOS_OP &lt;dbl&gt;, ## # VOTOS_VALIDOS &lt;dbl&gt;, ## # VOTOS_BLANCO &lt;dbl&gt;, ## # VOTOS_NULOS &lt;dbl&gt;, … Si deseas utilizar esta segunda forma, puedes aplicar la función read_csv() para archivos separados con comas. 1.6 Manipulación de datos con Tidyverse Por manipulación de datos nos referimos a que, una vez que tengamos nuestro data frame, es usual aplicar algunas funciones para personalizar aún más la data con la que estamos trabajando. Algunas de estas funciones son por ejemplo: Filtrar data: Normalmente no utilizamos toda la data disponible, sino sólo una parte. Para ello podemos filtrar la data con ciertos criterios. Seleccionar columnas: El mismo razonamiento que el punto anterior. Para hacer más ágil la manipulación de un data frame, podemos quedarnos sólo con aquellas columnas que vamos a utilizar. Ordenar según una variable: Hay ocasiones en que necesitamos que nuestra data esté ordenada según una determinada variable. Crear estadísticos de resumen: Lo veremos en la siguiente sesión, pero siempre necesitamos crear estadísticos de resumen, como los relaciones a tendencia central, posición o dispersión. Crear nuevas variable: Esto nos permite generar nuevas variables a partir de las que ya se encuentran en nuestra data. Para todas estas funciones mencionadas se recomienda utilizar el paquete dplyr(). Pueden visualizar un video tutorial aquí: El uso de Tidyverse será transversal a lo largo del curso. "],["manipulación-de-datos-y-estadísticos-descriptivos.html", "Sesión 2 Manipulación de datos y estadísticos descriptivos 2.1 Preparación 2.2 Base de datos 2.3 Estadísticos de tendencia central 2.4 Estadísticos de posición 2.5 Estadísticos de dispersión", " Sesión 2 Manipulación de datos y estadísticos descriptivos 2.1 Preparación 2.1.1 Entorno de trabajo Crea la carpeta donde guardarás toda la información creada. Te recomiendo que para cada clase crees un Proyecto de R. Eso te permitirá mantener un orden. Una vez creado, puedes crear un R Markdown como este para empezar a trabajar. 2.1.2 Paquetes a utilizar Ahora bien, DE PREFERENCIA, siempre que inicies un R Markdown o Script en el R Studio tienes que abrir los paquetes que vas a utilizar. En este caso vamos a utilizar el paquete Tidyverse (que ya incluye muchos otros paquetes ya explicados). Puedes abrirlo con el siguiente comando: # install.packages(&quot;tidyverse&quot;) library(tidyverse) Recuerda que la función library(nombre_del_paquete) sirve para abrir los paquetes YA INSTALADOS. Si te sale un mensaje que dice que “no existe el paquete” debes instalarlo con la función install.packages(línea previa). 2.2 Base de datos 2.2.1 Importación Para la parte I, II y III de esta clase utilizaremos la base de datos de los resultados de las elecciones presidenciales del 2011. Vamos a realizar un breve reporte sobre los porcentajes obtenidos por la organización política “Gana Perú”. library(readxl) elecciones &lt;- read_excel(&quot;RESULTADOS_EG_PRESIDENCIAL_2011.xls&quot;) Luego de abrir una base de datos, siempre te recomiendo darle un primer vistazo. Puedes ver las primeras diez filas con: head(elecciones, 10) ## # A tibble: 10 × 11 ## UBIGEO DEPARTAMENTO PROVINCIA ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 010101 AMAZONAS CHACHAPOYAS ## 2 010101 AMAZONAS CHACHAPOYAS ## 3 010101 AMAZONAS CHACHAPOYAS ## 4 010101 AMAZONAS CHACHAPOYAS ## 5 010101 AMAZONAS CHACHAPOYAS ## 6 010101 AMAZONAS CHACHAPOYAS ## 7 010101 AMAZONAS CHACHAPOYAS ## 8 010101 AMAZONAS CHACHAPOYAS ## 9 010101 AMAZONAS CHACHAPOYAS ## 10 010101 AMAZONAS CHACHAPOYAS ## # ℹ 8 more variables: ## # DISTRITO &lt;chr&gt;, ## # ORGANIZACIÓN_POLITICA &lt;chr&gt;, ## # VOTOS_VALIDOS_OP &lt;dbl&gt;, ## # VOTOS_VALIDOS &lt;dbl&gt;, ## # VOTOS_BLANCO &lt;dbl&gt;, ## # VOTOS_NULOS &lt;dbl&gt;, … También puedes hacerle click en Environment o colocar: #View(elecciones) 2.2.2 Verificar el tipo de variable Veamos las variables, podemos utilizar la función names(nombre_de_data) names(elecciones) ## [1] &quot;UBIGEO&quot; ## [2] &quot;DEPARTAMENTO&quot; ## [3] &quot;PROVINCIA&quot; ## [4] &quot;DISTRITO&quot; ## [5] &quot;ORGANIZACIÓN_POLITICA&quot; ## [6] &quot;VOTOS_VALIDOS_OP&quot; ## [7] &quot;VOTOS_VALIDOS&quot; ## [8] &quot;VOTOS_BLANCO&quot; ## [9] &quot;VOTOS_NULOS&quot; ## [10] &quot;NUMERO_ELECTORES&quot; ## [11] &quot;VOTOS_EMITIDOS&quot; Puedes editar los nombres si deseas. En este caso vamos a cambiar el nombre de la variable ORGANIZACIÓN_POLITICA por ORG_POL. elecciones&lt;- elecciones %&gt;% rename(ORG_POL=ORGANIZACIÓN_POLITICA) Observación: Ten en cuenta que aquí estamos sobre escribiendo. Veamos si el formato es adecuado. str(elecciones) ## tibble [23,958 × 11] (S3: tbl_df/tbl/data.frame) ## $ UBIGEO : chr [1:23958] &quot;010101&quot; &quot;010101&quot; &quot;010101&quot; &quot;010101&quot; ... ## $ DEPARTAMENTO : chr [1:23958] &quot;AMAZONAS&quot; &quot;AMAZONAS&quot; &quot;AMAZONAS&quot; &quot;AMAZONAS&quot; ... ## $ PROVINCIA : chr [1:23958] &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; ... ## $ DISTRITO : chr [1:23958] &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; ... ## $ ORG_POL : chr [1:23958] &quot;FUERZA NACIONAL&quot; &quot;PARTIDO POLITICO ADELANTE&quot; &quot;DESPERTAR NACIONAL&quot; &quot;GANA PERU&quot; ... ## $ VOTOS_VALIDOS_OP: num [1:23958] 6 24 35 2971 7 ... ## $ VOTOS_VALIDOS : num [1:23958] 11148 11148 11148 11148 11148 ... ## $ VOTOS_BLANCO : num [1:23958] 1034 1034 1034 1034 1034 ... ## $ VOTOS_NULOS : num [1:23958] 340 340 340 340 340 340 340 340 340 340 ... ## $ NUMERO_ELECTORES: num [1:23958] 15748 15748 15748 15748 15748 ... ## $ VOTOS_EMITIDOS : num [1:23958] 12522 12522 12522 12522 12522 ... Ok, los caracteres están como chr y las variables numéricas como num. 2.2.3 Creación de subset Aquí nos debemos preguntar: a lo largo de nuestro trabajo, vamos a utilizar toda la data o sólo una parte? En este caso puntual, sólo vamos a utilizar aquellos resultados de la organización política Gana Perú. Por ello, vamos a hacer un filtro con ese criterio y vamos a **crear un subset o una data ya filtrada”. A esta data la llamaremos elecciones_GN. elecciones_GN &lt;- elecciones%&gt;% filter(ORG_POL==&quot;GANA PERU&quot;) 2.3 Estadísticos de tendencia central Como nuestro objetivo es explorar el porcentaje de votos válidos obtenidos por Gana Perú, vamos a crear esa variable en base a dos variable observables: el número de votos obtenidos y el número total de votos válidos. Utilizamos la función mutate(). elecciones_GN&lt;-elecciones_GN %&gt;% mutate(porc.voto=VOTOS_VALIDOS_OP/VOTOS_VALIDOS*100) Ahora podemos calculas los tres estadísticos de tendencia central con la función summarise(). elecciones_GN%&gt;% summarise(Media=mean(porc.voto, na.rm = TRUE), Mediana=median(porc.voto, na.rm = TRUE)) ## # A tibble: 1 × 2 ## Media Mediana ## &lt;dbl&gt; &lt;dbl&gt; ## 1 38.9 35.1 # Recuerda que colocamos na.rm = TRUE para que haga el cálculo omitiendo # aquellas celdas vacías (valores perdidos) 2.4 Estadísticos de posición 2.4.1 Percentiles Los estadísticos de posición son medidas que indican la posición relativa de un valor dentro de un conjunto de datos ordenados de menor a mayor. Los estadísticos de posición más utilizados son los percentiles (y sus derivados que son los cuartiles y deciles). Los estadísticos de posición son útiles para resumir los datos y obtener información sobre la distribución de los mismos. Se utilizan para describir la dispersión de los datos alrededor de la mediana, así como identificar valores atípicos o extremos en el conjunto de datos. Para calcular un determinado percentil podemos solicitarlo utilizando la función summarise() junto con quantile(). Vamos a solicitar los cuartiles (el percentil 25, 50 y 75) de la variable porc.voto: elecciones_GN %&gt;% summarise(P25=quantile(porc.voto, 0.25, na.rm=TRUE), P50=quantile(porc.voto, 0.50, na.rm=TRUE), P75=quantile(porc.voto, 0.75, na.rm=TRUE)) ## # A tibble: 1 × 3 ## P25 P50 P75 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.6 35.1 56.0 Si son muchos percentiles también puedes usar la función concatenar c(), como en el siguiente ejemplo: elecciones_GN %&gt;% summarise(quantile(porc.voto, c(0.10, 0.25,0.50, 0.75, 0.90), na.rm=TRUE)) ## Warning: Returning more (or less) than 1 row ## per `summarise()` group was ## deprecated in dplyr 1.1.0. ## ℹ Please use `reframe()` instead. ## ℹ When switching from `summarise()` ## to `reframe()`, remember that ## `reframe()` always returns an ## ungrouped data frame and adjust ## accordingly. ## Call ## `lifecycle::last_lifecycle_warnings()` ## to see where this warning was ## generated. ## # A tibble: 5 × 1 ## quantile(porc.voto, c(0.1, 0.25,…¹ ## &lt;dbl&gt; ## 1 11.4 ## 2 21.6 ## 3 35.1 ## 4 56.0 ## 5 70.7 ## # ℹ abbreviated name: ## # ¹​`quantile(porc.voto, c(0.1, 0.25, 0.5, 0.75, 0.9), na.rm = TRUE)` 2.5 Estadísticos de dispersión Las medidas de dispersión son una serie de estadísticos que nos permiten conocer la variabilidad o amplitud de los datos en un conjunto de observaciones. Estas medidas nos indican cuánto se alejan los valores de la media, la mediana o cualquier otro estadístico de tendencia central, lo que es de gran utilidad para entender la homogeneidad o heterogeneidad de la distribución de los datos. Las medidas de dispersión más comunes son el rango, la varianza, la desviación estándar, el rango intercuartílico y el coeficiente de variación. 2.5.1 El Rango El rango es una medida de dispersión que se utiliza para determinar la amplitud total de un conjunto de datos. Es la diferencia entre el valor máximo y el valor mínimo en un conjunto de datos. Es una medida muy sencilla de calcular y proporciona una idea general de la variabilidad de los datos. Sin embargo, no toma en cuenta la distribución de los datos en el conjunto, lo que puede hacer que la medida sea menos informativa en ciertos casos. elecciones_GN %&gt;% filter(ORG_POL==&quot;GANA PERU&quot;) %&gt;% summarise(Rango=max(porc.voto, na.rm=TRUE)-min(porc.voto, na.rm=TRUE)) ## # A tibble: 1 × 1 ## Rango ## &lt;dbl&gt; ## 1 98.1 En este caso vemos que los valores de la variable porc.voto tiene un rango de 98.1 unidades (porcentuales) . 2.5.2 La Varianza La varianza es una medida de dispersión que indica qué tan dispersos están los datos con respecto a su media. \\[\\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{n}\\] Matemáticamente, la varianza se define como la media aritmética de los cuadrados de las diferencias entre cada valor de la variable y la media. Lo calculamos utilizando la función var(): elecciones_GN %&gt;% summarise(Varianza=var(porc.voto, na.rm=TRUE)) ## # A tibble: 1 × 1 ## Varianza ## &lt;dbl&gt; ## 1 472. En este ejemplo, el porcentaje de los Votos obtenidos por Gana Perú tiene una dispersión de 472 entre los distritos del Perú. Recuerda que la varianza puede servir a la hora de comparar dos variable que están medidas en las mismas unidades (como si comparáramos este porcentaje vs el obtenido por Fuerza 2011). Si deseamos analizar una única variable es más fácil ver la Desviación Estándar. 2.5.3 La Desviación Estándar Es la medida de dispersión más común y se calcula como la raíz cuadrada de la varianza. Nos indica cuánto se desvían los valores de la distribución con respecto a la media. \\[\\sqrt{\\text{Var}(X)}\\] La desviación estándar se expresa en las mismas unidades que los datos originales, lo que la hace fácilmente interpretable. Una desviación estándar pequeña indica que los datos están muy concentrados alrededor de la media, mientras que una desviación estándar grande indica que los datos están más dispersos y que existe una mayor variabilidad en los mismos. Lo calculamos usando la función sd(): elecciones_GN %&gt;% summarise(Desv_est=sd(porc.voto, na.rm=TRUE)) ## # A tibble: 1 × 1 ## Desv_est ## &lt;dbl&gt; ## 1 21.7 La forma de interpretación es que los valores observados están en promedio a una distancia de 21 unidades de la media. 2.5.4 El Rango intercuartílico El rango intercuartil (IQR por sus siglas en inglés) es una medida de dispersión utilizada en estadística que se define como la diferencia entre el tercer y primer cuartil de un conjunto de datos. En otras palabras, el IQR mide la distancia entre el 25% y el 75% de los datos ordenados de menor a mayor. El IQR es una medida de dispersión robusta que no se ve afectada por valores atípicos o extremos en los datos, a diferencia del rango o la desviación estándar. Es útil para resumir la variabilidad de los datos en un intervalo de valores que contiene la mayor parte de los datos y que no está influenciado por los valores extremos. El IQR se puede utilizar para identificar valores atípicos o para comparar la variabilidad entre diferentes conjuntos de datos. Lo calculamos utilizando la función IQR(): elecciones_GN %&gt;% summarise(IQR=IQR(porc.voto, na.rm=TRUE)) ## # A tibble: 1 × 1 ## IQR ## &lt;dbl&gt; ## 1 34.4 En este caso vemos que el IQR de la variable porc.voto es de 34.4. Es decir, la distancia entre el primer cuartil y el tercer cuartil es de 34 unidades en la escala de la variable (porcentaje). 2.5.5 Curvas de distribución de frecuencias Las curvas de distribución son representaciones gráficas de la distribución de una variable aleatoria. Estas curvas se utilizan para mostrar cómo se distribuyen los valores de una variable en un conjunto de datos. El principal objetivo examinar las posiciones relativas de la media, la mediana y la moda, para estimar la FORMA DE UNA DISTRIBUCIÓN DE FRECUENCIAS. Tenemos dos grandes opciones: Distribución normal: Curva de distribución de frecuencias donde la media, la mediana y la moda de una variable son iguales entre sí y la distribución de la puntuaciones tiene forma de campana. Distribución sesgada: Curva de distribución de frecuencias en la cual la media, la mediana y la moda de una variable son desiguales y algunos de los sujetos tienen puntuaciones sumamente altas o bajas. Ahora veamos nuestro ejemplo, a qué curva de distribución se asemejará? Vemos que nuestra variable de Porcentaje de votos valídos obtenido por Gana Perú a nivel distrital, posee una distribución con un sesgo positivo. Veamos si es cierto la teoría. Grafiquemos la media (38.90), mediana (35.13) y moda (27.00) en dicha curva de distribución. Corroboramos lo explicitado por la teoría en cuento a la relación entre el sesgo y el posicionamiento de los estadísticos de tendencia central. "],["visualización-de-datos-con-ggplot2.html", "Sesión 3 Visualización de datos con ggplot2 3.1 GGPLOT2: Una herramienta para la visualización 3.2 Código", " Sesión 3 Visualización de datos con ggplot2 3.1 GGPLOT2: Una herramienta para la visualización 3.1.1 Descripción ggplot2 es una biblioteca de visualización de datos en R que se utiliza para crear gráficos de alta calidad y personalizados. Fue desarrollada por Hadley Wickham y se basa en la “Gramática de los gráficos” propuesta por Leland Wilkinson. En ggplot2, los gráficos se crean mediante la combinación de componentes llamados capas. Cada capa representa una parte del gráfico, como los datos, los ejes, la leyenda y la geometría utilizada para representar los datos, como barras, puntos, líneas, etc. ggplot2 también utiliza el concepto de “mapeo estético” para relacionar variables de los datos con propiedades visuales del gráfico, como el color, la forma y el tamaño. Esto permite crear gráficos altamente personalizados que resaltan patrones y tendencias en los datos. En resumen, ggplot2 es una herramienta poderosa y flexible para crear visualizaciones de datos en R, que se basa en una sintaxis intuitiva y coherente que facilita la creación de gráficos complejos y personalizados. 3.1.2 Las siete capas En ggplot2, las visualizaciones se crean mediante la combinación de capas. Cada capa representa una parte del gráfico, como los datos, los ejes, la leyenda y la geometría utilizada para representar los datos, como barras, puntos, líneas, etc. A continuación, se describen brevemente las capas más comunes de ggplot2: Capa de datos (Data): Esta capa representa los datos que se van a visualizar y se proporcionan mediante un objeto de marco de datos o un tibble. Capa de mapeo estético (Aesthetics): Esta capa establece la relación entre las variables de los datos y las propiedades visuales del gráfico. Se utiliza la función aes() para establecer la relación. Por ejemplo, aes(x = variable_x, y = variable_y) establece que la variable x se va a representar en el eje x y la variable y en el eje y. Capa de geometría (Geometries): Esta capa representa la forma en que los datos se van a visualizar. La geometría se especifica utilizando una de las funciones geom_*() disponibles en ggplot2, como geom_point() para crear gráficos de dispersión, geom_line() para crear gráficos de líneas, geom_bar() para crear gráficos de barras, etc. Capa de facetas (Facets): Esta capa permite dividir los datos en subconjuntos y visualizarlos en diferentes paneles. Se utiliza la función facet_*() para crear facetas. Por ejemplo, facet_wrap(~variable_z) divide los datos en diferentes paneles según los valores de la variable z. Capa de transformaciones estadísticas (Statistical transformations): Esta capa permite aplicar transformaciones estadísticas a los datos antes de visualizarlos. Se utiliza la función stat_*() para aplicar transformaciones estadísticas. Por ejemplo, stat_smooth() ajusta una curva suave a los datos. Capa de coordenadas (Coordinates): Esta capa permite cambiar las coordenadas del gráfico. Se utiliza la función coord_*() para cambiar las coordenadas. Por ejemplo, coord_polar() cambia las coordenadas del gráfico a coordenadas polares. Capa de temas (Themes): Esta capa permite cambiar el aspecto visual del gráfico. Se utiliza la función theme_*() para cambiar el tema. Por ejemplo, theme_dark() cambia el tema del gráfico a un tema oscuro. Cada una de estas capas en ggplot2 tiene una función específica que ayuda a crear visualizaciones personalizadas y de alta calidad. La combinación adecuada de estas capas permite crear gráficos que resalten patrones y tendencias en los datos de manera efectiva. 3.1.3 Slow ggplot Es un enfoque utilizado para la creación de gráficos en ggplot. La lógica de esta aproximación es incremental, es decir, teniendo en cuenta que el código requerido para generar un gráfico de ggplot involucra varias capas (o sentencias), se sugiere aprender a utilizar el paquete haciendo énfasis en cada una de las capas y los cambios que genera en el output. Es más lento que crear todo el código de golpe, pero es mucho más claro, tanto para el usuario que recién aprende la librería, como para la explicación a un público no conocedor del software. Inclusive se han creado paquetes específicamente vinculados con este enfoque, como el Flipbookr de Evangeline Reynolds. Aquí puedes revisar un breve post en el que comenta sobre el slow ggplot. Se sugiero revisar este link donde el autor ha diseñado un aplicativo para ver justamente cómo se construye un ggplot de forma incremental. Al lado izquierdo veras un conjunto de decisiones que puede tomar el investigador, usando la data gapminder, y a la derecha los cambios que se genera en el código y, por supuesto, en el output final. También puedes ir directamente al aplicativo presionando este link. 3.2 Código En esta presentación te muestro un resumen sobre este capítulo, presentando el código y los outputs respectivos. Cabe recalcar que el código presentado sigue el enfoque de slow ggplot. "],["estadística-inferencial-introducción.html", "Sesión 4 Estadística Inferencial: Introducción 4.1 Antecedentes relevantes 4.2 Elementos básicos de probabilidad 4.3 Ejercicio con R 4.4 Elementos básicos de la inferencia 4.5 Intervalo de confianza de una media 4.6 Ejercicio con R: ENADES 2022", " Sesión 4 Estadística Inferencial: Introducción 4.1 Antecedentes relevantes La desviación estándar La desviación estándar es una parte integral de la estadística inferencial y entender su funcionamiento nos ayudará a utilizarla más adelante. Recuerda, la desviación estándar es la raíz cuadrada de la sumatoria de todas las diferencias entre los valores observados y su media. \\[\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2}\\] La desviación estándar se llama “estándar” porque proporciona una unidad de medida común para comparar unidades observadas de medida muy diferente. Ejemplo: imaginemos que tenemos la variable ingreso que tiene media 120 y desviación estándar 10 y deseamos analizar el caso X = 130. Forma de puntuación Fórmula Ejemplo Puntuación en bruto \\[X\\] 130 soles Puntuación de desviación \\[X - \\bar{X}\\] 10 soles Puntuación estandarizada Z \\[Z = \\frac{X - \\bar{X}}{S_X}\\] 1 SD Presta atención a las unidades de medida! En el ejemplo anterior podemos corroborar que nosotros podemos medir la lejanía de una observación respecto a su media, no sólo en las unidades originales, sino diciendo cuántas desviaciones estándar la separa de su media. Está última medida se conoce como puntuaciones Z y es muy útil para comparar niveles de dispersión de variables observadas. Algunas reglas: 1) mientras más lejana esté una observación de su media, mayor será su puntuación de desvación y su puntuación Z. 2) El signo de la puntuación Z indica si la observación está por debajo o por encima de la media. La distribución (curva) normal y las SD Anteriormente te comenté que la curva normal (forma de campana) ejercía un rol fundamental en el contexto de la estadística inferencial. Esto es gracias a una propiedades muy interesantes. La principal: cuando una variable tiene distribución de puntuaciones que es normal (casi) la totalidad de observaciones están distribuidas +- 3 desviaciones estándar (puntuaciones Z) respecto de su media. Este principio matemático sienta las bases de la imaginación estadística, dado que muchos fenómenos naturales poseen distribuciones de frecuencias en forma de campana como la curva normal. Como se ve en la figura, son 4 los principios: 1) 50% de las puntuaciones caen encima de la media y 50% debajo; 2) Prácticamente todas las puntuaciones caen dentro de 3 SD a partir de la media en ambas direcciones (en realidad el 99.7%); 3) Cerca del 95% de las puntuaciones de una variable normalmente distribuida caen dentro de una distancia de +- 2 SD respecto de la media; y 4) Alrededor del 68% de las puntuaciones caen dentro de una distancia de +-1 SD respecto de la media. 4.2 Elementos básicos de probabilidad Conceptos Probabilidad: Análisis y comprensión de las ocurrencias (de eventos) por el azar. Más específicamente, es un detalle de con qué frecuencia es probable que ocurra un evento de interés particular entre un gran número de ensayos. La fórmula general para presentar una probabilidad es: \\[P(\\text{éxito}) = \\frac{\\text{# de éxitos}}{\\text{# de ensayos}}\\] Existen diversas reglas básica de la teoría de la probabilidad. Te recomiendo leer las páginas 172 - 176 de Ritchey (2006). Curva normal y probabilidades En probabilidades hay un principio que utiliza el concepto de la curva normal que hemos explicado anteriormente. Dada cierta variable que se distribuye normalmente, podemos calcular puntuaciones estándarizadas (puntuaciones Z) y usarlas de tal forma que calculas el número de puntuaciones que cae entre dos puntos en la distribución. Veamos nuevamente la curva. Ahora, dependiendo las puntuaciones Z que elijamos vamos a hacer un cálculo de qué porcentaje de puntuaciones se encuentran bajo la curva. Dicho de en otras palabras, vamos a examinar el área bajo la curva. En este sentido, el área bajo la curva representa probabilidades de ocurrencia. Veamos el siguiente ejemplo, donde tenemos el promedio de edad en una población de estudio (X=69) con desviación estándar 3 (SD=3). Con este ejemplo, para el intervalo de 66 - 72 podemos afirmar lo siguiente: La proporción de casos entre estos dos puntos es 68%. El área bajo la curva entre estas dos puntuaciones es 68%. La probabilidad de seleccionar al azar un caso entre estas dos puntuaciones es 68%. 4.3 Ejercicio con R Usted tiene la siguiente variable con distribución normal, de media 100 y desviación estándar 20. media &lt;- 100 desviacion_estandar &lt;- 20 variable_aleatoria &lt;- rnorm(n = 1000, mean = media, sd = desviacion_estandar) summary(variable_aleatoria) ## Min. 1st Qu. Median Mean ## 42.67 85.85 99.98 99.76 ## 3rd Qu. Max. ## 114.46 169.82 Visualicemos si tiene la curva normal. library(tidyverse) variable_aleatoria |&gt; data.frame() |&gt; ggplot() + aes(x=variable_aleatoria) + geom_density() Comprobemos la teoría. Pintemos las líneas de -1 SD, -2 SD, -3SD, la media, +1 SD, +2 SD y +3SD. Recuerda que la desviación estándar es 20. Cuánto porcentaje de casos se encuentran entre 80 (-1SD) y 120 (+1SD)? variable_aleatoria |&gt; data.frame() |&gt; filter(variable_aleatoria&gt;=80, variable_aleatoria&lt;=120) |&gt; count() |&gt; mutate(p=n/1000) ## n p ## 1 667 0.667 Y de 60 a 140? variable_aleatoria |&gt; data.frame() |&gt; filter(variable_aleatoria&gt;=60, variable_aleatoria&lt;=140) |&gt; count() |&gt; mutate(p=n/1000) ## n p ## 1 956 0.956 4.4 Elementos básicos de la inferencia 4.4.1 Definiciones preliminares La estadística inferencial utiliza la muestra de datos para hacer estimaciones y tomar decisiones acerca de las características de una población. Esto implica la utilización de técnicas y métodos para inferir información sobre la población a partir de la información recopilada en la muestra. Algunas definiciones básicas: Definición Descripción Población Se refiere al conjunto total de individuos, objetos, eventos, medidas o cualquier otra cosa que se quiera estudiar. En estadística inferencial, la población se utiliza como el objeto de estudio, y se busca inferir información sobre ella a partir de la muestra. Muestra Es un subconjunto de la población que se utiliza para hacer inferencias sobre la población en su conjunto. La selección de la muestra debe hacerse de tal forma que represente de manera adecuada las características de la población. Estadístico Es una medida numérica que se utiliza para resumir o describir alguna característica de la muestra. Los estadísticos se calculan a partir de los datos de la muestra y se utilizan para hacer inferencias sobre los parámetros de la población. Parámetro Es una medida numérica que describe alguna característica de la población. En estadística inferencial, el objetivo es hacer inferencias sobre los parámetros de la población a partir de los datos de la muestra. 4.4.2 Estimación puntual Imaginemos que tenemos una población de 45 individuos. A cada uno se le ha preguntado acerca de cuántas veces han asistido a una manifestación, marcha o evento público en los últimos 3 años. La data es la siguiente: poblacion&lt;- c(1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,2,2,2,2,4,4,4,4,4,4,4,4,5,5,5,5,5,5) length(poblacion) ## [1] 45 Veámoslo gráficamente. poblacion=as.data.frame(poblacion) poblacion |&gt; ggplot()+ aes(x=poblacion) |&gt; geom_bar() Como dato previo sabemos que en esta población la media es de 2.71. Ojo, normalmente nosotros no sabemos cuál es el parámetro poblacional, sólo lo planteamos así con fines didácticos. mean(poblacion$poblacion) ## [1] 2.711111 Ahora vamos a hacer un estudio inferencial a partir de una muestra de 5 personas (n) para calcular la media poblacional (parámetro). Primero hay que preguntarnos, cuántas muestras posibles existen para 50 distintos elementos? No son pocas. n_muestras = choose(50, 5) n_muestras ## [1] 2118760 Ok, elijamos una muestra aleatoria. Utilicemos la tabla de números aleatorios (el antepasado del famoso set.seed() de R). Si no es legible, puedes entrar en este link - Elijamos una muestra de 5 casos según la tabla de números aleatorios, es decir, los casos: 10, 22, 24, 42 y 37. muestra1= poblacion[c(10, 22, 24, 42, 37),] muestra1 ## [1] 2 2 3 5 4 Calculemos la media muestral (estadístico): mean(muestra1) ## [1] 3.2 Según la muestra que hemos calculado, hemos obtenido una media de 3.2. Si nosotros nos quedamos sólo con ese valor entonces estamos eligiendo una estimación puntual. ESTIMACIÓN PUNTUAL: Estadístico proporcionado sin indicar un rango de error. 4.4.3 Error de muestreo Si nosotros hacemos este ejercicio una y otra vez, es decir seleccionamos varias muestras, probablemente obtengamos resultados diferentes. Cada media muestral es ligeramente mayor o menor que la anterior. muestra2= poblacion[c(4, 23, 41, 21, 45),] muestra2 ## [1] 1 2 5 2 5 mean(muestra2) ## [1] 3 Esto quiere decir que los estadísticos muestrales no son los valores exactos de parámetros poblacionales. Son muchas las posibilidades de que obtengas medias muestrales ligeramente diferentes. Aquí los teóricos descubrieron un principio: cuando uno realiza muestreos repetidos y calcula sus respectivos estadísticos, los valores tendían a agruparse alrededor de un valor particular, el cual sería el verdadero parámetro poblacional. 4.4.4 Distribuciones de muestreo. Ahora bien, ya sabemos que podemos solicitar varias muestras. Qué distribución tendrían todas las medias que calculemos en un determinado número de muestras? Calculemos 100 000 muestras para el vector de nuestra población y calculemos la media de cada uno en una tabla. Para ello, le pedimos ayuda al R. poblacion&lt;- c(1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,2,2,2,2,4,4,4,4,4,4,4,4,5,5,5,5,5,5) muestras &lt;- replicate(100000, sample(poblacion, size=5)) medias &lt;- apply(muestras, 2, mean) df &lt;- data.frame(media=medias) Ahora grafiquemos las medias calculadas. df |&gt; ggplot()+ aes(x=medias) + geom_histogram(bins = 10) Descubrimos que cuando obtenemos muchas muestras y calculamos sus estadísticos (medias), estan en conjunto adquieren una distribución normal. Esto ocurre, a pesar de que la distribución de las puntuaciones originales no tenía distribución normal. Una distribución muestral de medias tiene forma normal. Puesto que una distribución muestral de medias adopta una forma normal se puede calcular la probabilidad de ocurrencia de cualquier resultado muestral Una distribución muestral nos indica con qué frecuencia un estadístico muestral tiene la probabilidad de fallar respecto al valor real del parámetro poblacional 4.4.5 Error estándar Recuerdas que habíamos dicho que en la curva normal se podría evidenciar que la totalidad de las observaciones se encontraban entre -3 y +3 desviaciones estándar respecto de la media? En el caso de las distribuciones muestrales, esa desviación estándar es conocida como error estándar. El error estándar mide la dispersión del error de muestreo que ocurre cuando se muestrea repetidamente una población (como lo hicimos líneas arriba). \\[s_{\\hat{x}} = \\frac{s}{\\sqrt{n}}\\] Entonces los puntos más importantes del EE son: El error estándar es una medida de cuánto se espera que varíen las medias de las muestras tomadas de una población determinada. A medida que el tamaño de la muestra aumenta, el error estándar tiende a disminuir. El error estándar es importante en el cálculo de los intervalos de confianza. Cuanto menor sea el error estándar, menor será la variabilidad de las medias muestrales y más preciso será el intervalo de confianza. El error estándar se calcula dividiendo la desviación estándar de la población entre la raíz cuadrada del tamaño de la muestra. En la mayoría de los casos, la desviación estándar de la población no se conoce y se utiliza la desviación estándar de la muestra para estimar el error estándar. 4.4.6 Teorema del límite central El teorema del límite central (TLC) es uno de los conceptos más importantes de la estadística y es fundamental en el muestreo y la inferencia estadística. En términos simples, el teorema del límite central dice que si tomamos suficientes muestras aleatorias grandes de una población, la distribución de las medias de esas muestras será una distribución normal, sin importar cómo se vea la distribución original de la población. Esto es importante en el muestreo porque nos permite hacer inferencias precisas sobre una población, incluso si no conocemos su distribución. Si podemos asumir que la distribución de la población es aproximadamente normal, entonces podemos usar la distribución normal de las medias de las muestras para hacer predicciones y estimaciones precisas sobre la población. Además, el TLC nos permite calcular intervalos de confianza y realizar pruebas de hipótesis (siguiente secciòn) con mayor precisión, lo que nos permite tomar decisiones más informadas basadas en los datos muestrales. En resumen, el teorema del límite central es una herramienta clave en la inferencia estadística y nos permite hacer generalizaciones precisas sobre una población a partir de datos muestrales. Máquina de Galton El Tablero de Galton ilustra cómo la distribución de frecuencias de los resultados de muchos eventos aleatorios independientes se acerca a una distribución normal, independientemente de la forma de la distribución original, siempre que el número de eventos sea lo suficientemente grande. 4.4.7 Ley de los grandes números La ley de los grandes números es un teorema en estadística que establece que, a medida que el tamaño de una muestra aumenta, la media muestral se acerca a la media poblacional. En otras palabras, cuando se toman muestras cada vez más grandes de una población, se espera que la media de esas muestras se acerque cada vez más a la media real de la población. Esta ley es importante porque permite a los investigadores obtener estimaciones precisas de los parámetros de una población a partir de una muestra relativamente pequeña. Además, esta ley también es fundamental para la teoría de la probabilidad y es utilizada en muchas áreas de la estadística y de la ciencia en general. Mientras más grande sea la muestra, la media muestras se acerca a la media poblacional. 4.5 Intervalo de confianza de una media El intervalo de confianza es un rango de valores posibles de un parámetro expresado con un grado específico de confianza. Si tenemos un nivel de confianza de 95% quiere decir que si realizamos 100 veces el mismo procedimiento de muestreo y calculamos los estadísticos de interés, 95 veces nos van a salir resultados en los intervalos calculados. Si lo realizamos con un 99% de confianza, de igual manera, si realizamos 100 veces el procedimiento, 99 veces nos va a salir resultados en el intervalo resultante. Esto lo tenemos claro gracias a la explicación del rol que cumple la curva normal y sus propiedades. A MAYOR CONFIANZA MENOR ES LA PRECISIÓN (LOS INTERVALOS SON MÁS AMPLIOS) Para el cálculo de un intervalo de confianza utilizamos la siguiente fórmula. Recuerda - Ese valor que se suma y se resta a la media muestral es el término de error, sin embargo, es más conocido como margen de error. 4.6 Ejercicio con R: ENADES 2022 El Instituto de Estudios Peruanos, por encargo de Oxfam en Perú, elaboró la I Encuesta Nacional de percepción de Desigualdades – ENADES 2022. Este estudio pone a disposición del público el análisis estadístico más completo a la fecha sobre la percepción de las diferentes formas de desigualdad en el Perú. Además de factores económicos, la presente encuesta incluye indicadores que permiten medir la magnitud de una serie de brechas sociales y políticas: desde diferencias de género, clase y relaciones étnico-raciales, hasta dimensiones subjetivas de la desigualdad y sus vínculos con orientaciones políticas. Como se muestra a lo largo del informe, la base de datos de este proyecto provee herramientas valiosas a expertos de diferentes campos, tanto académicos como profesionales, estudiantes y personas interesadas en el análisis multidimensional de la desigualdad en el país. Puedes abrir el cuestionario de la encuestas aquí También puedes ver el informe aquí 4.6.1 Abrir base de datos library(haven) enades&lt;-read_spss(&quot;data/ENADES_2022.sav&quot;) # Con esta función abrimos archivos de SPSS # enades&lt;-read_spss(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/ENADES_2022.sav&quot;) 4.6.2 Identificar una variable numérica Elijamos la variable P17: En una escala del 1 al 10, en la que 1 es “Totalmente inaceptable” y 10 es “Totalmente aceptable”. ¿Hasta qué punto es aceptable la desigualdad en el Perú? Dígame un número de 1 a 10, recuerde que 1 es “Totalmente inaceptable” y 10 es “Totalmente aceptable (RESPUESTA ESPONTÁNEA) La convertimos en numérica. enades$p17&lt;-as.numeric(enades$p17) Solicitamos los estadísticos descriptivos para darle una primera mirada. summary(enades$p17) ## Min. 1st Qu. Median Mean ## 1.000 1.000 5.000 4.571 ## 3rd Qu. Max. NA&#39;s ## 7.000 10.000 23 Podemos graficarlo enades |&gt; ggplot() + aes(x=p17)+ geom_bar() 4.6.3 Cálculo del estimador puntual: media muestral mean(enades$p17, na.rm = TRUE) ## [1] 4.571334 4.6.4 Cálculo del intervalo de confianza al 95% Establecemos nuestra información de insumo. media&lt;-mean(enades$p17, na.rm = TRUE) SE&lt;- sd(enades$p17, na.rm = TRUE) z&lt;- 1.96 n &lt;-length(enades$p17) Ahora calculamos el error estándar: errorst &lt;- SE/sqrt(n) Una vez calculado el error estándar podemos calcular los límite superior e inferior. ### Calculamos el límite inferior lim_inf&lt;- media - (z*errorst) ### Calculamos el límite superior lim_sup&lt;- media + (z*errorst) ### Creamos el listado de valores para visualizarlo en versión data frame interval_m &lt;- data.frame(n, media, SE, z, errorst, lim_inf,lim_sup) interval_m ## n media SE z ## 1 1530 4.571334 3.133069 1.96 ## errorst lim_inf lim_sup ## 1 0.08009848 4.414341 4.728327 Para verlo mejor podemos utilizar el paquete kableExtra: library(kableExtra) interval_m |&gt; kbl() |&gt; kable_styling() n media SE z errorst lim_inf lim_sup 1530 4.571334 3.133069 1.96 0.0800985 4.414341 4.728327 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
