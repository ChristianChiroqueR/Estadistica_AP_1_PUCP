[["index.html", "Estadística para el Análisis Político I Sesión 1 Introducción 1.1 Los pilares de la Estadística PRUEBA 1.2 Los objetos: elementos básicos en el R 1.3 Las Estructuras en el R 1.4 Nuestras herramientas: Paquetes y funciones 1.5 Abrir una base de datos 1.6 Manipulación de datos con Tidyverse", " Estadística para el Análisis Político I Docente: Christian Chiroque Ruiz 2023-08-31 Sesión 1 Introducción 1.1 Los pilares de la Estadística PRUEBA 1.1.1 Definición La estadística es una rama de las matemáticas que se enfoca en el análisis, interpretación y presentación de datos. Los estadísticos se centran en la teoría estadística y en el desarrollo de métodos para analizar datos. Los estadísticos también pueden diseñar estudios experimentales para recopilar datos de una manera rigurosa y controlada. En términos generales, la estadística se divide en dos áreas principales: estadística descriptiva y estadística inferencial. La estadística descriptiva se centra en la organización y presentación de datos, mientras que la estadística inferencial se ocupa de la inferencia y la predicción a partir de datos. 1.1.2 Unidad de análisis En estadística, una unidad de análisis se refiere a la entidad o elemento individual que se estudia en una investigación o estudio estadístico. Esta entidad puede ser una persona, un animal, un objeto, una organización, un evento, una muestra, etc. Dependiendo del objetivo del estudio y de la pregunta que se pretenda responder, la unidad de análisis puede variar. Es importante tener en cuenta que la elección de la unidad de análisis puede tener un impacto significativo en los resultados de un estudio estadístico. Por lo tanto, es fundamental definir claramente la unidad de análisis y asegurarse de que sea coherente con los objetivos y la hipótesis de la investigación. 1.1.3 Tipos de variable En estadística, una variable es cualquier característica, propiedad o atributo que puede tomar diferentes valores y que se puede medir o observar en los elementos de una población o muestra. Distinguimos los tipos de variables porque diferentes métodos estadísticos pueden ser aplicados a cada tipo. Variables numéricas: Se expresan en términos numéricos y se pueden medir con precisión. Los valores representan diferentes magnitudes (cantidad) de la variable. Ejemplos: ingreso anual, número de sobrinos, edad y número de años de estudio. También denominadas cuantitativas. Variables categóricas: No se expresan en términos numéricos y no se pueden medir con precisión. Se miden utilizando un conjunto de categorías o etiquetas. Ejemplo: estado civil, departamento de residencia, tipo de música favorita. También denominadas cualitativas. 1.1.4 Escalas de medición Identifica cómo es medida la variable. Para las variables numéricas: Se utiliza la escala de intervalo: Existe una distancia específica entre cada par de valores y es comparable. Ej: Existe la misma distancia entre 1500 y 1000 soles que entre 1000 soles y 500 soles. Para las variables categóricas: Se utiliza: Escala nominal: Si las categorías no están ordenadas (no hay un alto o bajo, o comparación de intensidad). Las distintas categorías son llamadas “niveles”. Escala ordinal: Es un caso particular. No son nominales porque tienen un orden natural y no son numéricas porque usan etiquetas (niveles). Ej: Nivel de satisfacción con el Poder Ejecutivo. 1.2 Los objetos: elementos básicos en el R Vamos a examinar la clase de algunos de los elementos más básicos en R: los números, los caracteres y los elementos lógicos. class(1.5) ## [1] &quot;numeric&quot; # Para escribir valores character siempre entre comillas class(&quot;rojo&quot;) ## [1] &quot;character&quot; # Para escribir valores booleanos siempre usar mayúscula. class(TRUE) ## [1] &quot;logical&quot; En R, los datos pueden ser coercionados, es decir, forzados, para transformarlos de un tipo a otro. as.numeric(&quot;5&quot;) ## [1] 5 as.integer(5.1) ## [1] 5 as.character(5) ## [1] &quot;5&quot; Podemos asignarles etiquetas (nombres) a esos elementos. x &lt;- 5.5 class(x) ## [1] &quot;numeric&quot; y &lt;- &quot;perro&quot; class(y) ## [1] &quot;character&quot; z &lt;- TRUE class(z) ## [1] &quot;logical&quot; Considerar que también se puede usar el signo “=”. Sin embargo, tiene algunas diferencias en cuanto a su uso en el programa. Por ejemplo, uno puede escribir esta sentencia X &lt;-5+5 y 5+5-&gt;X. Sin embargo, el sistema no acepta lo siguiente: 5+5 = X 1.3 Las Estructuras en el R 1.3.1 Los vectores Un vector es una colección de uno o más datos del mismo tipo. Tipo. Un vector tiene el mismo tipo que los datos que contiene. Si tenemos un vector que contiene datos de tipo numérico, el vector será también de tipo numérico. Los vectores son atómicos, pues sólo pueden contener datos de un sólo tipo, no es posible mezclar datos de tipos diferentes dentro de ellos. Largo. Es el número de elementos que contiene un vector. El largo es la única dimensión que tiene esta estructura de datos. NO TIENE DIMENSIÓN (dim) Ejemplo: Vamos a crear tres vectores: uno numérico, uno de caracter y uno lógico. Podemos utilizar la función length() para medir el largo de estos (cuántos elementos contiene). vector_numerico &lt;- c(1, 2, 3, 4, 5) length(vector_numerico) ## [1] 5 vector_caracter &lt;- c(&quot;arbol&quot;, &quot;casa&quot;, &quot;persona&quot;) length(vector_caracter) ## [1] 3 vector_logico&lt;- c(TRUE, TRUE, FALSE, FALSE, TRUE) length(vector_logico) ## [1] 5 También podemos utilizar la función class() para corroborar que cada vector tiene la misma clase de los elementos que contiene. class(vector_numerico) ## [1] &quot;numeric&quot; class(vector_caracter) ## [1] &quot;character&quot; class(vector_logico) ## [1] &quot;logical&quot; Tener en cuenta que los vectores también pueden tener valores perdidos (NA). vector_con_NA &lt;- c(1,2,3,NA,5) length(vector_con_NA) ## [1] 5 class(vector_con_NA) ## [1] &quot;numeric&quot; 1.3.2 Un vector especial: los factores Un factor es un tipo de datos específico en R. Puede ser descrito como un dato numérico representado por una etiqueta. Supongamos que tenemos un conjunto de datos que representan el género de personas encuestadas por teléfono, pero estos se encuentran capturados con los números 1 y 2. genero &lt;- c(1,1,1,2,2,1,2) El número 1 corresponde a Mujer y el 2 a Hombre. A diferencia del carácter, el factor tiene NIVELES (levels). Podemos crear un vector de tipo factor con la función factor(). genero_en_factor=factor(genero, levels= 1:2, labels=c(&quot;Mujer&quot;, &quot;Hombre&quot;)) genero_en_factor ## [1] Mujer Mujer Mujer Hombre Hombre Mujer Hombre ## Levels: Mujer Hombre En la práctica, muchas veces vamos a ver las variables de tipo factor en nuestro análisis. Por ello, debes ser muy cuidadoso en la preparación previa que debes realizar a la base de datos antes de aplicar las funciones. Asimismo, un factor puede estar ordenado o no ordenado. Esto nos sirve, por ejemplo, para crear variables de tipo ordinal. Podemos indicarlo ello, con el argumento ordered=. Veamos: confianza=c(1, 1, 3, 2) confianza_en_factor=factor(confianza, levels= 1:3, labels=c(&quot;Bajo&quot;, &quot;Medio&quot;, &quot;Alto&quot;), ordered = TRUE) confianza_en_factor ## [1] Bajo Bajo Alto Medio ## Levels: Bajo &lt; Medio &lt; Alto Vemos que nos indica los niveles, pero en este caso están ordenados de menor a mayor. 1.3.3 Data frames Los data frames son estructuras de datos de dos dimensiones (rectangulares) que pueden contener datos de diferentes tipos, por lo tanto, son heterogéneas. Compuesto por vectores. Estructura más usada para ciencia de datos. Mientras que en una matriz todas las celdas deben contener datos del mismo tipo, los renglones de un data frame admiten datos de distintos tipos, pero sus columnas conservan la restricción de contener datos de un sólo tipo. En términos generales, los renglones en un data frame representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables. mi_df &lt;- data.frame( &quot;variable1&quot; = 1:3, &quot;variable2&quot; = c(1.2, 3.4, 4.5), &quot;variable3&quot; = as.character(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)), &quot;variable4&quot; = as.factor(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)) ) #Para crear un DT los vectores de insumo deben ser del mismo largo mi_df ## variable1 variable2 variable3 variable4 ## 1 1 1.2 a 1 ## 2 2 3.4 b 2 ## 3 3 4.5 c 3 str(mi_df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ variable1: int 1 2 3 ## $ variable2: num 1.2 3.4 4.5 ## $ variable3: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; ## $ variable4: Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 2 3 1.3.4 Propiedades Un data.frame tiene: Dimensión: un número de filas y un número de columnas. dim(mi_df) #FILAS Y COLUMNAS ## [1] 3 4 Largo: número de casos length(mi_df) ## [1] 4 Nombre de columnas: Podemos consultar el nombre de las columnas (variables) con la función names(). names(mi_df) ## [1] &quot;variable1&quot; &quot;variable2&quot; &quot;variable3&quot; &quot;variable4&quot; 1.3.5 Índices Usar índices para obtener subconjuntos es el procedimiento más universal en R, pues funciona para todas las estructuras de datos. Un índice en R representa una posición. Cuando usamos índices le pedimos a R que extraiga de una estructura los datos que se encuentran en una o varias posiciones específicas dentro de ella. Ejemplos: Seleccionar la columna 2: mi_df [,2] ## [1] 1.2 3.4 4.5 Para seleccionar una columna, también podemos usar el símbolo de $. Es bastante usado en varias funciones. mi_df$variable2 ## [1] 1.2 3.4 4.5 Seleccionar el caso (fila) 2: mi_df [2,] ## variable1 variable2 variable3 variable4 ## 2 2 3.4 b 2 Seleccionar el elemento que se encuentra en la fila 2 y la columna 2: mi_df [2,2] ## [1] 3.4 1.4 Nuestras herramientas: Paquetes y funciones 1.4.1 Paquetes En R, un paquete es un conjunto de herramientas y funciones predefinidas que permiten a los usuarios realizar tareas específicas, como análisis de datos o visualización de gráficos. Los paquetes pueden ser instalados desde los repositorios de CRAN u otros lugares (como repositorios). Para instalar un paquete necesitas escribir install.packages(\"nombre_del_paquete\"). Luego de instalarlo, para comenzar a utilizarlo debes abrirlo con el siguiente comando library(nombre_del_paquete). 1.4.2 Funciones Las funciones son bloques de código que realizan una tarea específica. Pueden ser definidas por el usuario o pueden ser proporcionadas por un paquete (esto es lo más común). Las funciones toman argumentos, que son valores que se pasan a la función para que los utilice en su tarea. Los argumentos de una función son variables o valores que se pasan a la función para que sean utilizados en la tarea que se está realizando. Algunos argumentos son obligatorios, lo que significa que deben ser proporcionados para que la función pueda realizar su tarea, mientras que otros son opcionales y tienen un valor predeterminado si no se especifican. Para ver qué argumentos tiene una función puedes entrar a la documentación de la misma. Por ejemplo, el paquete “dplyr” es un conjunto de herramientas que se utiliza para manipular y transformar datos en R. Una de las funciones de “dplyr” es “filter”, que se utiliza para filtrar filas en un conjunto de datos. Un argumento obligatorio para la función “filter” es el conjunto de datos que se va a filtrar, mientras que un argumento opcional es la condición que se utilizará para filtrar los datos. 1.5 Abrir una base de datos En la práctica tenemos el reto de manipular bases de datos que se encuentran en distintos tipos de archivo. Algunas veces se encuentran en formato .xlsx (Excel regular), pero otras veces las encontramos en formato .csv (separado con comas o puntos y comas), .sav (archivos desde el SPSS), entre otros. Para ello, tenemos dos opciones. La más sencilla es utilizar el paquete rio y utilizamos la función import(). Este paquete es una navaja suiza porque te permite abrir distintos formatos con la misma función. library(rio) elecciones_2011&lt;-import(&quot;RESULTADOS_EG_PRESIDENCIAL_2011.xls&quot;) head(elecciones_2011,3) ## UBIGEO DEPARTAMENTO PROVINCIA DISTRITO ORGANIZACIÓN_POLITICA VOTOS_VALIDOS_OP VOTOS_VALIDOS VOTOS_BLANCO ## 1 010101 AMAZONAS CHACHAPOYAS CHACHAPOYAS FUERZA NACIONAL 6 11148 1034 ## 2 010101 AMAZONAS CHACHAPOYAS CHACHAPOYAS PARTIDO POLITICO ADELANTE 24 11148 1034 ## 3 010101 AMAZONAS CHACHAPOYAS CHACHAPOYAS DESPERTAR NACIONAL 35 11148 1034 ## VOTOS_NULOS NUMERO_ELECTORES VOTOS_EMITIDOS ## 1 340 15748 12522 ## 2 340 15748 12522 ## 3 340 15748 12522 O también podemos utilizar el paquete readxl() que pertenece al universo de Tidyverse. Lo bueno de esta función es que cuando visualizas la data se realiza de forma más ordenada. library(readxl) elecciones_2011&lt;-read_excel(&quot;RESULTADOS_EG_PRESIDENCIAL_2011.xls&quot;) head(elecciones_2011,3) ## # A tibble: 3 × 11 ## UBIGEO DEPARTAMENTO PROVINCIA DISTRITO ORGANIZACIÓN_POLITICA VOTOS_VALIDOS_OP VOTOS_VALIDOS VOTOS_BLANCO VOTOS_NULOS ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 010101 AMAZONAS CHACHAPOYAS CHACHAPOY… FUERZA NACIONAL 6 11148 1034 340 ## 2 010101 AMAZONAS CHACHAPOYAS CHACHAPOY… PARTIDO POLITICO ADE… 24 11148 1034 340 ## 3 010101 AMAZONAS CHACHAPOYAS CHACHAPOY… DESPERTAR NACIONAL 35 11148 1034 340 ## # ℹ 2 more variables: NUMERO_ELECTORES &lt;dbl&gt;, VOTOS_EMITIDOS &lt;dbl&gt; Si deseas utilizar esta segunda forma, puedes aplicar la función read_csv() para archivos separados con comas. 1.6 Manipulación de datos con Tidyverse Por manipulación de datos nos referimos a que, una vez que tengamos nuestro data frame, es usual aplicar algunas funciones para personalizar aún más la data con la que estamos trabajando. Algunas de estas funciones son por ejemplo: Filtrar data: Normalmente no utilizamos toda la data disponible, sino sólo una parte. Para ello podemos filtrar la data con ciertos criterios. Seleccionar columnas: El mismo razonamiento que el punto anterior. Para hacer más ágil la manipulación de un data frame, podemos quedarnos sólo con aquellas columnas que vamos a utilizar. Ordenar según una variable: Hay ocasiones en que necesitamos que nuestra data esté ordenada según una determinada variable. Crear estadísticos de resumen: Lo veremos en la siguiente sesión, pero siempre necesitamos crear estadísticos de resumen, como los relaciones a tendencia central, posición o dispersión. Crear nuevas variable: Esto nos permite generar nuevas variables a partir de las que ya se encuentran en nuestra data. Para todas estas funciones mencionadas se recomienda utilizar el paquete dplyr(). Pueden visualizar un video tutorial aquí: El uso de Tidyverse será transversal a lo largo del curso. "],["manipulación-de-datos-y-estadísticos-descriptivos.html", "Sesión 2 Manipulación de datos y estadísticos descriptivos 2.1 Preparación 2.2 Base de datos 2.3 Estadísticos de tendencia central 2.4 Estadísticos de posición 2.5 Estadísticos de dispersión", " Sesión 2 Manipulación de datos y estadísticos descriptivos 2.1 Preparación 2.1.1 Entorno de trabajo Crea la carpeta donde guardarás toda la información creada. Te recomiendo que para cada clase crees un Proyecto de R. Eso te permitirá mantener un orden. Una vez creado, puedes crear un R Markdown como este para empezar a trabajar. 2.1.2 Paquetes a utilizar Ahora bien, DE PREFERENCIA, siempre que inicies un R Markdown o Script en el R Studio tienes que abrir los paquetes que vas a utilizar. En este caso vamos a utilizar el paquete Tidyverse (que ya incluye muchos otros paquetes ya explicados). Puedes abrirlo con el siguiente comando: # install.packages(&quot;tidyverse&quot;) library(tidyverse) Recuerda que la función library(nombre_del_paquete) sirve para abrir los paquetes YA INSTALADOS. Si te sale un mensaje que dice que “no existe el paquete” debes instalarlo con la función install.packages(línea previa). 2.2 Base de datos 2.2.1 Importación Para la parte I, II y III de esta clase utilizaremos la base de datos de los resultados de las elecciones presidenciales del 2011. Vamos a realizar un breve reporte sobre los porcentajes obtenidos por la organización política “Gana Perú”. library(readxl) elecciones &lt;- read_excel(&quot;RESULTADOS_EG_PRESIDENCIAL_2011.xls&quot;) Luego de abrir una base de datos, siempre te recomiendo darle un primer vistazo. Puedes ver las primeras diez filas con: head(elecciones, 10) ## # A tibble: 10 × 11 ## UBIGEO DEPARTAMENTO PROVINCIA DISTRITO ORGANIZACIÓN_POLITICA VOTOS_VALIDOS_OP VOTOS_VALIDOS VOTOS_BLANCO VOTOS_NULOS ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 010101 AMAZONAS CHACHAPOYAS CHACHAPO… FUERZA NACIONAL 6 11148 1034 340 ## 2 010101 AMAZONAS CHACHAPOYAS CHACHAPO… PARTIDO POLITICO ADE… 24 11148 1034 340 ## 3 010101 AMAZONAS CHACHAPOYAS CHACHAPO… DESPERTAR NACIONAL 35 11148 1034 340 ## 4 010101 AMAZONAS CHACHAPOYAS CHACHAPO… GANA PERU 2971 11148 1034 340 ## 5 010101 AMAZONAS CHACHAPOYAS CHACHAPO… FONAVISTAS DEL PERU 7 11148 1034 340 ## 6 010101 AMAZONAS CHACHAPOYAS CHACHAPO… JUSTICIA, TECNOLOGIA… 4 11148 1034 340 ## 7 010101 AMAZONAS CHACHAPOYAS CHACHAPO… FUERZA 2011 2651 11148 1034 340 ## 8 010101 AMAZONAS CHACHAPOYAS CHACHAPO… PARTIDO DESCENTRALIS… 2 11148 1034 340 ## 9 010101 AMAZONAS CHACHAPOYAS CHACHAPO… ALIANZA POR EL GRAN … 1479 11148 1034 340 ## 10 010101 AMAZONAS CHACHAPOYAS CHACHAPO… ALIANZA SOLIDARIDAD … 773 11148 1034 340 ## # ℹ 2 more variables: NUMERO_ELECTORES &lt;dbl&gt;, VOTOS_EMITIDOS &lt;dbl&gt; También puedes hacerle click en Environment o colocar: #View(elecciones) 2.2.2 Verificar el tipo de variable Veamos las variables, podemos utilizar la función names(nombre_de_data) names(elecciones) ## [1] &quot;UBIGEO&quot; &quot;DEPARTAMENTO&quot; &quot;PROVINCIA&quot; &quot;DISTRITO&quot; ## [5] &quot;ORGANIZACIÓN_POLITICA&quot; &quot;VOTOS_VALIDOS_OP&quot; &quot;VOTOS_VALIDOS&quot; &quot;VOTOS_BLANCO&quot; ## [9] &quot;VOTOS_NULOS&quot; &quot;NUMERO_ELECTORES&quot; &quot;VOTOS_EMITIDOS&quot; Puedes editar los nombres si deseas. En este caso vamos a cambiar el nombre de la variable ORGANIZACIÓN_POLITICA por ORG_POL. elecciones&lt;- elecciones %&gt;% rename(ORG_POL=ORGANIZACIÓN_POLITICA) Observación: Ten en cuenta que aquí estamos sobre escribiendo. Veamos si el formato es adecuado. str(elecciones) ## tibble [23,958 × 11] (S3: tbl_df/tbl/data.frame) ## $ UBIGEO : chr [1:23958] &quot;010101&quot; &quot;010101&quot; &quot;010101&quot; &quot;010101&quot; ... ## $ DEPARTAMENTO : chr [1:23958] &quot;AMAZONAS&quot; &quot;AMAZONAS&quot; &quot;AMAZONAS&quot; &quot;AMAZONAS&quot; ... ## $ PROVINCIA : chr [1:23958] &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; ... ## $ DISTRITO : chr [1:23958] &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; &quot;CHACHAPOYAS&quot; ... ## $ ORG_POL : chr [1:23958] &quot;FUERZA NACIONAL&quot; &quot;PARTIDO POLITICO ADELANTE&quot; &quot;DESPERTAR NACIONAL&quot; &quot;GANA PERU&quot; ... ## $ VOTOS_VALIDOS_OP: num [1:23958] 6 24 35 2971 7 ... ## $ VOTOS_VALIDOS : num [1:23958] 11148 11148 11148 11148 11148 ... ## $ VOTOS_BLANCO : num [1:23958] 1034 1034 1034 1034 1034 ... ## $ VOTOS_NULOS : num [1:23958] 340 340 340 340 340 340 340 340 340 340 ... ## $ NUMERO_ELECTORES: num [1:23958] 15748 15748 15748 15748 15748 ... ## $ VOTOS_EMITIDOS : num [1:23958] 12522 12522 12522 12522 12522 ... Ok, los caracteres están como chr y las variables numéricas como num. 2.2.3 Creación de subset Aquí nos debemos preguntar: a lo largo de nuestro trabajo, vamos a utilizar toda la data o sólo una parte? En este caso puntual, sólo vamos a utilizar aquellos resultados de la organización política Gana Perú. Por ello, vamos a hacer un filtro con ese criterio y vamos a **crear un subset o una data ya filtrada”. A esta data la llamaremos elecciones_GN. elecciones_GN &lt;- elecciones%&gt;% filter(ORG_POL==&quot;GANA PERU&quot;) 2.3 Estadísticos de tendencia central Como nuestro objetivo es explorar el porcentaje de votos válidos obtenidos por Gana Perú, vamos a crear esa variable en base a dos variable observables: el número de votos obtenidos y el número total de votos válidos. Utilizamos la función mutate(). elecciones_GN&lt;-elecciones_GN %&gt;% mutate(porc.voto=VOTOS_VALIDOS_OP/VOTOS_VALIDOS*100) Ahora podemos calculas los tres estadísticos de tendencia central con la función summarise(). elecciones_GN%&gt;% summarise(Media=mean(porc.voto, na.rm = TRUE), Mediana=median(porc.voto, na.rm = TRUE)) ## # A tibble: 1 × 2 ## Media Mediana ## &lt;dbl&gt; &lt;dbl&gt; ## 1 38.9 35.1 # Recuerda que colocamos na.rm = TRUE para que haga el cálculo omitiendo # aquellas celdas vacías (valores perdidos) 2.4 Estadísticos de posición 2.4.1 Percentiles Los estadísticos de posición son medidas que indican la posición relativa de un valor dentro de un conjunto de datos ordenados de menor a mayor. Los estadísticos de posición más utilizados son los percentiles (y sus derivados que son los cuartiles y deciles). Los estadísticos de posición son útiles para resumir los datos y obtener información sobre la distribución de los mismos. Se utilizan para describir la dispersión de los datos alrededor de la mediana, así como identificar valores atípicos o extremos en el conjunto de datos. Para calcular un determinado percentil podemos solicitarlo utilizando la función summarise() junto con quantile(). Vamos a solicitar los cuartiles (el percentil 25, 50 y 75) de la variable porc.voto: elecciones_GN %&gt;% summarise(P25=quantile(porc.voto, 0.25, na.rm=TRUE), P50=quantile(porc.voto, 0.50, na.rm=TRUE), P75=quantile(porc.voto, 0.75, na.rm=TRUE)) ## # A tibble: 1 × 3 ## P25 P50 P75 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.6 35.1 56.0 Si son muchos percentiles también puedes usar la función concatenar c(), como en el siguiente ejemplo: elecciones_GN %&gt;% summarise(quantile(porc.voto, c(0.10, 0.25,0.50, 0.75, 0.90), na.rm=TRUE)) ## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in dplyr 1.1.0. ## ℹ Please use `reframe()` instead. ## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()` always returns an ungrouped data frame and ## adjust accordingly. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## # A tibble: 5 × 1 ## `quantile(porc.voto, c(0.1, 0.25, 0.5, 0.75, 0.9), na.rm = TRUE)` ## &lt;dbl&gt; ## 1 11.4 ## 2 21.6 ## 3 35.1 ## 4 56.0 ## 5 70.7 2.5 Estadísticos de dispersión Las medidas de dispersión son una serie de estadísticos que nos permiten conocer la variabilidad o amplitud de los datos en un conjunto de observaciones. Estas medidas nos indican cuánto se alejan los valores de la media, la mediana o cualquier otro estadístico de tendencia central, lo que es de gran utilidad para entender la homogeneidad o heterogeneidad de la distribución de los datos. Las medidas de dispersión más comunes son el rango, la varianza, la desviación estándar, el rango intercuartílico y el coeficiente de variación. 2.5.1 El Rango El rango es una medida de dispersión que se utiliza para determinar la amplitud total de un conjunto de datos. Es la diferencia entre el valor máximo y el valor mínimo en un conjunto de datos. Es una medida muy sencilla de calcular y proporciona una idea general de la variabilidad de los datos. Sin embargo, no toma en cuenta la distribución de los datos en el conjunto, lo que puede hacer que la medida sea menos informativa en ciertos casos. elecciones_GN %&gt;% filter(ORG_POL==&quot;GANA PERU&quot;) %&gt;% summarise(Rango=max(porc.voto, na.rm=TRUE)-min(porc.voto, na.rm=TRUE)) ## # A tibble: 1 × 1 ## Rango ## &lt;dbl&gt; ## 1 98.1 En este caso vemos que los valores de la variable porc.voto tiene un rango de 98.1 unidades (porcentuales) . 2.5.2 La Varianza La varianza es una medida de dispersión que indica qué tan dispersos están los datos con respecto a su media. \\[\\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{n}\\] Matemáticamente, la varianza se define como la media aritmética de los cuadrados de las diferencias entre cada valor de la variable y la media. Lo calculamos utilizando la función var(): elecciones_GN %&gt;% summarise(Varianza=var(porc.voto, na.rm=TRUE)) ## # A tibble: 1 × 1 ## Varianza ## &lt;dbl&gt; ## 1 472. En este ejemplo, el porcentaje de los Votos obtenidos por Gana Perú tiene una dispersión de 472 entre los distritos del Perú. Recuerda que la varianza puede servir a la hora de comparar dos variable que están medidas en las mismas unidades (como si comparáramos este porcentaje vs el obtenido por Fuerza 2011). Si deseamos analizar una única variable es más fácil ver la Desviación Estándar. 2.5.3 La Desviación Estándar Es la medida de dispersión más común y se calcula como la raíz cuadrada de la varianza. Nos indica cuánto se desvían los valores de la distribución con respecto a la media. \\[\\sqrt{\\text{Var}(X)}\\] La desviación estándar se expresa en las mismas unidades que los datos originales, lo que la hace fácilmente interpretable. Una desviación estándar pequeña indica que los datos están muy concentrados alrededor de la media, mientras que una desviación estándar grande indica que los datos están más dispersos y que existe una mayor variabilidad en los mismos. Lo calculamos usando la función sd(): elecciones_GN %&gt;% summarise(Desv_est=sd(porc.voto, na.rm=TRUE)) ## # A tibble: 1 × 1 ## Desv_est ## &lt;dbl&gt; ## 1 21.7 La forma de interpretación es que los valores observados están en promedio a una distancia de 21 unidades de la media. 2.5.4 El Rango intercuartílico El rango intercuartil (IQR por sus siglas en inglés) es una medida de dispersión utilizada en estadística que se define como la diferencia entre el tercer y primer cuartil de un conjunto de datos. En otras palabras, el IQR mide la distancia entre el 25% y el 75% de los datos ordenados de menor a mayor. El IQR es una medida de dispersión robusta que no se ve afectada por valores atípicos o extremos en los datos, a diferencia del rango o la desviación estándar. Es útil para resumir la variabilidad de los datos en un intervalo de valores que contiene la mayor parte de los datos y que no está influenciado por los valores extremos. El IQR se puede utilizar para identificar valores atípicos o para comparar la variabilidad entre diferentes conjuntos de datos. Lo calculamos utilizando la función IQR(): elecciones_GN %&gt;% summarise(IQR=IQR(porc.voto, na.rm=TRUE)) ## # A tibble: 1 × 1 ## IQR ## &lt;dbl&gt; ## 1 34.4 En este caso vemos que el IQR de la variable porc.voto es de 34.4. Es decir, la distancia entre el primer cuartil y el tercer cuartil es de 34 unidades en la escala de la variable (porcentaje). 2.5.5 Curvas de distribución de frecuencias Las curvas de distribución son representaciones gráficas de la distribución de una variable aleatoria. Estas curvas se utilizan para mostrar cómo se distribuyen los valores de una variable en un conjunto de datos. El principal objetivo examinar las posiciones relativas de la media, la mediana y la moda, para estimar la FORMA DE UNA DISTRIBUCIÓN DE FRECUENCIAS. Tenemos dos grandes opciones: Distribución normal: Curva de distribución de frecuencias donde la media, la mediana y la moda de una variable son iguales entre sí y la distribución de la puntuaciones tiene forma de campana. Distribución sesgada: Curva de distribución de frecuencias en la cual la media, la mediana y la moda de una variable son desiguales y algunos de los sujetos tienen puntuaciones sumamente altas o bajas. Ahora veamos nuestro ejemplo, a qué curva de distribución se asemejará? Vemos que nuestra variable de Porcentaje de votos valídos obtenido por Gana Perú a nivel distrital, posee una distribución con un sesgo positivo. Veamos si es cierto la teoría. Grafiquemos la media (38.90), mediana (35.13) y moda (27.00) en dicha curva de distribución. Corroboramos lo explicitado por la teoría en cuento a la relación entre el sesgo y el posicionamiento de los estadísticos de tendencia central. "],["visualización-de-datos-con-ggplot2.html", "Sesión 3 Visualización de datos con ggplot2 3.1 GGPLOT2: Una herramienta para la visualización 3.2 Código", " Sesión 3 Visualización de datos con ggplot2 3.1 GGPLOT2: Una herramienta para la visualización 3.1.1 Descripción ggplot2 es una biblioteca de visualización de datos en R que se utiliza para crear gráficos de alta calidad y personalizados. Fue desarrollada por Hadley Wickham y se basa en la “Gramática de los gráficos” propuesta por Leland Wilkinson. En ggplot2, los gráficos se crean mediante la combinación de componentes llamados capas. Cada capa representa una parte del gráfico, como los datos, los ejes, la leyenda y la geometría utilizada para representar los datos, como barras, puntos, líneas, etc. ggplot2 también utiliza el concepto de “mapeo estético” para relacionar variables de los datos con propiedades visuales del gráfico, como el color, la forma y el tamaño. Esto permite crear gráficos altamente personalizados que resaltan patrones y tendencias en los datos. En resumen, ggplot2 es una herramienta poderosa y flexible para crear visualizaciones de datos en R, que se basa en una sintaxis intuitiva y coherente que facilita la creación de gráficos complejos y personalizados. 3.1.2 Las siete capas En ggplot2, las visualizaciones se crean mediante la combinación de capas. Cada capa representa una parte del gráfico, como los datos, los ejes, la leyenda y la geometría utilizada para representar los datos, como barras, puntos, líneas, etc. A continuación, se describen brevemente las capas más comunes de ggplot2: Capa de datos (Data): Esta capa representa los datos que se van a visualizar y se proporcionan mediante un objeto de marco de datos o un tibble. Capa de mapeo estético (Aesthetics): Esta capa establece la relación entre las variables de los datos y las propiedades visuales del gráfico. Se utiliza la función aes() para establecer la relación. Por ejemplo, aes(x = variable_x, y = variable_y) establece que la variable x se va a representar en el eje x y la variable y en el eje y. Capa de geometría (Geometries): Esta capa representa la forma en que los datos se van a visualizar. La geometría se especifica utilizando una de las funciones geom_*() disponibles en ggplot2, como geom_point() para crear gráficos de dispersión, geom_line() para crear gráficos de líneas, geom_bar() para crear gráficos de barras, etc. Capa de facetas (Facets): Esta capa permite dividir los datos en subconjuntos y visualizarlos en diferentes paneles. Se utiliza la función facet_*() para crear facetas. Por ejemplo, facet_wrap(~variable_z) divide los datos en diferentes paneles según los valores de la variable z. Capa de transformaciones estadísticas (Statistical transformations): Esta capa permite aplicar transformaciones estadísticas a los datos antes de visualizarlos. Se utiliza la función stat_*() para aplicar transformaciones estadísticas. Por ejemplo, stat_smooth() ajusta una curva suave a los datos. Capa de coordenadas (Coordinates): Esta capa permite cambiar las coordenadas del gráfico. Se utiliza la función coord_*() para cambiar las coordenadas. Por ejemplo, coord_polar() cambia las coordenadas del gráfico a coordenadas polares. Capa de temas (Themes): Esta capa permite cambiar el aspecto visual del gráfico. Se utiliza la función theme_*() para cambiar el tema. Por ejemplo, theme_dark() cambia el tema del gráfico a un tema oscuro. Cada una de estas capas en ggplot2 tiene una función específica que ayuda a crear visualizaciones personalizadas y de alta calidad. La combinación adecuada de estas capas permite crear gráficos que resalten patrones y tendencias en los datos de manera efectiva. 3.1.3 Slow ggplot Es un enfoque utilizado para la creación de gráficos en ggplot. La lógica de esta aproximación es incremental, es decir, teniendo en cuenta que el código requerido para generar un gráfico de ggplot involucra varias capas (o sentencias), se sugiere aprender a utilizar el paquete haciendo énfasis en cada una de las capas y los cambios que genera en el output. Es más lento que crear todo el código de golpe, pero es mucho más claro, tanto para el usuario que recién aprende la librería, como para la explicación a un público no conocedor del software. Inclusive se han creado paquetes específicamente vinculados con este enfoque, como el Flipbookr de Evangeline Reynolds. Aquí puedes revisar un breve post en el que comenta sobre el slow ggplot. Se sugiero revisar este link donde el autor ha diseñado un aplicativo para ver justamente cómo se construye un ggplot de forma incremental. Al lado izquierdo veras un conjunto de decisiones que puede tomar el investigador, usando la data gapminder, y a la derecha los cambios que se genera en el código y, por supuesto, en el output final. También puedes ir directamente al aplicativo presionando este link. 3.2 Código En esta presentación te muestro un resumen sobre este capítulo, presentando el código y los outputs respectivos. Cabe recalcar que el código presentado sigue el enfoque de slow ggplot. "],["estadística-inferencial-introducción.html", "Sesión 4 Estadística Inferencial: Introducción 4.1 Antecedentes relevantes 4.2 Elementos básicos de probabilidad 4.3 Ejercicio con R 4.4 Elementos básicos de la inferencia 4.5 Intervalo de confianza de una media 4.6 Ejercicio con R: ENADES 2022", " Sesión 4 Estadística Inferencial: Introducción 4.1 Antecedentes relevantes La desviación estándar La desviación estándar es una parte integral de la estadística inferencial y entender su funcionamiento nos ayudará a utilizarla más adelante. Recuerda, la desviación estándar es la raíz cuadrada de la sumatoria de todas las diferencias entre los valores observados y su media. \\[\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2}\\] La desviación estándar se llama “estándar” porque proporciona una unidad de medida común para comparar unidades observadas de medida muy diferente. Ejemplo: imaginemos que tenemos la variable ingreso que tiene media 120 y desviación estándar 10 y deseamos analizar el caso X = 130. Forma de puntuación Fórmula Ejemplo Puntuación en bruto \\[X\\] 130 soles Puntuación de desviación \\[X - \\bar{X}\\] 10 soles Puntuación estandarizada Z \\[Z = \\frac{X - \\bar{X}}{S_X}\\] 1 SD Presta atención a las unidades de medida! En el ejemplo anterior podemos corroborar que nosotros podemos medir la lejanía de una observación respecto a su media, no sólo en las unidades originales, sino diciendo cuántas desviaciones estándar la separa de su media. Está última medida se conoce como puntuaciones Z y es muy útil para comparar niveles de dispersión de variables observadas. Algunas reglas: 1) mientras más lejana esté una observación de su media, mayor será su puntuación de desvación y su puntuación Z. 2) El signo de la puntuación Z indica si la observación está por debajo o por encima de la media. La distribución (curva) normal y las SD Anteriormente te comenté que la curva normal (forma de campana) ejercía un rol fundamental en el contexto de la estadística inferencial. Esto es gracias a una propiedades muy interesantes. La principal: cuando una variable tiene distribución de puntuaciones que es normal (casi) la totalidad de observaciones están distribuidas +- 3 desviaciones estándar (puntuaciones Z) respecto de su media. Este principio matemático sienta las bases de la imaginación estadística, dado que muchos fenómenos naturales poseen distribuciones de frecuencias en forma de campana como la curva normal. Como se ve en la figura, son 4 los principios: 1) 50% de las puntuaciones caen encima de la media y 50% debajo; 2) Prácticamente todas las puntuaciones caen dentro de 3 SD a partir de la media en ambas direcciones (en realidad el 99.7%); 3) Cerca del 95% de las puntuaciones de una variable normalmente distribuida caen dentro de una distancia de +- 2 SD respecto de la media; y 4) Alrededor del 68% de las puntuaciones caen dentro de una distancia de +-1 SD respecto de la media. 4.2 Elementos básicos de probabilidad Conceptos Probabilidad: Análisis y comprensión de las ocurrencias (de eventos) por el azar. Más específicamente, es un detalle de con qué frecuencia es probable que ocurra un evento de interés particular entre un gran número de ensayos. La fórmula general para presentar una probabilidad es: \\[P(\\text{éxito}) = \\frac{\\text{# de éxitos}}{\\text{# de ensayos}}\\] Existen diversas reglas básica de la teoría de la probabilidad. Te recomiendo leer las páginas 172 - 176 de Ritchey (2006). Curva normal y probabilidades En probabilidades hay un principio que utiliza el concepto de la curva normal que hemos explicado anteriormente. Dada cierta variable que se distribuye normalmente, podemos calcular puntuaciones estándarizadas (puntuaciones Z) y usarlas de tal forma que calculas el número de puntuaciones que cae entre dos puntos en la distribución. Veamos nuevamente la curva. Ahora, dependiendo las puntuaciones Z que elijamos vamos a hacer un cálculo de qué porcentaje de puntuaciones se encuentran bajo la curva. Dicho de en otras palabras, vamos a examinar el área bajo la curva. En este sentido, el área bajo la curva representa probabilidades de ocurrencia. Veamos el siguiente ejemplo, donde tenemos el promedio de edad en una población de estudio (X=69) con desviación estándar 3 (SD=3). Con este ejemplo, para el intervalo de 66 - 72 podemos afirmar lo siguiente: La proporción de casos entre estos dos puntos es 68%. El área bajo la curva entre estas dos puntuaciones es 68%. La probabilidad de seleccionar al azar un caso entre estas dos puntuaciones es 68%. 4.3 Ejercicio con R Usted tiene la siguiente variable con distribución normal, de media 100 y desviación estándar 20. media &lt;- 100 desviacion_estandar &lt;- 20 variable_aleatoria &lt;- rnorm(n = 1000, mean = media, sd = desviacion_estandar) summary(variable_aleatoria) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 33.34 86.46 99.57 100.21 113.54 160.82 Visualicemos si tiene la curva normal. library(tidyverse) variable_aleatoria |&gt; data.frame() |&gt; ggplot() + aes(x=variable_aleatoria) + geom_density() Comprobemos la teoría. Pintemos las líneas de -1 SD, -2 SD, -3SD, la media, +1 SD, +2 SD y +3SD. Recuerda que la desviación estándar es 20. Cuánto porcentaje de casos se encuentran entre 80 (-1SD) y 120 (+1SD)? variable_aleatoria |&gt; data.frame() |&gt; filter(variable_aleatoria&gt;=80, variable_aleatoria&lt;=120) |&gt; count() |&gt; mutate(p=n/1000) ## n p ## 1 666 0.666 Y de 60 a 140? variable_aleatoria |&gt; data.frame() |&gt; filter(variable_aleatoria&gt;=60, variable_aleatoria&lt;=140) |&gt; count() |&gt; mutate(p=n/1000) ## n p ## 1 958 0.958 4.4 Elementos básicos de la inferencia 4.4.1 Definiciones preliminares La estadística inferencial utiliza la muestra de datos para hacer estimaciones y tomar decisiones acerca de las características de una población. Esto implica la utilización de técnicas y métodos para inferir información sobre la población a partir de la información recopilada en la muestra. Algunas definiciones básicas: Definición Descripción Población Se refiere al conjunto total de individuos, objetos, eventos, medidas o cualquier otra cosa que se quiera estudiar. En estadística inferencial, la población se utiliza como el objeto de estudio, y se busca inferir información sobre ella a partir de la muestra. Muestra Es un subconjunto de la población que se utiliza para hacer inferencias sobre la población en su conjunto. La selección de la muestra debe hacerse de tal forma que represente de manera adecuada las características de la población. Estadístico Es una medida numérica que se utiliza para resumir o describir alguna característica de la muestra. Los estadísticos se calculan a partir de los datos de la muestra y se utilizan para hacer inferencias sobre los parámetros de la población. Parámetro Es una medida numérica que describe alguna característica de la población. En estadística inferencial, el objetivo es hacer inferencias sobre los parámetros de la población a partir de los datos de la muestra. 4.4.2 Estimación puntual Imaginemos que tenemos una población de 45 individuos. A cada uno se le ha preguntado acerca de cuántas veces han asistido a una manifestación, marcha o evento público en los últimos 3 años. La data es la siguiente: poblacion&lt;- c(1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,2,2,2,2,4,4,4,4,4,4,4,4,5,5,5,5,5,5) length(poblacion) ## [1] 45 Veámoslo gráficamente. poblacion=as.data.frame(poblacion) poblacion |&gt; ggplot()+ aes(x=poblacion) |&gt; geom_bar() Como dato previo sabemos que en esta población la media es de 2.71. Ojo, normalmente nosotros no sabemos cuál es el parámetro poblacional, sólo lo planteamos así con fines didácticos. mean(poblacion$poblacion) ## [1] 2.711111 Ahora vamos a hacer un estudio inferencial a partir de una muestra de 5 personas (n) para calcular la media poblacional (parámetro). Primero hay que preguntarnos, cuántas muestras posibles existen para 50 distintos elementos? No son pocas. n_muestras = choose(50, 5) n_muestras ## [1] 2118760 Ok, elijamos una muestra aleatoria. Utilicemos la tabla de números aleatorios (el antepasado del famoso set.seed() de R). Si no es legible, puedes entrar en este link - Elijamos una muestra de 5 casos según la tabla de números aleatorios, es decir, los casos: 10, 22, 24, 42 y 37. muestra1= poblacion[c(10, 22, 24, 42, 37),] muestra1 ## [1] 2 2 3 5 4 Calculemos la media muestral (estadístico): mean(muestra1) ## [1] 3.2 Según la muestra que hemos calculado, hemos obtenido una media de 3.2. Si nosotros nos quedamos sólo con ese valor entonces estamos eligiendo una estimación puntual. ESTIMACIÓN PUNTUAL: Estadístico proporcionado sin indicar un rango de error. 4.4.3 Error de muestreo Si nosotros hacemos este ejercicio una y otra vez, es decir seleccionamos varias muestras, probablemente obtengamos resultados diferentes. Cada media muestral es ligeramente mayor o menor que la anterior. muestra2= poblacion[c(4, 23, 41, 21, 45),] muestra2 ## [1] 1 2 5 2 5 mean(muestra2) ## [1] 3 Esto quiere decir que los estadísticos muestrales no son los valores exactos de parámetros poblacionales. Son muchas las posibilidades de que obtengas medias muestrales ligeramente diferentes. Aquí los teóricos descubrieron un principio: cuando uno realiza muestreos repetidos y calcula sus respectivos estadísticos, los valores tendían a agruparse alrededor de un valor particular, el cual sería el verdadero parámetro poblacional. 4.4.4 Distribuciones de muestreo. Ahora bien, ya sabemos que podemos solicitar varias muestras. Qué distribución tendrían todas las medias que calculemos en un determinado número de muestras? Calculemos 100 000 muestras para el vector de nuestra población y calculemos la media de cada uno en una tabla. Para ello, le pedimos ayuda al R. poblacion&lt;- c(1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,2,2,2,2,4,4,4,4,4,4,4,4,5,5,5,5,5,5) muestras &lt;- replicate(100000, sample(poblacion, size=5)) medias &lt;- apply(muestras, 2, mean) df &lt;- data.frame(media=medias) Ahora grafiquemos las medias calculadas. df |&gt; ggplot()+ aes(x=medias) + geom_histogram(bins = 10) Descubrimos que cuando obtenemos muchas muestras y calculamos sus estadísticos (medias), estan en conjunto adquieren una distribución normal. Esto ocurre, a pesar de que la distribución de las puntuaciones originales no tenía distribución normal. Una distribución muestral de medias tiene forma normal. Puesto que una distribución muestral de medias adopta una forma normal se puede calcular la probabilidad de ocurrencia de cualquier resultado muestral Una distribución muestral nos indica con qué frecuencia un estadístico muestral tiene la probabilidad de fallar respecto al valor real del parámetro poblacional 4.4.5 Error estándar Recuerdas que habíamos dicho que en la curva normal se podría evidenciar que la totalidad de las observaciones se encontraban entre -3 y +3 desviaciones estándar respecto de la media? En el caso de las distribuciones muestrales, esa desviación estándar es conocida como error estándar. El error estándar mide la dispersión del error de muestreo que ocurre cuando se muestrea repetidamente una población (como lo hicimos líneas arriba). \\[s_{\\hat{x}} = \\frac{s}{\\sqrt{n}}\\] Entonces los puntos más importantes del EE son: El error estándar es una medida de cuánto se espera que varíen las medias de las muestras tomadas de una población determinada. A medida que el tamaño de la muestra aumenta, el error estándar tiende a disminuir. El error estándar es importante en el cálculo de los intervalos de confianza. Cuanto menor sea el error estándar, menor será la variabilidad de las medias muestrales y más preciso será el intervalo de confianza. El error estándar se calcula dividiendo la desviación estándar de la población entre la raíz cuadrada del tamaño de la muestra. En la mayoría de los casos, la desviación estándar de la población no se conoce y se utiliza la desviación estándar de la muestra para estimar el error estándar. 4.4.6 Teorema del límite central El teorema del límite central (TLC) es uno de los conceptos más importantes de la estadística y es fundamental en el muestreo y la inferencia estadística. En términos simples, el teorema del límite central dice que si tomamos suficientes muestras aleatorias grandes de una población, la distribución de las medias de esas muestras será una distribución normal, sin importar cómo se vea la distribución original de la población. Esto es importante en el muestreo porque nos permite hacer inferencias precisas sobre una población, incluso si no conocemos su distribución. Si podemos asumir que la distribución de la población es aproximadamente normal, entonces podemos usar la distribución normal de las medias de las muestras para hacer predicciones y estimaciones precisas sobre la población. Además, el TLC nos permite calcular intervalos de confianza y realizar pruebas de hipótesis (siguiente secciòn) con mayor precisión, lo que nos permite tomar decisiones más informadas basadas en los datos muestrales. En resumen, el teorema del límite central es una herramienta clave en la inferencia estadística y nos permite hacer generalizaciones precisas sobre una población a partir de datos muestrales. Máquina de Galton El Tablero de Galton ilustra cómo la distribución de frecuencias de los resultados de muchos eventos aleatorios independientes se acerca a una distribución normal, independientemente de la forma de la distribución original, siempre que el número de eventos sea lo suficientemente grande. 4.4.7 Ley de los grandes números La ley de los grandes números es un teorema en estadística que establece que, a medida que el tamaño de una muestra aumenta, la media muestral se acerca a la media poblacional. En otras palabras, cuando se toman muestras cada vez más grandes de una población, se espera que la media de esas muestras se acerque cada vez más a la media real de la población. Esta ley es importante porque permite a los investigadores obtener estimaciones precisas de los parámetros de una población a partir de una muestra relativamente pequeña. Además, esta ley también es fundamental para la teoría de la probabilidad y es utilizada en muchas áreas de la estadística y de la ciencia en general. Mientras más grande sea la muestra, la media muestras se acerca a la media poblacional. 4.5 Intervalo de confianza de una media El intervalo de confianza es un rango de valores posibles de un parámetro expresado con un grado específico de confianza. Si tenemos un nivel de confianza de 95% quiere decir que si realizamos 100 veces el mismo procedimiento de muestreo y calculamos los estadísticos de interés, 95 veces nos van a salir resultados en los intervalos calculados. Si lo realizamos con un 99% de confianza, de igual manera, si realizamos 100 veces el procedimiento, 99 veces nos va a salir resultados en el intervalo resultante. Esto lo tenemos claro gracias a la explicación del rol que cumple la curva normal y sus propiedades. A MAYOR CONFIANZA MENOR ES LA PRECISIÓN (LOS INTERVALOS SON MÁS AMPLIOS) Para el cálculo de un intervalo de confianza utilizamos la siguiente fórmula. Recuerda - Ese valor que se suma y se resta a la media muestral es el término de error, sin embargo, es más conocido como margen de error. 4.6 Ejercicio con R: ENADES 2022 El Instituto de Estudios Peruanos, por encargo de Oxfam en Perú, elaboró la I Encuesta Nacional de percepción de Desigualdades – ENADES 2022. Este estudio pone a disposición del público el análisis estadístico más completo a la fecha sobre la percepción de las diferentes formas de desigualdad en el Perú. Además de factores económicos, la presente encuesta incluye indicadores que permiten medir la magnitud de una serie de brechas sociales y políticas: desde diferencias de género, clase y relaciones étnico-raciales, hasta dimensiones subjetivas de la desigualdad y sus vínculos con orientaciones políticas. Como se muestra a lo largo del informe, la base de datos de este proyecto provee herramientas valiosas a expertos de diferentes campos, tanto académicos como profesionales, estudiantes y personas interesadas en el análisis multidimensional de la desigualdad en el país. Puedes abrir el cuestionario de la encuestas aquí También puedes ver el informe aquí 4.6.1 Abrir base de datos library(haven) enades&lt;-read_spss(&quot;data/ENADES_2022.sav&quot;) # Con esta función abrimos archivos de SPSS # enades&lt;-read_spss(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/ENADES_2022.sav&quot;) 4.6.2 Identificar una variable numérica Elijamos la variable P17: En una escala del 1 al 10, en la que 1 es “Totalmente inaceptable” y 10 es “Totalmente aceptable”. ¿Hasta qué punto es aceptable la desigualdad en el Perú? Dígame un número de 1 a 10, recuerde que 1 es “Totalmente inaceptable” y 10 es “Totalmente aceptable (RESPUESTA ESPONTÁNEA) La convertimos en numérica. enades$p17&lt;-as.numeric(enades$p17) Solicitamos los estadísticos descriptivos para darle una primera mirada. summary(enades$p17) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.000 5.000 4.571 7.000 10.000 23 Podemos graficarlo enades |&gt; ggplot() + aes(x=p17)+ geom_bar() 4.6.3 Cálculo del estimador puntual: media muestral mean(enades$p17, na.rm = TRUE) ## [1] 4.571334 4.6.4 Cálculo del intervalo de confianza al 95% Establecemos nuestra información de insumo. media&lt;-mean(enades$p17, na.rm = TRUE) SE&lt;- sd(enades$p17, na.rm = TRUE) z&lt;- 1.96 n &lt;-length(enades$p17) Ahora calculamos el error estándar: errorst &lt;- SE/sqrt(n) Una vez calculado el error estándar podemos calcular los límite superior e inferior. ### Calculamos el límite inferior lim_inf&lt;- media - (z*errorst) ### Calculamos el límite superior lim_sup&lt;- media + (z*errorst) ### Creamos el listado de valores para visualizarlo en versión data frame interval_m &lt;- data.frame(n, media, SE, z, errorst, lim_inf,lim_sup) interval_m ## n media SE z errorst lim_inf lim_sup ## 1 1530 4.571334 3.133069 1.96 0.08009848 4.414341 4.728327 Para verlo mejor podemos utilizar el paquete kableExtra: library(kableExtra) interval_m |&gt; kbl() |&gt; kable_styling() n media SE z errorst lim_inf lim_sup 1530 4.571334 3.133069 1.96 0.0800985 4.414341 4.728327 "],["ic-de-una-proporción-e-introducción-a-pruebas-de-hipótesis.html", "Sesión 5 IC de una proporción e introducción a pruebas de hipótesis 5.1 Recordemos 5.2 Intervalo de una proporción 5.3 Prueba de hipótesis: Una introducción", " Sesión 5 IC de una proporción e introducción a pruebas de hipótesis 5.1 Recordemos Abramos los paquetes que vamos a necesitar: library(pacman) p_load(haven, tidyverse, lsr, kableExtra) ## Installing package into &#39;C:/Users/Christian Chiroque/AppData/Local/R/win-library/4.3&#39; ## (as &#39;lib&#39; is unspecified) ## Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3: ## cannot open URL &#39;http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES&#39; ## package &#39;lsr&#39; successfully unpacked and MD5 sums checked ## ## The downloaded binary packages are in ## C:\\Users\\Christian Chiroque\\AppData\\Local\\Temp\\Rtmp6dHGK6\\downloaded_packages ## ## lsr installed # Esto equivale a: # library(haven) # library(tidyverse) # library(lsr) # library(kableExtra) # Es útil cuando necesitamos abrir un gran número de librerías! Continuemos con el ejercicio que vimos en la última clase utilizando la ENADES. enades&lt;-read_spss(&quot;data/ENADES_2022.sav&quot;) # Con esta función abrimos archivos de SPSS # Si quiere abrirlo desde GitHub entonces corre la siguiente línea: # enades&lt;-read_spss(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/ENADES_2022.sav&quot;) Elijamos la variable P17: En una escala del 1 al 10, en la que 1 es “Totalmente inaceptable” y 10 es “Totalmente aceptable”. ¿Hasta qué punto es aceptable la desigualdad en el Perú? Dígame un número de 1 a 10, recuerde que 1 es “Totalmente inaceptable” y 10 es “Totalmente aceptable (RESPUESTA ESPONTÁNEA) enades$p17&lt;-as.numeric(enades$p17) #Lo convertimos a numérica summary(enades$p17) # Calculamos estadísticos de resumen de forma rápida. ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.000 5.000 4.571 7.000 10.000 23 5.1.1 Pasos previos Cuál es la variable? Nivel de aceptación de la desigualdad en el Perú. Qué deseamos saber? Con nuestra muestra deseamos conocer el parámetro poblacional, es decir, la media de aceptación de la desigualdad en la población. 5.1.2 Estimación puntual El estimador puntual es una estadística descriptiva que se utiliza para estimar el valor desconocido de un parámetro poblacional a partir de una muestra. El estimador puntual proporciona una única estimación del valor del parámetro y se calcula a partir de los datos de la muestra. En este caso, la mejor estimación de la media población es simplemente la media muestral. mean(enades$p17, na.rm=TRUE) #Calculamos la media, obviando valores perdidos. ## [1] 4.571334 5.1.3 IC para una media Ahora, una vez identificado el estimador puntual, podemos cambiar nuestra estimación, ahora utilizando intervalos. La clase pasada lo hicimos con la fórmula, paso por paso. Ahora, utilizaremos una función: #library(lsr) #Recuerda que esta función está en el paquete lsr ciMean(enades$p17, na.rm = T) # Calculamos el intervalo de confianza de p17, obviando valores perdidos. ## 2.5% 97.5% ## [1,] 4.413023 4.729645 5.1.4 IC para una media según grupos Ahora bien, en la mayoría de casos lo que deseamos es comparar grupos poblacionales. Por ejemplo: Quién gana en promedio más dinero, los hombres o las mujeres? Quién apoya más, en promedio, a la democracia, los de la zona A o la zona B? Entre otros. Lo primero que necesitamos para comparar es justamente un grupo de comparación. Ya hablando en programación del R, necesitamos un factor. Recuerda que el factor era una variable que visualmente son números, pero teóricamente sabemos que cada número es un nivel. Utilizaremos la variable zona3, la cual separa a los encuestados según su procedencia en: Lima Metropolitana y Perú sin Lima. Veamos: enades$zona3&lt;-factor(enades$zona3, # Nombre de la variable a convertir levels=1:2, # Definimos los niveles (esta variable sólo tenía 2 niveles) labels=c(&quot;Lima Metropolitana&quot;, &quot;Perú sin Lima&quot;)) #Colocamos sus etiquetas #Con este comando hemos sobreescrito la variable zona3. Ahora lo que inicialmente era una variable numérica, ahora es un factor. Corroboremos: str(enades$zona3) #Solicitamos la estructura de la variable zona3 ## Factor w/ 2 levels &quot;Lima Metropolitana&quot;,..: 1 1 2 2 2 2 2 2 2 2 ... Solicitemos el intervalo de confianza de la variable p17 para cada grupo identificado: p17_segun_zona&lt;-enades %&gt;% group_by(zona3) %&gt;% #Agrupamos por zona summarise(mean = mean(p17, na.rm = TRUE), #Utilizamos summarise y pedimos la media, ci_lower = ciMean(p17, na.rm = T)[1], # También el PRIMER ELEMENTO de la función ciMean ci_upper = ciMean(p17, na.rm = T)[2]) #Y el SEGUNDO ELEMENTO de la función ciMean p17_segun_zona ## # A tibble: 2 × 4 ## zona3 mean ci_lower ci_upper ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Lima Metropolitana 4.36 4.10 4.62 ## 2 Perú sin Lima 4.69 4.49 4.89 Según el cálculo, para Lima Metropolitana la media poblacional se encuentra entre 4.09 y 4.62, mientras que para Perú sin Lima se encuentra entre 4.48 y 4.88. 5.1.5 Barras de error En el contexto de intervalos de confianza, las barras de error se utilizan para representar el nivel de incertidumbre en una estimación puntual del parámetro poblacional. Un intervalo de confianza es un rango de valores plausible para el valor del parámetro poblacional, y se construye a partir de una muestra aleatoria y un nivel de confianza específico. Las barras de error en un gráfico de intervalos de confianza se construyen a partir de los límites superior e inferior del intervalo de confianza. Generalmente se dibujan líneas verticales que se extienden desde el valor estimado del parámetro (que puede ser una media, una proporción, una diferencia de medias, etc.) hasta los límites del intervalo de confianza. Por ejemplo, si se estima la media de una variable a partir de una muestra y se desea construir un intervalo de confianza al 95%, las barras de error se construirán a partir del límite inferior y superior del intervalo de confianza, que contendrán el verdadero valor de la media poblacional con una probabilidad del 95%. Las barras de error en un gráfico de intervalos de confianza pueden ser útiles para comparar la precisión de las estimaciones entre diferentes grupos o condiciones. Si las barras de error son muy pequeñas, esto sugiere que la estimación es muy precisa y que hay una alta confianza en la validez del intervalo de confianza. Por otro lado, si las barras de error son grandes, esto sugiere que la estimación es menos precisa y que hay una mayor incertidumbre en el intervalo de confianza. Podemos utilizar un comando básico como la función plotmeans(): library(gplots) plotmeans(enades$p17~enades$zona3, p=0.95, xlab=&quot;Ámbito&quot;, ylab=&quot;Aceptación desigualdad&quot;, main=&quot;Gráfico de medias de Aceptación de la desigualdad en el Perú&quot;) Sin embargo, te recomiendo utilizar ggplot()! p17_segun_zona |&gt; #Data ggplot()+ #Iniciamos el ggplot. A partir de ahora son +! ya no |&gt;! aes(y=mean, x=zona3)+ #Los grupos en el eje X y la media en el eje Y geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=0.2)+ #Graficamos la barra de error geom_text(aes(label=paste(round(mean, 2))), vjust=0, size=5)+ #Colocamos el texto del valor de la media xlab(&quot;Procedencia&quot;) + #Etiqueta del eje X ylab(&quot;Nivel de aceptación de la desigualdad&quot;) # Etiqueta del eje y Una guía: Se superponen los intervalos? Si los intervalos de confianza se superponen significa que no hay una diferencia estadísticamente significativa entre las estimaciones correspondientes a cada intervalo. Es decir, la diferencia entre las estimaciones no es lo suficientemente grande como para ser considerada significativa desde un punto de vista estadístico. IMPORTANTE!! Es importante tener en cuenta que la superposición de los intervalos de confianza no es una prueba concluyente de que no hay una diferencia significativa entre las estimaciones. Se debe realizar una prueba de hipótesis para determinar si la diferencia es estadísticamente significativa o no. Sin embargo, la superposición de los intervalos de confianza puede ser una indicación inicial de que la diferencia no es significativa y que no se debe buscar más evidencia. 5.1.6 Ahora hazlo tú! Cree un .Rmd y realice lo siguiente: Utilizando la variable monto mínimo mensual que requiere su hogar para vivir (P08). Calcule: Brinde el estimador puntual de la media poblacional. Calcule el intervalo de confianza de la media poblacional. Calcule los intervalos de confianza de la media según si el individuo vive en el área urbana o rural (area2). En otras palabras, compare la media de la variable en esos dos grupos. Realice un gráfico de barras de error. Existe indicio de DIFERENCIA entre los dos grupos? Tienes 15 minutos! 5.2 Intervalo de una proporción 5.2.1 Definición En estadística, el intervalo de una proporción es un rango de valores posibles para la proporción de una característica en una población, que se estima a partir de una muestra aleatoria de la población. Al igual que con la media, el intervalo de una proporción se construye utilizando un nivel de confianza específico y se utiliza para determinar la precisión de la estimación de la proporción en la población. Por ejemplo, si se desea estimar la proporción de personas en una población que votará por un candidato específico, se puede seleccionar una muestra aleatoria de la población y estimar la proporción de personas que votarán por ese candidato en la muestra. A partir de esta estimación, se puede construir un intervalo de confianza que contendrá el valor real de la proporción en la población con un cierto nivel de confianza. El ancho del intervalo depende del tamaño de la muestra y del nivel de confianza especificado. A medida que el tamaño de la muestra aumenta, el intervalo se estrecha y se vuelve más preciso. Del mismo modo, a medida que se aumenta el nivel de confianza, el intervalo se amplía y se vuelve menos preciso. El intervalo de una proporción es una herramienta útil en la inferencia estadística, ya que permite a los investigadores cuantificar la incertidumbre en una estimación de la proporción y determinar si una diferencia entre dos proporciones es estadísticamente significativa. 5.2.2 Fórmula Recuerda que en este caso, al igual que en la media, todo gira en torno a los principios de la curva normal, el número de desviaciones estándar/errores estándar a la izquierda y a la derecha, el teorema central del límite y la ley de los grandes números. Si alguno de estos conceptos no están claros, te recomiendo regresar a la sesión 4 y repasarlos! Para calcular el intervalo de confianza de una proporción variamos un poco la fórmula que ya conocemos hasta ahora. Primero hay que tener en cuenta que cuando calculamos la proporción, nos estamos refiriendo específicamente a la proporción de UNA CATEGORÍA de una variable CATEGÓRICA. Hago el énfasis en ello porque siempre se genera la confusión de “a qué le estoy calculando la proporción”. Dicho de otra manera, nosotros debemos poner el ojo en una categoría de una variable nominal/ordinal al principio de este cálculo. \\[\\text{Intervalo de confianza para una proporción: } \\hat{p} \\pm z \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] Donde: \\(\\hat{p}\\) = Proporción muestral de la categoría elegida z = Puntuación crítica dependiendo de nuestro nivel de confianza elegido \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) = Error estándar de una proporción 5.2.3 IC de proporción con la fórmula Primero ubiquemos una variable categórica. Utilicemos la p11.1: ¿Qué tan desigual es el acceso de los peruanos a la EDUCACIÓN? 1. Muy desigual 2. Poco desigual 3. Nada desigual #Configuramos nuestra variable como factor enades$p11.1&lt;-factor(enades$p11.1, # Nombre de la variable a convertir levels=1:3, # Definimos los niveles (esta variable sólo tenía 2 niveles) labels=c(&quot;Muy desigual&quot;, &quot;Poco desigual&quot;, &quot;Nada desigual&quot;)) #Colocamos sus etiquetas La muestra se divide entre estas tres opciones. Ahora elegimos UNA CATEGORÍA de estas tres a la que vamos a calcular la proporción. En este caso vamos a elegir la proporción de individuos que afirmó que el acceso a la educación en el Perú es “Muy desigual”. #Calculamos p enades |&gt; count(p11.1) |&gt; mutate(p=n/sum(n)) ## # A tibble: 4 × 3 ## p11.1 n p ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Muy desigual 912 0.596 ## 2 Poco desigual 503 0.329 ## 3 Nada desigual 80 0.0523 ## 4 &lt;NA&gt; 35 0.0229 Ahora sabemos que nuestro \\(\\hat{p}\\) = 0.59 p&lt;-0.5961 El número total de casos es: n&lt;- 1530 Ahora definimos la puntuación crítica. Cuánto era para 95% de confianza? z&lt;-1.96 Ahora el error estándar \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) error_estandar&lt;- sqrt((p*(1-p))/n) Nuestro límite inferior es la proporción muestral menos z*error estándar: limite_inferior&lt;- p-z*error_estandar limite_inferior ## [1] 0.5715129 Y el límite superior es la proporción muestral más z*error estándar: limite_superior&lt;- p+z*error_estandar limite_superior ## [1] 0.6206871 Lo podemos colocar todo en un data frame resultados&lt;-data.frame(p, error_estandar, limite_inferior, limite_superior) resultados |&gt; kbl() |&gt; kable_styling() p error_estandar limite_inferior limite_superior 0.5961 0.0125444 0.5715129 0.6206871 5.2.4 IC de proporción con prop.test() Ahora bien, también podemos utilizar la función prop.test(): Recordemos: enades |&gt; count(p11.1) |&gt; mutate(p=n/sum(n)) ## # A tibble: 4 × 3 ## p11.1 n p ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Muy desigual 912 0.596 ## 2 Poco desigual 503 0.329 ## 3 Nada desigual 80 0.0523 ## 4 &lt;NA&gt; 35 0.0229 Colocamos la frecuencia de la categoría elegida y el tamaño total de la muestra. prop.test(912, 1530)$conf.int ## [1] 0.5709514 0.6207178 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Por qué colocamos frecuencia? Porque así está configurada esta función. 5.2.5 IC de proporción según grupos Ahora comparemos la proporción elegida (aquellos que creen que el acceso de los peruanos a la EDUCACIÓN es MUY DESIGUAL) entre el sector rural y urbano (area2). enades$area2&lt;- factor(enades$area2, # Nombre de la variable a convertir levels=1:2, # Definimos los niveles (esta variable sólo tenía 2 niveles) labels=c(&quot;Urbano&quot;, &quot;Rural&quot;)) #Colocamos sus etiquetas Primero, nos tenemos que preguntar cuál es la frecuencia en cada grupo enades |&gt; group_by(area2) |&gt; count(p11.1) |&gt; mutate(p=n/sum(n)) ## # A tibble: 8 × 4 ## # Groups: area2 [2] ## area2 p11.1 n p ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Urbano Muy desigual 769 0.604 ## 2 Urbano Poco desigual 409 0.321 ## 3 Urbano Nada desigual 67 0.0526 ## 4 Urbano &lt;NA&gt; 28 0.0220 ## 5 Rural Muy desigual 143 0.556 ## 6 Rural Poco desigual 94 0.366 ## 7 Rural Nada desigual 13 0.0506 ## 8 Rural &lt;NA&gt; 7 0.0272 enades |&gt; count(area2) ## # A tibble: 2 × 2 ## area2 n ## &lt;fct&gt; &lt;int&gt; ## 1 Urbano 1273 ## 2 Rural 257 Ahora calculamos la función teniendo en cuenta que: En el primer grupo hay 769 casos de éxito de un total de 1273. Lo ingresamos a la función: prop.test(x=c(769), n=c(1273))$conf.int ## [1] 0.5765493 0.6309849 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 En el segundo grupo hay 143 casos de éxito de un total de 257. prop.test(x=c(143), n=c(257))$conf.int ## [1] 0.4933505 0.6177753 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Lo podemos ordenar en un data.frame. resultados_prop&lt;- data.frame(Grupo =c(&quot;Urbano&quot;, &quot;Rural&quot;), P =c(0.60, 0.55), Lim_inf=c(0.57, 0.49), Lim_sup= c(0.63,0.61)) resultados_prop ## Grupo P Lim_inf Lim_sup ## 1 Urbano 0.60 0.57 0.63 ## 2 Rural 0.55 0.49 0.61 Lo visualizamos: resultados_prop |&gt; #Data ggplot()+ #Iniciamos el ggplot. A partir de ahora son +! ya no |&gt;! aes(y=p, x=Grupo)+ #Los grupos en el eje X y la media en el eje Y geom_errorbar(aes(ymin=Lim_inf, ymax=Lim_sup), width=0.2)+ #Graficamos la barra de error xlab(&quot;Procedencia&quot;) + #Etiqueta del eje X ylab(&quot;P la cobertura en educación es Muy Desigual&quot;) # Etiqueta del eje y Otra alternativa es pedirlos al mismo tiempo en la función prop.test(). En este caso lo que nos muestra, es el intervalo de la diferencia. prop.test(x=c(769, 143), n=c(1273, 257))$conf.int ## [1] -0.02108879 0.11641800 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 En este caso, se sigue la siguiente interpretación: Si el intervalo de confianza no contiene el valor cero, esto sugiere que las proporciones de los dos grupos son significativamente diferentes y que la hipótesis nula de que las proporciones son iguales debe ser rechazada. Por otro lado, si el intervalo de confianza contiene el valor cero, no podemos rechazar la hipótesis nula y no podemos concluir que las proporciones son significativamente diferentes. 5.2.6 Ahora hazlo tú! Cree un .Rmd y realice lo siguiente: Utilizando la variable “ideología2” (1: Izquierda, 2:Centro, 3:Derecha): Realice lo siguiente: Brinde el estimador puntual de la proporción de población de Izquierda en el Perú. Calcule el intervalo de confianza de la proporción poblacional de Izquierda en el Perú. Calcule por separado, utilizando prop.test(), la proporción de población de izquierda en el área urbano y rural (area2). Mediante la función prop.test(), compare los dos grupos y evalúe el intervalo de confianza de la diferencia en los p poblacionales. 5.3 Prueba de hipótesis: Una introducción 5.3.1 Definición Una prueba de hipótesis es un método para determinar si una afirmación sobre una población es verdadera o falsa, basándose en la información obtenida de una muestra aleatoria de la población. Este enfoque proporciona una forma sistemática y rigurosa para tomar decisiones informadas y hacer inferencias sobre la población en función de la evidencia empírica. Para aplicar una prueba debemos seguir pasos muy ordenados y entender qué estamos haciendo en cada uno de ellos. En términos referenciales, es como seguir un proceso de investigación policial! Vamos a utilizar esta analogía! 5.3.2 Paso 1: Establer hipótesis En este punto somos como detectives (como Benoit Blanc en Knives Out) y estas son nuestras hipótesis de investigación. Si vamos a investigar un crimen partimos con la H0 que la persona investigada es inocente. Nuestra hipótesis alterna es que es culpable! Las hipótesis son afirmaciones o suposiciones acerca de una población o de un conjunto de datos que se pretende analizar. En el contexto de las pruebas de hipótesis, se suelen plantear dos hipótesis: la hipótesis nula (H0) y la hipótesis alternativa (H1). La hipótesis nula establece que no hay diferencia significativa entre las muestras o poblaciones que se están comparando, mientras que la hipótesis alternativa indica que sí existe una diferencia significativa. Hipótesis Descripción Hipótesis nula (H0) Es una afirmación que se establece inicialmente y que se asume verdadera a menos que se disponga de suficiente evidencia para rechazarla. La hipótesis nula generalmente establece que no hay diferencia significativa entre dos grupos o variables que se están comparando, o que no hay efecto de un tratamiento o una intervención. Hipótesis alternativa (H1) Es la afirmación opuesta a la hipótesis nula y establece que sí hay una diferencia significativa entre los dos grupos o variables que se están comparando, o que sí hay un efecto de un tratamiento o intervención. La hipótesis alternativa se utiliza para probar lo que se espera demostrar si se rechaza la hipótesis nula. Tener en cuenta que la forma cómo planteamos nuestras hipótesis va a repercutir en todo el flujograma. 5.3.3 Paso 2: Verificar supuestos Antes de ver los hechos, debemos ver el contexto en el cual se encuentran. No es lo mismo que el crimen se realice en una casa o al aire libre. Entender previamente estas características nos permitirá investigar mejor! Es nuestra escena del crimen! Recuerdas todos los principios de la curva normal, desviaciones estándar, errores estándar (en el caso de las distribuciones muestrales), etc? Eso también se aplica aquí. Partimos de la idea de que si se extraen muchas muestras repetidas (como en la clase anterior), los estadísticos muestrales se centrarán alrededor de un valor con una distribución aproximadamente normal. En este caso debemos corroborar ciertas características de la distribución de nuestra muestra. Algunas de estas: Que las variables provengan de una distribución gaussiana o normal. Que las varianzas poblacionales sean iguales. Es decir, que la curva de distribución muestral en cada grupo analizado sea homogénea. Si son iguales, el camino de la investigación se dirigirá por un lado, si es diferente irá por otro. 5.3.4 Paso 3: Establecer nivel de significancia Cuando investigamos un crimen debemos cotejar el comportamiento del investigado frente a una actitud vista como normal o regular. Si encontramos que el investigado asistía a lugares muy fuera de su rutina puede ser un indicio! Como en el caso del intervalo de confianza, elegir el nivel de confianza con el que estamos trabajando es vital. El nivel de significancia en una prueba de hipótesis es la probabilidad máxima de que un resultado observado sea producto del azar, es decir, que no haya una diferencia real entre las muestras o poblaciones que se están comparando. Es un valor establecido previamente por el investigador antes de realizar la prueba de hipótesis y representa el umbral a partir del cual se considera que los resultados son estadísticamente significativos (o en nuestro caso, a partir de qué punto catalogamos un hecho como sospechoso). Por ejemplo, si se realiza una prueba de hipótesis con un nivel de significancia del 5%, esto significa que hay una probabilidad máxima del 5% de que los resultados observados se deban al azar, y que hay una diferencia real entre las muestras o poblaciones que se están comparando. En otras palabras, si el valor p obtenido en la prueba de hipótesis es menor que el nivel de significancia establecido, se rechaza la hipótesis nula y se concluye que hay evidencia suficiente para aceptar la hipótesis alternativa. Por ejemplo, si investigamos un robo y nos damos cuenta que el sospechoso, que había vivido toda su vida en Lima y nunca había viajado a ningún departamento, justamente había decidido mudarse a Europa a los pocos días de acontecido el robo. Es importante destacar que el nivel de significancia elegido puede tener un impacto en la interpretación de los resultados y que, en última instancia, es el investigador quien debe decidir el nivel adecuado en función del contexto de la investigación y del grado de incertidumbre que esté dispuesto a asumir. 5.3.5 Paso 4: Aplicar test estadístico y obtener p-value Ahora debemos recopilar evidencia! Tenemos que identificar el comportamiento extraño, pero para eso debemos saber cuál es el comportamiento normal. En estadística, el comportamiento normal nos lo entrega un test estadístico. Este nos brinda un estadístico de prueba, el cual va a ser la referencia con la cual vamos a medir la rareza de lo que encontramos en la práctica en nuestra data. Antes de seguir te pido que recuerdes esto: Ok, ahora sí. El test estadístico lo que te indica es cuál es el valor de umbral a la confianza elegida, pero en términos de EE (en el ejemplo de arriba, sería 20). Luego de eso, tomandolo como referencia, te dice qué tan raro es el estadístico OBSERVADO en tu muestra y te brinda la probabilidad de que eso ocurriese. Esa probabilidad es tu p-valor. En el ejemplo anterior, sabemos que 59 o 141 estaban dentro de ese otro 5% de probabilidad. Sin embargo, el test estadístico nos da la probabilidad exacta, esta probabilidad exacta se llama p-valor. 5.3.6 Paso 5: Tomar una decisión Ahora, de acuerdo a la evidencia. Debes tomar una decisión. Si el evento es demasiado raro entonces tienes que rechazar la hipótesis nula (es inocente). Si el evento es normal entonces NO puedes rechazar una hipótesis nula. Si el p-valor es menor que el nivel de significancia establecido (por ejemplo, α = 0.05), se rechaza la hipótesis nula (H0: Medias iguales). Si el p-valor es mayor que el nivel de significancia establecido NO rechazamos la hipótesis nula. Es importante tener en cuenta que el p-valor no proporciona información sobre la magnitud de la diferencia observada entre las muestras o poblaciones que se están comparando, sino simplemente sobre la probabilidad de que los resultados observados sean producto del azar. Además, la interpretación del p-valor debe contextualizarse adecuadamente y no se debe basar únicamente en la regla general del nivel de significancia establecido. OJO: EN TÉRMINOS ESTRICTOS NUNCA DECIMOS QUE ACEPTAMOS LA ALTERNATIVA. 5.3.7 Paso 6: Interpretación El paso final de la investigación es dar tu conclusión como detective. Entonces decimos que con un 95% podemos rechazar que el sospechoso sea inocente, por lo que lo presentamos al Poder Judicial para que un juez dictamine la sentencia. Cumplimos con nuestro trabajo. El paso final es proporcionar una interpretación de los resultados. Podemos utilizar el siguiente fraseo. SI EL P-VALOR &lt;0.05 A un 95% de confianza, obtuvimos un p-valor de _____, por lo que rechazamos la hipótesis nula de que _______ y concluimos que existe _______ estadísticamente significativa. SI EL P-VALOR &gt;=0.05 A un 95% de confianza, obtuvimos un p-valor de _____, por lo que NO rechazamos la hipótesis nula de que _______ y concluimos que NO existe _______ estadísticamente significativa. "],["comparación-en-dos-muestras-t.test-y-prop.html", "Sesión 6 Comparación en dos muestras (t.test y prop.test) 6.1 Recordemos 6.2 Diferencia de medias (t.test) 6.3 Diferencia de proporciones (prop.test)", " Sesión 6 Comparación en dos muestras (t.test y prop.test) 6.1 Recordemos 6.1.1 Prueba de hipótesis Habíamos dicho que una prueba de hipótesis estadística es una herramienta utilizada para evaluar si una afirmación sobre una población es probablemente verdadera o falsa, basándose en los datos de una muestra de esa población. Puedes pensar en una prueba de hipótesis como un juicio, en el cual el acusado es la hipótesis nula (afirmación a evaluar) y la hipótesis alternativa es la acusación. 6.1.2 Flujograma También habíamos establecido algunos pasos básicos para entender el proceso de utilización de una prueba de hipótesis. 6.1.3 Tipos de prueba de hipótesis Ahora bien, existen múltiples pruebas de hipótesis dependiendo de: Características de la data: Con fines de este curso, vamos a abordar un conjunto de pruebas de hipótesis que están construidas para tratar con datos que siguen una distribución normal. Este tipo de pruebas de hipótesis se le conoce como métodos paramétricos. En otras palabras, vamos a asumir que las variables que estamos eligiendo provienen de una distribución normal. Objetivo del investigador: Una prueba de hipótesis nos puede servir para evaluar muchas afirmaciones sobre la población. En este curso vamos a abordar algunas de las preguntas de investigación más conocidas: La media de esta variable numérica es distinta en dos grupos a nivel de la población? La media de esta variable numérica es ditinta en tres a más grupos a nivel de la población? Dos variables categóricas están asociadas en la población? Dos variables numéricas están asociadas en la población? Es posible modelar la relación entre una variable numérica (dependiente) y un conjunto de variables (independientes)? 6.2 Diferencia de medias (t.test) La prueba T para comparar una media en dos grupos es una técnica estadística que se utiliza para determinar si la diferencia entre las medias de dos grupos es estadísticamente significativa o simplemente el resultado del azar. La prueba T se basa en la distribución T de Student, que es una distribución de probabilidad que se utiliza cuando el tamaño de la muestra es pequeño o la varianza poblacional es desconocida. Será bastante común que en la descripción de una prueba de hipótesis se haga referencia a una distribución teórica que ayudará a establecer las probabilidades. Aquí está el core estadístico que sustenta la inferencia! 6.2.1 Pregunta de investigación Para iniciar, debemos tener claro nuestra pregunta que deseamos responder. ¿El monto mínimo promedio necesario para que un hogar pueda subsistir (p08) será diferente entre el área urbana y rural (area2) a nivel poblacional? 6.2.2 Paso 0: Análisis exploratorio de datos (EDA) No está en el flujograma, pero siempre debes seguir algunos pasos previos. Desde lo más general a lo más específico, nosotros debemos: Abrir la base de datos. Vamos a seguir usando ENADES. Puedes ver el cuestionario aqui: library(pacman) p_load(haven, tidyverse, lsr, kableExtra) enades&lt;-read_spss(&quot;data/ENADES_2022.sav&quot;) CONFIGURACIÓN ADECUADA DE LAS VARIABLES A UTILIZAR En este caso deseo comparar la variable monto mínimo mensual que requiere su hogar para vivir (p08)… enades$p08&lt;-as.numeric(enades$p08) str(enades$p08) ## num [1:1530] 20000 16000 15000 15000 10000 10000 10000 10000 10000 10000 ... …En los grupos establecidos por la variable ámbito (urbano/rural) enades$area2&lt;-factor(enades$area2, # Nombre de la variable a convertir levels=1:2, # Definimos los niveles (esta variable sólo tenía 2 niveles) labels=c(&quot;Urbano&quot;, &quot;Rural&quot;)) #Colocamos sus etiquetas str(enades$area2) ## Factor w/ 2 levels &quot;Urbano&quot;,&quot;Rural&quot;: 1 1 1 1 1 1 1 1 1 1 ... EXPLORACIÓN DE LOS ESTADÍSTICOS MUESTRALES Si bien nos interesa calcular la media poblacional, primero tenemos que tener en cuenta cuál es la media en nuestra muestra. Ahora hacemos un primer sondeo de cuál es la media muestral… enades |&gt; summarise(mean(p08, na.rm=T)) ## # A tibble: 1 × 1 ## `mean(p08, na.rm = T)` ## &lt;dbl&gt; ## 1 2334. …y cuál es la media muestral en cada uno de los grupos seleccionados que deseamos comparar. enades |&gt; group_by(area2) |&gt; summarise(mean(p08, na.rm=T)) ## # A tibble: 2 × 2 ## area2 `mean(p08, na.rm = T)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 Urbano 2490. ## 2 Rural 1497. Esto nos ayuda a corroborar que tenemos toda la información necesitaria lista para llevar a cabo una prueba de hipótesis. 6.2.3 Paso 1: Establecer hipótesis Debemos plantear las hipótesis nula y alternativa. Recuerda que cada prueba tiene su hipótesis nula, por lo que hay que memorizar algunas de estas. En el caso de la Prueba T, las hipótesis son las siguientes: Hipótesis Descripción Notación Hipótesis nula Las medias poblacionales son iguales \\(H_0: \\mu_1 = \\mu_2\\) Hipótesis alterna Las medias poblacionales son diferentes $H_1: _1 _2 $ Estas son las hipótesis que vamos a validar con nuestra prueba. 6.2.4 Paso 2: Verificar supuestos INDEPENDENCIA Las muestras deben ser independientes. El muestreo debe ser aleatorio. Vamos a asumir ello porque normalmente no tenemos control sobre el proceso de muestreo. DISTRITUCIÓN NORMAL Para los fines de este curso, asumimos que la variable numérica proviene de una distribución normal en la población. HOMOGENEIDAD DE VARIANZAS Identificamos si las varianzas son iguales en los dos grupos analizados. En el caso de que sean diferentes, vamos a necesitar hacer un **ajuste* a la fórmular de cálculo. Veamos antes una visualización de los datos: enades |&gt; ggplot() + aes(x = p08, fill = area2) + geom_density(alpha = 0.3) + scale_fill_manual(values = c(&quot;#00BFC4&quot;, &quot;#F8766D&quot;)) + labs(title = &quot;Distribución de variable p08 por grupo&quot;, x = &quot;Variable&quot;, y = &quot;Densidad&quot;) ## Warning: Removed 127 rows containing non-finite values (`stat_density()`). De forma predeterminada, vemos que la varianza, es decir cómo varian los datos en el área urbana es distinta a la rural (la mayoría de datos se concentra en un rango menor) Otra forma de compararlo, es a través de un boxplot: enades |&gt; ggplot() + aes(x = p08, fill = area2) + geom_boxplot() + scale_fill_manual(values = c(&quot;#00BFC4&quot;, &quot;#F8766D&quot;)) + labs(title = &quot;Distribución de variable p08 por grupo&quot;, x = &quot;Variable&quot;, y = &quot;Densidad&quot;) + coord_flip() ## Warning: Removed 127 rows containing non-finite values (`stat_boxplot()`). Esto lo corroboramos con el gráfico de cajas. El 50% superior en el caso urbano tiene un rango mucho mayor. Asimismo, podemos ver algunos valores extremos que se alejan a valores muy altos, en el caso del sector Urbano. Para fines de este ejercicio, vamos a considerar que las varianzas son diferentes en ambos grupos analizados. 6.2.5 Paso 3: Establecer nivel de significancia Estamos trabajando a un 95% de confianza, por lo que nuestro nivel de significancia será 0.05. \\[\\alpha = 0.05\\] 6.2.6 Paso 4: Calcular estadístico de prueba y p-valor Ahora utilizemos la función t.test() para realizar el cálculo del Estadístico T y el cálculo del p-valor. t.test(p08 ~ area2, #Colocamos la variable numérica y la variable grupo. Ese símbolo ~ (en Windows) es ALT + 126 alternative = &quot;two.sided&quot;, # Siempre que comparemos un estadístico en dos grupos usaremos &quot;two.sided&quot;. data = enades, # Precisamos la data var.equal=F) # Precisamos si las varianzas son iguales. En este caso colocamos F, porque identificamos diferencia ## ## Welch Two Sample t-test ## ## data: p08 by area2 ## t = 10.992, df = 466.14, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group Urbano and group Rural is not equal to 0 ## 95 percent confidence interval: ## 815.0508 1169.9052 ## sample estimates: ## mean in group Urbano mean in group Rural ## 2489.569 1497.091 El estadístico de prueba en este caso es el Estadítico T, el cual es 10.992. Ahora bien, lo que hace la prueba es comparar este valor observable del Estadístico T con un valor teórico, de acuerdo a la distribución T. Dicha comparación lo que arroja es una probabilidad de obtener ese valor observado, en caso la hipótesis nula (medias iguales) es verdadera En este caso, nuestro p-value es p-value &lt; 2.2e-16. El cual es un número muy pequeño. RECUERDA: El número 2.2e-16 es una forma abreviada de escribir un número muy pequeño en notación científica. En este caso, equivale a 0.00000000000000022 o 2.2 multiplicado por 10 elevado a la potencia -16. Es común utilizar esta notación en estadística y otras áreas de la ciencia para representar números muy grandes o muy pequeños de manera más concisa. 6.2.7 Paso 5: Tomar una decisión Tenemos los siguientes escenarios Resultado Decisión \\(p-value &lt;=\\alpha\\) Rechazamos la hipótesis nula. \\(p-value &gt;\\alpha\\) No rechazamos la hipótesis nula. En nuestro caso nuestro habíamos escogido un \\(\\alpha = 0.05\\) por lo que al obtener un p-valor de 2.2e-16 (0.00000000000000022) rechazamos la hipótesis nula de que nuestras medias poblacionales son iguales. En otras palabras, un p-valor de 0.00000000000000022 significa que hay una probabilidad muy baja de obtener los resultados observados en la muestra si la verdadera diferencia entre las medias poblacionales es cero (hipótesis nula). Es decir, si la hipótesis nula fuera verdadera y no hay diferencia real entre las medias poblacionales, la probabilidad de observar una diferencia tan grande o mayor entre las medias muestrales es muy baja, de solo 0.000002. 6.2.8 Paso 6: Interpretación Ahora bien, al finalizar este proceso debemos interpretar nuestros resultados. En este punto tienes que recordar que estamos trabajando con probabilidades, no existen certezas absolutas, por lo tanto, nuestra interpretación final debe considerar ello. Deberíamos concluir: Luego de realizar una prueba T, a un 95% de confianza, obtuvimos un p-valor de 2.2e-16, por lo que rechazamos la hipótesis nula de que el monto mínimo mensual que requiere su hogar para vivir es igual en el área urbana y en el área rural. Por ello, concluimos que existen diferencias estadísticamente significativas en ambos grupos poblacionales. 6.2.9 Ahora hazlo tú! Considera la variable p19 (ideología). Deseamos saber si existe diferencia en el promedio de esta variable en el sector rural y urbano. Realice una comparación de medias en ambos grupos utilizando la prueba de hipótesis más pertinente. Muestre sus resultados e interprete. 6.2.10 A tener en cuenta 6.2.10.1 Prueba de hipótesis para igualdad de varianzas Como te sugerí líneas arriba, siempre es bueno realizar una exploración visual con un boxplot para encontrar indicios sobre la existencia de homogeneidad de varianzas en dos o más grupos. Sin perjuicio de ello, para poder verificar este supuesto de homogenedidad de varianzas de forma más rigurosa podemos aplicar una Prueba de Levene de Homogeneidad de Varianzas. library(DescTools) LeveneTest(enades$p08, enades$area2) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 14.258 0.0001661 *** ## 1401 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Recordamos nuestras hipótesis a validar: Hipótesis Descripción Notación Hipótesis nula Las varianzas poblacionales son homogéneas \\(H_0: v_1 = v_2\\) Hipótesis alterna Las varianzas poblacionales son diferentes $H_1: v_1 v_2 $ Luego de realizar una prueba de Levene, a un 95% de confianza, obtuvimos un p-valor de 0.0001660599, por lo que rechazamos la hipótesis nula de que la varianza del monto mínimo mensual que requiere su hogar para vivir es igual/homogénea en el área urbana y en el área rural. Por ello, concluimos que existen diferencias estadísticamente significativas en las varianzas de ambos grupos poblacionales. 6.2.10.2 Valores faltantes en las encuestas En las encuestas, los valores perdidos son aquellos que faltan en las respuestas proporcionadas por los encuestados. Esto puede suceder porque el encuestado decidió no responder a la pregunta, porque no entendió la pregunta o porque la pregunta simplemente no se aplicaba a él o ella. Para manejar los valores perdidos, a menudo se utiliza la técnica de imputación, que implica reemplazar los valores faltantes por algún valor estimado. En algunos casos, se puede asignar un valor específico, como 99 o 999, para indicar que el encuestado no sabe o no responde a la pregunta. Este enfoque se utiliza comúnmente en encuestas de opinión pública y otras encuestas que involucran preguntas sensibles. Sin embargo, es importante tener en cuenta que asignar un valor específico a los valores perdidos puede afectar los resultados de la encuesta y la validez de los análisis estadísticos posteriores. Por lo tanto, es importante evaluar cuidadosamente las estrategias de manejo de valores perdidos y elegir la opción más adecuada para el conjunto de datos y el análisis específicos. 6.3 Diferencia de proporciones (prop.test) Así como existe una prueba de hipótesis para comparar medias, también existe una para comparar proporciones. La prueba de diferencia de proporciones (prop.test) en R es una prueba estadística que se utiliza para comparar dos proporciones en una muestra o en dos muestras independientes. Para ello, se utiliza la distribución teórica chi cuadrado. La hipótesis nula es que no hay diferencia entre las proporciones, mientras que la hipótesis alternativa es que hay una diferencia significativa entre las proporciones. Un ejemplo de su aplicación en ciencias sociales podría ser en una encuesta en la que se desea comparar la proporción de hombres y mujeres que apoyan una determinada candidatura política. Se tomarían dos muestras independientes, una de hombres y otra de mujeres, y se compararían las proporciones de apoyo a la candidatura en cada grupo mediante la prueba de diferencia de proporciones. Si se encontrara una diferencia significativa, se podría concluir que hay una diferencia en las proporciones de apoyo entre hombres y mujeres. Veámoslo con un ejemplo. 6.3.1 Pregunta de investigación Al igual que en prueba T, debemos tener claro nuestra pregunta que deseamos responder: ¿La proporción de personas que opinan que existe Mucha desigualdad económica en el país será diferente entre el área urbana y rural (area2) a nivel poblacional? 6.3.2 Paso 0: Análisis exploratorio de datos (EDA) Vamos a seguir usando ENADES. CONFIGURACIÓN ADECUADA DE LAS VARIABLES A UTILIZAR Vamos a hacer una primera exploración de la variable: summary(enades$p04) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.000 1.000 1.717 2.000 4.000 37 Vemos que sì tenemos valores perdidos. En este caso 37. table(enades$p04) # Cuando solicitas la tabla no te muestra los NA! ## ## 1 2 3 4 ## 868 265 275 85 No obstante, no tenemos problemas de casos que hayan contestado NS/NR (codificados con 99). Procedemos a convertirlo en factor: enades$p04&lt;-factor(enades$p04, # Nombre de la variable a convertir levels=1:4, # Definimos los niveles (esta variable sólo tenía 2 niveles) labels=c(&quot;Mucho&quot;, &quot;Algo&quot;, &quot;Poco&quot;, &quot;Nada&quot;)) #Colocamos sus etiquetas str(enades$p04) ## Factor w/ 4 levels &quot;Mucho&quot;,&quot;Algo&quot;,..: 3 1 NA 1 3 1 1 1 1 2 ... En los grupos seguimos teniendo la variable ámbito (urbano/rural). # enades$area2&lt;-factor(enades$area2, # Nombre de la variable a convertir # levels=1:2, # Definimos los niveles (esta variable sólo tenía 2 niveles) # labels=c(&quot;Urbano&quot;, &quot;Rural&quot;)) #Colocamos sus etiquetas str(enades$area2) ## Factor w/ 2 levels &quot;Urbano&quot;,&quot;Rural&quot;: 1 1 1 1 1 1 1 1 1 1 ... EXPLORACIÓN DE LOS ESTADÍSTICOS MUESTRALES Primero, veamos la proporción de encuestados que contestó cada categoría de la pregunta p04. Para eso, creamos una tabla simple: tabla_simple&lt;-table(enades$p04,enades$area2) #Se sugiere colocar el grupo como segunda variable. tabla_simple #Mostramos la tabla creada ## ## Urbano Rural ## Mucho 754 114 ## Algo 221 44 ## Poco 204 71 ## Nada 68 17 Luego, sobre la tabla creada, solicitamos algunos cambios. tabla_simple |&gt; # Esta es la tabla simple que creaste arriba prop.table(2) |&gt; # Que nos muestre la tabla en proporciones. El &quot;2&quot; es para pedir que sea por columnas. round(4) |&gt; # Que redondee los valores a 4 dígitos addmargins(1) |&gt; # Que la FILA de totales. Si deseamos la columna de totales entonces sería 2. kbl() |&gt; # Estos dos últimos son sólo para que se muestre en HTML (mejor formato). kable_styling() Urbano Rural Mucho 0.6047 0.4634 Algo 0.1772 0.1789 Poco 0.1636 0.2886 Nada 0.0545 0.0691 Sum 1.0000 1.0000 Damos cuenta que en el sector urbano el 60.47% sostuvo que existe Mucha desigualdad económica, mientras que en el sector rural sólo el 46.34%. EFECTIVAMENTE OBSERVAMOS UNA DIFERENCIA DE PROPORCIONES A NIVEL MUESTRAL. Ahora probemos nuestra hipótesis a nivel poblacional 6.3.3 Paso 1: Establecer hipótesis Debemos plantear las hipótesis nula y alternativa. En el caso del Prop.Test, las hipótesis son las siguientes: Hipótesis Descripción Notación Hipótesis nula Las proporciones poblacionales son iguales \\(H_0: p_1 = p_2\\) Hipótesis alterna Las proporciones poblacionales son diferentes \\(H_1: p_1 \\neq p_2\\) Estas son las hipótesis que vamos a validar con nuestra prueba. 6.3.4 Paso 2: Verificar supuestos INDEPENDENCIA Las muestras deben ser independientes. El muestreo debe ser aleatorio. Vamos a asumir ello porque normalmente no tenemos control sobre el proceso de muestreo. En este caso no corroboramos varianza homogénea debido a que nuestra variable de interés es categórica. 6.3.5 Paso 3: Establecer nivel de significancia Estamos trabajando a un 95% de confianza, por lo que nuestro nivel de significancia será 0.05. \\[\\alpha = 0.05\\] 6.3.6 Paso 4: Calcular estadístico de prueba y p-valor Ahora utilizemos la función prop.test() para realizar el cálculo del Estadístico Chi-Cuadrado y el cálculo del p-valor. Para ello, necesitamos dos datos. ¿Cuántos CASOS han contestado Mucho en la pregunta p04 en cada grupo (urbano y rural)? tabla_simple ## ## Urbano Rural ## Mucho 754 114 ## Algo 221 44 ## Poco 204 71 ## Nada 68 17 Ok, en el ámbito Urbano hay 754 y en el ámbito 114. ¿Cuántos CASOS hay en cada grupo (urbano y rural)? tabla_simple |&gt; addmargins() ## ## Urbano Rural Sum ## Mucho 754 114 868 ## Algo 221 44 265 ## Poco 204 71 275 ## Nada 68 17 85 ## Sum 1247 246 1493 Ok, tenemos 1247 para el área urbana y 246 para el área rural. Ahora sí, calculemos el prop.test: prop.test(x=c(754,114), #Número de personas que contestaron Mucho en el ámbito Urbano y Rural n=c(754+221+204+68, 144+44+71+17)) #Número de personas que hay en el el ámbito Urbano y Rural en mi muestra. ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(754, 114) out of c(754 + 221 + 204 + 68, 144 + 44 + 71 + 17) ## X-squared = 33.071, df = 1, p-value = 8.884e-09 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.1252801 0.2579353 ## sample estimates: ## prop 1 prop 2 ## 0.6046512 0.4130435 #TEN CUIDADO CON EL ORDEN! 6.3.7 Paso 5: Tomar una decisión Tenemos los siguientes escenarios Resultado Decisión \\(p-value &lt;=\\alpha\\) Rechazamos la hipótesis nula. \\(p-value &gt;\\alpha\\) No rechazamos la hipótesis nula. En nuestro caso nuestro habíamos escogido un \\(\\alpha = 0.05\\) por lo que al obtener un p-valor de 0.3785 no podemos rechazar la hipótesis nula de que nuestras proporciones poblacionales son iguales. 6.3.8 Paso 6: Interpretación Concluimos que: Luego de realizar una prueba de diferencia de proporciones, a un 95% de confianza, obtuvimos un p-valor de 0.3785, por lo que no podemos rechazar la hipótesis nula de que las proporciones de población que opinan que existe Mucha desigualdad económica son iguales en el área urbana y en el área rural. Por ello, concluimos que no existen diferencias estadísticamente significativas en ambos grupos. 6.3.9 Ahora hazlo tú! Considera las siguientes variables: ideologia3: Las categorías de esta variable son 1:Izquierda, 2:Centro y 3:Derecha. zona3: Las categoría de esta variable son 1:Lima Metropolitana y 2: Perú sin Lima Deseamos responder a la pregunta: ¿Existe una diferencia en la proporción de personas que se identifican con una ideología de derecha según su lugar de procedencia (Lima Metropolitana o resto del país)? Realice la prueba de hipótesis más pertinente. Muestre sus resultados e interprete. "],["anova-comparación-en-más-de-dos-muestras.html", "Sesión 7 ANOVA: Comparación en más de dos muestras 7.1 Recordemos 7.2 ANOVA de un factor 7.3 Identificar el/los grupos diferentes 7.4 Ahora hazlo tú!", " Sesión 7 ANOVA: Comparación en más de dos muestras 7.1 Recordemos 7.1.1 Flujograma Anteriormente habíamos establecido algunos pasos básicos para entender el proceso de utilización de una prueba de hipótesis. 7.1.2 Qué sigue? 7.2 ANOVA de un factor El análisis de varianza (ANOVA) de un factor en R es una técnica estadística utilizada para comparar la media de una variable numérica entre más de dos grupos. Se realiza un ANOVA de un factor cuando se tiene un solo factor categórico (con tres o más niveles) que se utiliza para agrupar los datos, y se quiere determinar si existe una diferencia estadísticamente significativa entre las medias de los grupos. El ANOVA es una herramienta útil para analizar datos experimentales en los que se desea comparar las medias de más de dos grupos. En lugar de realizar múltiples pruebas t de dos muestras, el ANOVA nos permite realizar una sola prueba para determinar si hay una diferencia significativa entre los grupos. Los pasos para aplicación del ANOVA son los mismos que los seguidos para el T de Student, visto la clase anterior. 7.2.1 Pregunta de investigación 7.2.2 Paso 0: Análisis exploratorio de datos (EDA) ABRIR LA DATA library(pacman) p_load(haven, tidyverse, lsr, kableExtra) enades&lt;-read_spss(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/ENADES_2022.sav&quot;) CONFIGURACIÓN ADECUADA DE LAS VARIABLES A UTILIZAR 7.2.2.1 Nuestra numérica: Índice aditivo En este caso deseamos realizar un índice aditivo de Percepción sobre la desigualdad en el acceso a servicios públicos. Para ello, vamos a utilizar estas cuatro variables: Tenemos que tener en cuenta que las variables de insumo son ordinales y tienen las siguientes categorías: 1. Muy desigual, 2. Poco desigual y 3. Nada Desigual. Veamos los descriptivos para cada variable: summary(enades$p11.1) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.000 1.000 1.443 2.000 3.000 35 table(enades$p11.1) ## ## 1 2 3 ## 912 503 80 summary(enades$p11.2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.000 1.000 1.316 2.000 3.000 33 table(enades$p11.2) ## ## 1 2 3 ## 1080 361 56 summary(enades$p11.3) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.000 1.000 1.442 2.000 3.000 45 table(enades$p11.3) ## ## 1 2 3 ## 885 544 56 summary(enades$p11.4) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.000 1.000 1.178 1.000 3.000 38 table(enades$p11.4) ## ## 1 2 3 ## 1274 170 48 Ok, hemos corroborado que no tenemos el problema de los NS/NR (999, 888, etc.) Vamos a recodificar cada variable para que el orden sea intuitivo enades&lt;-enades |&gt; mutate(p11.1n=case_when(p11.1==1~ 3, p11.1==2~2, p11.1==3~1, TRUE~NA_real_), p11.2n=case_when(p11.2==1~ 3, p11.2==2~2, p11.2==3~1, TRUE~NA_real_), p11.3n=case_when(p11.3==1~ 3, p11.3==2~2, p11.3==3~1, TRUE~NA_real_), p11.4n=case_when(p11.4==1~ 3, p11.4==2~2, p11.4==3~1, TRUE~NA_real_)) Corroboramos: table(enades$p11.1n) ## ## 1 2 3 ## 80 503 912 table(enades$p11.2n) ## ## 1 2 3 ## 56 361 1080 table(enades$p11.3n) ## ## 1 2 3 ## 56 544 885 table(enades$p11.4n) ## ## 1 2 3 ## 48 170 1274 Ahora sí podremos realizar el índice aditivo. enades&lt;-enades |&gt; mutate(indice_aditivo=p11.1n+p11.2n+p11.3n+p11.4n) enades |&gt; count(indice_aditivo) ## # A tibble: 10 × 2 ## indice_aditivo n ## &lt;dbl&gt; &lt;int&gt; ## 1 4 5 ## 2 5 3 ## 3 6 15 ## 4 7 35 ## 5 8 82 ## 6 9 145 ## 7 10 274 ## 8 11 337 ## 9 12 550 ## 10 NA 84 Como nuestras variables de insumo tenían como valores posibles 1-3, nuestro índice tendrá una escala posible de 4 (si la persona contestó a todo con 1) a 12 (si la persona contestó a todo con 3). Sobre este índice ya creado, podemos cambiarle la escala para que no sea de 4-12 sino de 0-8. enades&lt;- enades |&gt; mutate(indice_aditivo=indice_aditivo-4) enades |&gt; count(indice_aditivo) ## # A tibble: 10 × 2 ## indice_aditivo n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 5 ## 2 1 3 ## 3 2 15 ## 4 3 35 ## 5 4 82 ## 6 5 145 ## 7 6 274 ## 8 7 337 ## 9 8 550 ## 10 NA 84 7.2.2.2 Nuestros grupos enades$ambito&lt;-factor(enades$ambito, # Nombre de la variable a convertir levels=1:3, # Definimos los niveles (esta variable sólo tenía 2 niveles) labels=c(&quot;Lima Metropolitana&quot;, &quot;Perú urbano&quot;, &quot;Perú rural&quot;)) #Colocamos sus etiquetas table(enades$ambito) ## ## Lima Metropolitana Perú urbano Perú rural ## 536 737 257 EXPLORACIÓN DE DATOS MUESTRALES enades |&gt; group_by(ambito) |&gt; summarise(Media=mean(indice_aditivo, na.rm = T)) ## # A tibble: 3 × 2 ## ambito Media ## &lt;fct&gt; &lt;dbl&gt; ## 1 Lima Metropolitana 6.79 ## 2 Perú urbano 6.53 ## 3 Perú rural 6.60 7.2.3 Paso 1: Establecer hipótesis Debemos plantear las hipótesis nula y alternativa. Recuerda que cada prueba tiene su hipótesis nula, por lo que hay que memorizar algunas de estas. En el caso de la Prueba T, las hipótesis son las siguientes: Hipótesis Descripción Hipótesis nula Las medias poblacionales en todos los grupos son iguales Hipótesis alterna Al menos una de las medias de las grupos es diferente de las demás. Estas son las hipótesis que vamos a validar con nuestra prueba. 7.2.4 Paso 2: Verificar supuestos INDEPENDENCIA Las muestras deben ser independientes. El muestreo debe ser aleatorio. Vamos a asumir ello porque normalmente no tenemos control sobre el proceso de muestreo. DISTRITUCIÓN NORMAL Para los fines de este curso, asumimos que la variable numérica proviene de una distribución normal en la población. HOMOGENEIDAD DE VARIANZAS Para los fines de este curso, asumimos que la variable numérica posee homogeneidad de varianzas en cada grupo analizado. 7.2.5 Paso 3: Establecer nivel de significancia Estamos trabajando a un 95% de confianza, por lo que nuestro nivel de significancia será 0.05. \\[\\alpha = 0.05\\] 7.2.6 Paso 4: Calcular estadístico de prueba y p-valor Para calcular el ANOVA de un factor utilizamos la función aov(): anova1 = aov(enades$indice_aditivo~enades$ambito) summary(anova1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## enades$ambito 2 19 9.679 4.34 0.0132 * ## Residuals 1443 3218 2.230 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 84 observations deleted due to missingness En este caso, al igual que con las otras pruebas de hipótesis, se calcula un estadístico y se compara con una distribución teórica. 7.2.7 Paso 5: Tomar una decisión Tenemos los siguientes escenarios Resultado Decisión \\(p-value &lt;=\\alpha\\) Rechazamos la hipótesis nula. \\(p-value &gt;\\alpha\\) No rechazamos la hipótesis nula. Habíamos escogido un \\(\\alpha = 0.05\\) por lo que al obtener un p-valor de 0.0132 rechazamos la hipótesis nula de que nuestras medias poblacionales son iguales en todos los grupos. En otras palabras, un p-valor de 0.0132 significa que hay una probabilidad muy baja de obtener los resultados observados en la muestra si la verdadera diferencia entre las medias poblacionales entre grupos es cero (hipótesis nula). Esta probabilidad se encuentra por debajo de nuestro último nivel de rareza que estamos dispuestos a aceptar (0.05) por lo que rechazamos la H0. 7.2.8 Paso 6: Interpretación Ahora bien, al finalizar este proceso debemos interpretar nuestros resultados: Luego de realizar una prueba ANOVA, a un 95% de confianza, obtuvimos un p-valor de 0.0132, por lo que rechazamos la hipótesis nula de que la media del Índice de percepción de Desigualdad es igual en todos los ámbitos analizados (Lima Metropolitana, Perú urbano, Perú rural). Por ello, concluimos que existen diferencias estadísticamente significativas en alguno de los grupos poblacionales indicados. 7.3 Identificar el/los grupos diferentes 7.3.1 Pruebas post hoc: Test de Tukey Las pruebas de comparaciones múltiples son pruebas estadísticas que se utilizan para comparar múltiples grupos y determinar si hay diferencias significativas entre las medias de los grupos. En lugar de realizar varias pruebas t o ANOVA independientes para comparar cada par de grupos, las pruebas de comparaciones múltiples realizan todas las comparaciones de manera simultánea. La prueba Tukey, también conocida como prueba de rango múltiple de Tukey, es una prueba de comparaciones múltiples que se utiliza para determinar cuáles de los pares de grupos tienen medias significativamente diferentes. Esta prueba se basa en el rango de las medias de los grupos y en la estimación del error estándar de la media. La prueba de Tukey compara la diferencia entre las medias de cada par de grupos con una cantidad crítica que se basa en la variabilidad de los datos. Si la diferencia entre las medias de dos grupos es mayor que esta cantidad crítica, se considera que las medias son significativamente diferentes. Una vez que hemos creado nuestro objeto ANOVA (en nuestro caso llamado anova1), podemos solicitar la prueba Tukey sobre dicho objeto. comparacion = TukeyHSD(anova1) comparacion ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = enades$indice_aditivo ~ enades$ambito) ## ## $`enades$ambito` ## diff lwr upr p adj ## Perú urbano-Lima Metropolitana -0.25275128 -0.4561655 -0.04933707 0.0100916 ## Perú rural-Lima Metropolitana -0.19066752 -0.4671122 0.08577713 0.2382426 ## Perú rural-Perú urbano 0.06208376 -0.2030873 0.32725478 0.8468574 Para leer este resultado debemos preguntarnos dos cosas: ¿En qué parejas existe una diferencia significativa? Vemos el p-value. Estas se deben leer como Pruebas T calculada para cada pareja de grupos. ¿Cuáles eran las hipótesis que testeamos? Corrobora el resultado viendo el intervalo de la diferencia. Si el 0 está incluido en este intervalo, entonces dicha diferencia puede ser 0. Siempre hay una correspondencia: si el p valor no es significativo, el intervalo de la diferencia incluye al 0. 7.3.2 Visualización Exploramos el objeto que creamos arriba. diferencias_parejas = as.data.frame(comparacion[1]) diferencias_parejas ## enades.ambito.diff enades.ambito.lwr enades.ambito.upr enades.ambito.p.adj ## Perú urbano-Lima Metropolitana -0.25275128 -0.4561655 -0.04933707 0.01009162 ## Perú rural-Lima Metropolitana -0.19066752 -0.4671122 0.08577713 0.23824258 ## Perú rural-Perú urbano 0.06208376 -0.2030873 0.32725478 0.84685736 Hemos generado un dataframe con los valores de las diferencias estimadas en cada pareja y los límites inferiores y superiores de cada diferencia estimada. Ahora creamos una columna adicional en dicho data frame que tenga el detalle de la pareja. diferencias_parejas$parejas = rownames(diferencias_parejas) Ahora solicitamos un gráfico de barras de error utilizando la librería ggplot2. diferencias_parejas |&gt; ggplot()+ aes(x=parejas, y=enades.ambito.diff)+ geom_errorbar(aes(ymin=enades.ambito.lwr, ymax=enades.ambito.upr), width=0.2)+ geom_text(aes(label=paste(round(enades.ambito.diff, 1))), vjust=-1, size=3)+ xlab(&quot;Comparaciones&quot;) + ylab(&quot;Diferencia encontrada&quot;)+ ylim(-1, 1) + coord_flip() + geom_hline(yintercept = 0, color = &quot;red&quot;, linetype=&quot;dotted&quot;) + theme_classic() Con este gráfico debemos corroborar los resultados que arroja la prueba de comparaciones múltiples de Tukey. 7.4 Ahora hazlo tú! Considera las siguientes variables: En una escala de 1 a 10, ¿qué tan de acuerdo o en desacuerdo se encuentra con las siguientes afirmaciones respecto al Perú? - El Estado peruano debe implementar políticas firmes para reducir la desigualdad de ingresos entre ricos y pobres. ideologia2: Variable que distingue si la persona se identifica con una postura de izquierda, centro o derecha. Deseamos responder a la pregunta: ¿Existe una diferencia en el apoyo a un modelo intervencionista del Estado según la postura ideológica? Realice la prueba de hipótesis más pertinente. Muestre sus resultados e interprete. "],["ejercicio-de-repaso.html", "Sesión 8 EJERCICIO DE REPASO 8.1 Base de datos: LAPOP 8.2 Problema 8.3 Ejercicio 1: Índice de respaldo al sistema político 8.4 Ejercicio 2: Exploración 8.5 Ejercicio 3: IC de una media 8.6 Ejercicio 4: Comparación en dos grupos 8.7 Ejercicio 5: Comparación en más de dos grupos", " Sesión 8 EJERCICIO DE REPASO 8.1 Base de datos: LAPOP El Proyecto de Opinión Pública de América Latina (LAPOP) es un proyecto de investigación multinacional especializando en el desarrollo, implementación y análisis de encuestas de opinión pública. La encuesta mide los valores y comportamientos democráticos en el continente americano usando muestras probabilísticas nacionales de la población adulta en edad de votar. Puede visualizar el cuestionario de la encuesta 2018 aqui Puedes descargar la data aqui library(haven) library(tidyverse) lapop&lt;-read_dta(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/DATA_Peru/PERU_LAPOP__2018.dta&quot;) 8.2 Problema A Ud. lo han contratado para examinar la postura que tiene la población frente a las instituciones políticas en el país. Debido a la complejidad del problema, se hará un análisis inferencial de distintos niveles y para el cual se debe responder con la técnica estadística más adecuada. 8.3 Ejercicio 1: Índice de respaldo al sistema político Construya un índice sobre Respaldo al sistema político con las siguientes preguntas. Asegúrese que la escala del índice final sea de 0-1. Configure adecuadamente la variable TAMANO a fin de que existan dos grupos (cree una copia que se llame “PROCEDENCIA”): 1: Zona urbana (Capital Nacional/Ciudad Grande/Ciudad mediana/ Ciudad pequeña) 2: Zona rural (Área rural) 8.4 Ejercicio 2: Exploración Muestre la media del índice creado a nivel de la muestra. Asimismo, calcule la media por grupo según procedencia. Presente un gráfico que permita comparar la distribución de la variable en ambos grupos. Interprete. 8.5 Ejercicio 3: IC de una media Cálcule el IC al 95% de confianza del Índice a nivel poblacional. Asimismo, presente un gráfico donde compare los intervalos de confianza según cada grupo según procedencia. 8.6 Ejercicio 4: Comparación en dos grupos Compare de forma más rigurosa la media del índice en ambos grupos poblacionales según procedencia. Verifique el supuesto de homogeneidad de varianzas. Tenga en cuenta los pasos de la prueba de hipótesis. 8.7 Ejercicio 5: Comparación en más de dos grupos Compare la media del índice en los CINCO grupos originales de la variable TAMANO. Asuma homogeneidad de varianzas. Responda: Existen diferencia entre los grupos? En qué parejas se ha evidenciado diferencia estadísticamente significativa? Realice un gráfico comparativo de los resultados obtenidos. "],["asociación-de-dos-factores.html", "Sesión 9 Asociación de dos factores 9.1 Base de datos: LAPOP 9.2 Pregunta de investigaciòn 9.3 Identificando las dos variables categóricas 9.4 Tablas de contingencia 9.5 Gráficando: Barras y barras apiladas 9.6 Prueba χ² 9.7 Ahora hazlo tú!", " Sesión 9 Asociación de dos factores 9.1 Base de datos: LAPOP El Proyecto de Opinión Pública de América Latina (LAPOP) es un proyecto de investigación multinacional especializando en el desarrollo, implementación y análisis de encuestas de opinión pública. La encuesta mide los valores y comportamientos democráticos en el continente americano usando muestras probabilísticas nacionales de la población adulta en edad de votar. Puede visualizar el cuestionario de la encuesta 2018 aqui Puedes descargar la data aqui library(haven) library(kableExtra) library(tidyverse) lapop&lt;-read_dta(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/DATA_Peru/PERU_LAPOP__2018.dta&quot;) 9.2 Pregunta de investigaciòn El presente estudio tiene como objetivo investigar la posible asociación entre el ámbito de residencia (urbano, rural) y la asistencia a manifestaciones. Las manifestaciones públicas son eventos que reflejan la participación activa de la población en la expresión de demandas, opiniones o protestas sobre temas sociales, políticos o económicos. La diferenciación (brechas) entre los contextos urbanos y rurales puede influir en la participación y la motivación de las personas para asistir a manifestaciones. Se plantea la hipótesis de que existe una asociación entre el ámbito de residencia y la asistencia a manifestaciones. 9.3 Identificando las dos variables categóricas Identifique la variable prot3 “En los últimos 12 meses ha participado en una manifestación o protesta pública” y configure adecuadamente. table(lapop$prot3) ## ## 1 2 ## 217 1303 lapop$prot3&lt;-factor(lapop$prot3, # Nombre de la variable a convertir levels=1:2, # Definimos los niveles (esta variable sólo tenía 2 niveles) labels=c(&quot;Sí participó&quot;, &quot;No participó&quot;)) #Colocamos sus etiquetas table(lapop$prot3) ## ## Sí participó No participó ## 217 1303 Configure adecuadamente la variable TAMANO a fin de que existan dos grupos (cree una copia que se llame “AMBITO”): 1: Zona urbana (Capital Nacional/Ciudad Grande/Ciudad mediana/ Ciudad pequeña) 2: Zona rural (Área rural) lapop&lt;-lapop |&gt; mutate(ambito=case_when(tamano&lt;5~ 1, tamano==5~2, TRUE~NA_real_)) Convertimos a factor y asignamos niveles: table(lapop$ambito) ## ## 1 2 ## 1165 356 lapop$ambito&lt;-factor(lapop$ambito, # Nombre de la variable a convertir levels=1:2, # Definimos los niveles (esta variable sólo tenía 2 niveles) labels=c(&quot;Urbano&quot;, &quot;Rural&quot;)) #Colocamos sus etiquetas table(lapop$ambito) ## ## Urbano Rural ## 1165 356 9.4 Tablas de contingencia Una tabla de contingencia, también conocida como tabla de frecuencia cruzada, es una forma de organizar y resumir datos en el contexto de un estudio o investigación. Se utiliza para analizar la relación entre dos o más variables categóricas y mostrar cómo se distribuyen las observaciones en cada combinación de categorías. En una tabla de contingencia, las variables de interés se representan en filas y columnas, y se registran las frecuencias o recuentos de las observaciones que caen en cada combinación de categorías. Esto permite visualizar de manera clara y estructurada cómo se relacionan las variables y si existe alguna asociación o dependencia entre ellas. Las tablas de contingencia son especialmente útiles cuando se quiere examinar la relación entre variables categóricas, como género y preferencia política, ubicación geográfica y nivel educativo, o, como en este caso, el ámbito y si asiste a manifestaciones o no. Ahora bien, estas tablas pueden contener recuentos absolutos o frecuencias relativas, y a menudo se incluyen totales marginales que muestran las frecuencias totales de cada variable. Vamos por partes utilizando el ejemplo que hemos presentado en este ejercicio. 9.4.1 TC de frecuencias Esta tabla se compone de filas y columnas que representan las categorías de las dos variables en estudio. En cada celda de la tabla se registra la frecuencia o recuento de observaciones que pertenecen a una combinación específica de categorías. tabla_simple&lt;-table(lapop$prot3, lapop$ambito) tabla_simple ## ## Urbano Rural ## Sí participó 157 60 ## No participó 1008 295 Palabra clave: FRECUENCIA! A esta tabla simple se le puede agregar los totales marginales agregando addmargins. Podemos agregar ambos, tanto los marginales de columnas como filas. tabla_simple |&gt; addmargins() ## ## Urbano Rural Sum ## Sí participó 157 60 217 ## No participó 1008 295 1303 ## Sum 1165 355 1520 O uno de estos: tabla_simple |&gt; addmargins(1) ## ## Urbano Rural ## Sí participó 157 60 ## No participó 1008 295 ## Sum 1165 355 RECUERDA: 1 para filas y 2 para columnas!! 9.4.2 TC de proporciones En una tabla de contingencia de proporciones, en lugar de mostrar los recuentos absolutos de observaciones en cada casilla, se presentan las proporciones o porcentajes de cada categoría en relación al total de observaciones. Esta tabla de contingencia de proporciones proporciona una visión relativa de la distribución de las categorías y permite comparar las proporciones entre diferentes grupos o categorías. El uso de la tabla de contingencia de proporciones puede ser útil para identificar patrones o tendencias en los datos y facilita la interpretación de los resultados en términos de porcentajes relativos. Ahora bien, debemos tener cuidado cuando solicitemos los porcentajes porque debemos indicar si los porcentajes se calculan por columna o filas. Al utilizar la funciónprop.table() en R, el argumento margin se utiliza para especificar si se deben calcular las proporciones por filas o por columnas en una tabla de contingencia. La elección de este argumento es importante en el marco de un análisis de asociación de variables categóricas, ya que determina cómo se calcularán las proporciones y cómo se interpretarán los resultados. Si se selecciona margin = 1, se calcularán las proporciones por filas, lo que significa que las proporciones se calcularán dentro de cada fila de la tabla. En este caso, las proporciones se interpretan como la frecuencia de una categoría en relación con el total de observaciones en esa fila. tabla_simple |&gt; prop.table(1) ## ## Urbano Rural ## Sí participó 0.7235023 0.2764977 ## No participó 0.7735994 0.2264006 Por otro lado, si se selecciona margin = 2, se calcularán las proporciones por columnas, lo que implica que las proporciones se calcularán dentro de cada columna de la tabla. En este caso, las proporciones se interpretan como la frecuencia de una categoría en relación con el total de observaciones en esa columna. tabla_simple |&gt; prop.table(2) ## ## Urbano Rural ## Sí participó 0.1347639 0.1690141 ## No participó 0.8652361 0.8309859 La elección adecuada del argumento margin depende del objetivo del análisis y las preguntas de investigación que se deseen responder. Es importante tener claridad sobre qué aspecto de la asociación entre las variables categóricas se quiere resaltar y qué interpretación es más relevante en el contexto del análisis. 9.4.3 Variable independiente y dependiente? En una tabla de contingencia, ambas variables se consideran igualmente importantes y no se establece una relación de dependencia o causalidad entre ellas. Sin embargo, el investigador normalmente desea investigar la distribución de una variable categórica particular (FILAS) en una variable de grupo (COLUMNAS). En nuestro caso, la primera sería “prot3” y la de grupo “ámbito”. Por ello, se recomienda utilizar la siguiente tabla: tabla_simple |&gt; prop.table(2) |&gt; addmargins(1) ## ## Urbano Rural ## Sí participó 0.1347639 0.1690141 ## No participó 0.8652361 0.8309859 ## Sum 1.0000000 1.0000000 CONSIDERACIONES PARA LA LECTURA: Leer fila por fila y de derecha a izquierda, y se interpretan los porcentajes. Comenzamos por la primera fila de respuestas y analizamos si varían los porcentajes entre las categorías de la variable de grupo (la que está en columnas). En este caso vemos el porcentaje de personas que sí participó en manifestaciones es de 16.9% en el ámbito rural y de 13.47% en el ámbito urbano. Más diferencia de porcentajes en cada fila (entre las categorías de la variable en columnas), más fuerte es la relación entre las variables. Si no hay diferencias de respuestas, es decir, si los porcentajes en una misma fila son muy similares es que no hay relación entre las variables En este caso vemos una ligera asociación. Ahora la siguiente pregunta que debemos hacernos es si esa diferencia es estadísticamente significativa. Además de proporcionar un resumen descriptivo de los datos, las tablas de contingencia también son la base para realizar pruebas estadísticas como la prueba de chi cuadrado. En resumen, una tabla de contingencia es una herramienta que permite organizar y resumir los datos de variables categóricas, facilitando el análisis de la relación entre ellas y la realización de pruebas estadísticas para evaluar la asociación o dependencia. 9.5 Gráficando: Barras y barras apiladas Ahora bien, siempre es mejor contar con un gráfico. Para ello, convertimos nuestra tabla (de proporciones por columna) en un data frame (es la manera más simple). tabla_simple_p&lt;-tabla_simple |&gt; prop.table(2) |&gt; round(2) df &lt;- as.data.frame(tabla_simple_p) Calculamos el gráfico de barras simple. df |&gt; ggplot()+ aes(x=Var2, y=Freq, fill=Var1)+ geom_bar(position = &quot;dodge&quot;, stat=&quot;identity&quot;) + geom_text(aes(label=Freq), position = position_dodge(width = 0.9), vjust=0, size = 3)+ labs(x=&quot;Ámbito&quot;, y=&quot;Proporción&quot;, fill=&quot;Asistió a una protesta&quot;) Sin embargo, es mejor visualizar a través de barras apiladas. barras_apiladas&lt;-df |&gt; ggplot()+ aes(x=Var2, y=Freq, fill=Var1) + geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;)+ geom_text(aes(label=Freq), position = position_stack(), vjust=1, size = 3)+ labs(x=&quot;Ámbito&quot;, y=&quot;Porcentaje&quot;, fill=&quot;Asistió a una protesta&quot;) barras_apiladas Las ventajas son las siguientes: Las barras apiladas son un tipo de gráfico en el cual se representan las proporciones de distintas categorías en forma de barras que se apilan una encima de la otra. En lugar de mostrar los valores absolutos, se muestran los valores relativos en forma de porcentajes. La importancia de utilizar barras apiladas de porcentaje en la comparación y determinación de asociación de dos variables categóricas radica en que permite visualizar de manera clara las proporciones o frecuencias relativas de cada categoría en relación con la variable “independiente” (grupo) y si existe asociación. Al mostrar los valores en forma de porcentajes, se elimina la influencia del tamaño de la muestra, lo que facilita la comparación entre diferentes grupos o subgrupos, incluso si tienen tamaños de muestra diferentes. Si no se identifica asociación se ven barras muy similares. 9.6 Prueba χ² La prueba chi-cuadrado (χ²) es una prueba estadística utilizada para determinar si existe una asociación entre dos variables categóricas en una población. Recuerda: ASOCIACIÓN Indica si la distribución de frecuencias de una variable está relacionada de alguna manera con la distribución de frecuencias de la otra variable. El objetivo de la prueba chi-cuadrado (χ²) es evaluar si las diferencias observadas en la distribución de frecuencias entre las categorías de las variables son significativas o podrían haber ocurrido por azar. La prueba chi-cuadrado se basa en comparar las frecuencias observadas en una tabla de contingencia con las frecuencias que se esperarían si no hubiera ninguna asociación entre las variables. La idea es calcular un estadístico chi-cuadrado, el cual mide la discrepancia entre las frecuencias observadas y las esperadas bajo la hipótesis nula de independencia. Es una prueba ampliamente utilizada y de fácil interpretación, lo que la convierte en una herramienta valiosa en el análisis de datos categóricos. 9.6.1 Paso 0: Análisis exploratorio En este paso realizamos la configuración y exploración que ya hemos realizado líneas arriba. Es decir, configuración de cada variable, exploración y cálculo de las tablas de contingencia, tanto de frecuencias absolutas como de proporciones. tabla_simple_p |&gt; kbl() |&gt; kable_styling() Urbano Rural Sí participó 0.13 0.17 No participó 0.87 0.83 Así como el gráfico: barras_apiladas De forma preliminar vemos grandes diferencias? Podríamos coincidir que no es mucho. Por ello, hace falta un análisis más riguroso con un método inferencial. 9.6.2 Paso 1: Establecer hipótesis Debemos plantear las hipótesis nula y alternativa. Hipótesis Descripción Hipótesis nula No existe asociación Hipótesis alterna Sí existe asociación. Estas son las hipótesis que vamos a validar con nuestra prueba. 9.6.3 Paso 2: Verificar supuestos Para el caso de la prueba chi2, existe un requisito y es que deben existir al menos 5 casos en cada celda de la tabla de frecuencia original. Esta es una condición importante que se debe cumplir al realizar el cálculo de la prueba de chi-cuadrado (χ^2) en el análisis de tablas de contingencia. Este requisito se basa en consideraciones estadísticas y tiene como objetivo asegurar la validez y confiabilidad de los resultados obtenidos. La importancia de este requisito radica en que, cuando el número de observaciones en una celda es demasiado pequeño, puede haber problemas relacionados con la precisión y la estabilidad de los resultados del análisis de chi-cuadrado. Si hay muy pocas observaciones en una celda, es más probable que se obtengan estimaciones inexactas de las frecuencias esperadas y, por lo tanto, los resultados pueden no ser representativos de la verdadera relación entre las variables. En nuestro caso sí pasamos este requisito. tabla_simple ## ## Urbano Rural ## Sí participó 157 60 ## No participó 1008 295 9.6.4 Paso 3: Establecer nivel de significancia Estamos trabajando a un 95% de confianza, por lo que nuestro nivel de significancia será 0.05. \\[\\alpha = 0.05\\] 9.6.5 Paso 4: Calcular estadístico de prueba y p-valor Para calcular la prueba de hipótesis utilizamos la función chisq.test: prueba &lt;- chisq.test(tabla_simple) # Colocamos la tabla con las frecuencias simples prueba ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tabla_simple ## X-squared = 2.3357, df = 1, p-value = 0.1264 En este caso calculamos que el estadístico de prueba. Recuerda que como vimos en el Anova, lo que hacemos es comparar un indicador (estadístico de prueba) calculado con la variabilidad de nuestra muestra con un valor teórico. Cuáles son los valores teóricos (frecuencias esperadas) con lo que se compara los valores observados? prueba$expected ## ## Urbano Rural ## Sí participó 166.3191 50.68092 ## No participó 998.6809 304.31908 Y cómo sabemos que estos valores son los que se esperaría si no existe asociación? Porque si calculamos los porcentajes por columna nos indica que no hay diferencia en la distribución según la variable de grupo: prop.table(prueba$expected,2) ## ## Urbano Rural ## Sí participó 0.1427632 0.1427632 ## No participó 0.8572368 0.8572368 Fruto de esa comparación se calcula el χ² el cual es 2.3357. Ahora bien el pvalor obtenido es 0.1264. 9.6.6 Paso 5: Tomar una decisión Tenemos los siguientes escenarios Resultado Decisión \\(p-value &lt;=\\alpha\\) Rechazamos la hipótesis nula. \\(p-value &gt;\\alpha\\) No rechazamos la hipótesis nula. Habíamos escogido un \\(\\alpha = 0.05\\) por lo que al obtener un p-valor de 0.1264 no podemos rechazar la hipótesis nula de que no existe asociación en las variables elegidas. Vemos que a pesar de que visualizamos una diferencia en nuestra muestra, esta no es estadísticamente significativa. 9.6.7 Paso 6: Interpretación Ahora bien, al finalizar este proceso debemos interpretar nuestros resultados: Luego de realizar una prueba chi cuadrado, a un 95% de confianza, obtuvimos un p-valor de 0.1264, por lo que no podemos rechazar la hipótesis nula de que existe asociación entre la asistencia a manifestaciones y la procedencia de la persona. Por ello, concluimos que no existen asociación estadísticamente significativas en los grupos poblacionales indicados. 9.7 Ahora hazlo tú! Ahora realiza la comparación de la misma variable prot3 (En los últimos 12 meses ha participado en una manifestación o protesta pública) pero considerando como grupo la posición ideológica de la persona Para ello, utiliza la variable l1. Recodifique a fin de que los valores 1-4 sea “Izquierda”, 5-6 “Centro” y 7-10 “Derecha”. Siga cada uno de los pasos indicados y concluya si ambas variables están asociadas o no en la población. "],["midiendo-la-asociación-de-dos-numéricas-la-correlación.html", "Sesión 10 Midiendo la asociación de dos numéricas: La Correlación 10.1 Objetivo de la sesión 10.2 Recordando: Plano cartesiano y diagrama de dispersión 10.3 Varianza 10.4 Covarianza 10.5 Coeficiente de Correlación de Pearson 10.6 Validación con prueba de hipótesis 10.7 Matriz de correlación 10.8 Toma nota", " Sesión 10 Midiendo la asociación de dos numéricas: La Correlación 10.1 Objetivo de la sesión Explorar la relación entre dos variables numéricas. Específicamente vamos a entrar a una primera parte que es crucial para entender los temas posteriores, la cual consta de: Identificar si existe una relación lineal y reconocer el porqué. Medir la magnitud de esa relación. 10.2 Recordando: Plano cartesiano y diagrama de dispersión El plano cartesiano es un sistema de coordenadas que se utiliza para representar y visualizar puntos en un espacio bidimensional. Está compuesto por dos ejes perpendiculares, el eje horizontal o eje de las abscisas (X) y el eje vertical o eje de las ordenadas (Y). Estos ejes se cruzan en un punto llamado origen, que se representa con las coordenadas (0,0). Cada punto en el plano cartesiano se representa mediante un par ordenado (x, y), donde “x” indica la posición horizontal del punto a lo largo del eje X y “y” indica la posición vertical del punto a lo largo del eje Y. El plano cartesiano proporciona un marco de referencia visual que facilita la representación gráfica de datos, funciones matemáticas, relaciones y patrones geométricos, permitiendo el análisis y la interpretación de información en el contexto bidimensional. Vamos a utilizar la siguiente información: pais&lt;-c(&quot;Noruega&quot;,&quot;Chile&quot;, &quot;Italia&quot;, &quot;Peru&quot;, &quot;Yemen&quot;) democracy&lt;-c(9.8,8.2,7.7,5.9,2.0) corruption&lt;-c(84,67,56,36,17) data&lt;-data.frame(pais, democracy,corruption) data ## pais democracy corruption ## 1 Noruega 9.8 84 ## 2 Chile 8.2 67 ## 3 Italia 7.7 56 ## 4 Peru 5.9 36 ## 5 Yemen 2.0 17 Ahora lo visualizamos en un diagrama de dispersión, donde cada caso está representado por un punto en el plano: library(tidyverse) library(ggrepel) data |&gt; ggplot()+ aes(x=democracy, y=corruption, label=pais)+ geom_point() + geom_text_repel()+ labs(x=&quot;Índice de democracia&quot;, y=&quot;Índice de percepción de la Corrupción&quot;)+ theme_light() 10.3 Varianza Recordarás que en las primeras clases vimos distintos estadísticos que servían para medir la dispersión de los datos. Dispersión: Medida de cuánto se alejan los datos individuales de su valor central, lo cual proporciona información sobre la variabilidad de los datos. Una de estas medidas era la varianza, la cual se definía como: \\[ \\text{Var}(X) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] En resumen lo que hacíamos era calcular la suma de cuadrados de todas las diferencias (distancias) de las observaciones con la media de una determinada variable y, finalmente, calculábamos un promedio dependiendo del tamaño de la población. var(data$democracy) ## [1] 8.897 var(data$corruption) ## [1] 686.5 ¿Recuerdas cuáles era la principal desventaja de la varianza y cuál era la alternativa que teníamos? 10.4 Covarianza La covarianza es una medida que indica cómo varían dos variables de forma conjunta. Si vemos la fórmula vamos a encontrar algunas partes familiares: \\[ \\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) \\] Forma de interpretarla: Una covarianza de cero indica que no hay una relación lineal entre las variables. Mientras más alejada de 0 esté la covarianza (mientras el número en valor absoluto sea más grande) indica que las variables tienen una fuerte relación lineal y tienden a cambiar juntas en la misma dirección. Relación lineal: Esto significa que los valores de una variable aumentan o disminuyen en correspondencia con los valores de la otra variable. Si la covarianza es positiva, significa que las variables tienden a cambiar en la misma dirección. Si la covarianza es negativa, significa que las variables tienden a cambiar en direcciones opuestas. Recordemos ambos conceptos con unos ejemplos sencillos 10.5 Coeficiente de Correlación de Pearson 10.5.1 Definición La correlación es una medida estadística que describe la relación o asociación entre dos variables. Indica la fuerza y la dirección de la relación lineal entre las variables y se mide a través de un coeficiente denominado coeficiente de correlación de Pearson. El coeficiente de Pearson se llama así en honor a Karl Pearson, un estadístico británico que desarrolló este coeficiente de correlación en el siglo XIX. Karl Pearson fue una figura prominente en el campo de la estadística y realizó numerosas contribuciones a la teoría y aplicación de métodos estadísticos. La razón por la que explicamos antes el tema de la covarianza es que podríamos decir que la correlación es la covarianza de dos variables normalizada en una escala de -1 a 1. Para ello, se utiliza la siguiente fórmula: \\[ r = \\frac{{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}}{{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}} \\] Ojo con el numerador! Veamos el excel y calculemos 10.5.2 Interpretación El coeficiente puede tomar los valores en el rango de -1 a 1. Con este podemos identificar dos caracterìsticas de la relación: FUERZA: Mientras el valor del coeficiente se aleje más del 0 (sea más grande como valor absoluto) ello indicará una mayor correlación entre las dos variables numéricas. DIRECCIÓN: Cuando el coeficiente tiene signo positivo, ello indicará que la relación tiene sentido directo, es decir, mientras una variable aumenta, la otra aumenta. Si el signo es negativo, mientras una variable aumenta la otra disminuye. EJEMPLOS: Cuando el coeficiente de correlación es 1, existe una correlación positiva perfecta, lo que significa que a medida que una variable aumenta, la otra variable también lo hace de manera proporcional. Cuando el coeficiente de correlación es -1, hay una correlación negativa perfecta, lo que implica que a medida que una variable aumenta, la otra variable disminuye de manera proporcional. Si el coeficiente de correlación es cercano a 0, indica una correlación débil o inexistente entre las variables, lo que significa que no hay una relación lineal clara entre ellas. Ahora bien, normalmente no es común obtener -1, 1 o 0, sino diversos valores. Para ello, nos puede servir la escala de Cohen, la cual proporciona una escala para identificar un valor numérico con una magnitud de la correlaciòn. No obstante, esta es referencial, será común encontrar otras escalas dependiendo del campo de estudio. Es decir, para nuestro caso, aplicaríamos lo siguiente: 10.6 Validación con prueba de hipótesis La prueba de hipótesis en la correlación se utiliza para evaluar si existe una relación significativa entre dos variables continuas. Permite determinar si la correlación observada en una muestra es estadísticamente diferente de cero, lo que indicaría que existe una asociación entre las variables en la población subyacente. En términos más específicos, la prueba de hipótesis en la correlación se basa en el coeficiente de correlación de Pearson (r) para evaluar si la correlación en la muestra es significativamente diferente de cero. Se establecen una hipótesis nula (H0) que asume que no hay correlación en la población, y una hipótesis alternativa (H1) que sugiere que hay una correlación significativa. Al realizar la prueba de hipótesis, se calcula un valor de prueba (generalmente t o z) y se compara con un valor crítico basado en el nivel de significancia elegido. Con esa comparación, se concluye que hay evidencia suficiente para afirmar que existe una correlación significativa entre las variables. 10.6.1 Paso 1: Establecer hipótesis Debemos plantear las hipótesis nula y alternativa. Hipótesis Descripción Hipótesis nula No existe correlación lineal Hipótesis alterna Sí existe correlación lineal 10.6.2 Paso 2: Verificar supuestos DISTRITUCIÓN NORMAL Para los fines de este curso, asumimos que la variable numérica proviene de una distribución normal en la población. 10.6.3 Paso 3: Establecer nivel de significancia Estamos trabajando a un 95% de confianza, por lo que nuestro nivel de significancia será 0.05. \\[\\alpha = 0.05\\] 10.6.4 Paso 4: Calcular estadístico de prueba y p-valor Para calcular la prueba de hipótesis utilizamos la función cor.test: cor.test(data$democracy, data$corruption) ## ## Pearson&#39;s product-moment correlation ## ## data: data$democracy and data$corruption ## t = 6.8234, df = 3, p-value = 0.00644 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6005652 0.9980492 ## sample estimates: ## cor ## 0.9692606 10.6.5 Paso 5: Tomar una decisión Tenemos los siguientes escenarios Resultado Decisión \\(p-value &lt;=\\alpha\\) Rechazamos la hipótesis nula. \\(p-value &gt;\\alpha\\) No rechazamos la hipótesis nula. Habíamos escogido un \\(\\alpha = 0.05\\) por lo que al obtener un p-valor de 0.00644 rechazamos la hipótesis nula de que no existe correlación lineal en las variables elegidas. 10.6.6 Paso 6: Interpretación Ahora bien, al finalizar este proceso debemos interpretar nuestros resultados: Luego de realizar una prueba de correlación, a un 95% de confianza, obtuvimos un p-valor de 0.00644, por lo que rechazamos la hipótesis nula de que existe correlaicón entre el nivel de percepción de la corrupción y el índice de democracia. Por ello, concluimos que sí una correlación estadísticamente significativa en las variables indicadas. 10.7 Matriz de correlación Una matriz de correlación es una tabla cuadrada que muestra las correlaciones entre múltiples variables. En esta matriz, cada celda representa la correlación entre dos variables, y la diagonal principal de la matriz generalmente se compone de correlaciones perfectas (1.0 o -1.0) ya que muestra la correlación de una variable consigo misma. La matriz de correlación es una herramienta útil en el análisis de datos multivariable, ya que proporciona una visión general de las relaciones de asociación entre todas las variables en estudio. Permite identificar patrones y tendencias de correlación entre variables y proporciona información sobre la fuerza y la dirección de las relaciones lineales entre ellas. La matriz de correlación es una herramienta importante para explorar la estructura de las relaciones entre variables y puede utilizarse para tomar decisiones informadas en el análisis de datos, como seleccionar variables para modelos de regresión, identificar variables colineales o evaluar la asociación entre conjuntos de variables. Probemos con el siguiente ejemplo: library(gapminder) data3&lt;-gapminder Primero, debemos contar con una subdata en la cual se encuentren solo las numéricas que deseamos correlacionar. Debemos quitar el resto de variables de tipo factor que no entren en el análisis. Ojo, esto sólo es para el cálculo de la matriz y la visualización de la misma. data3&lt;-data3 |&gt; select(4:6) head(data3,5) ## # A tibble: 5 × 3 ## lifeExp pop gdpPercap ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 28.8 8425333 779. ## 2 30.3 9240934 821. ## 3 32.0 10267083 853. ## 4 34.0 11537966 836. ## 5 36.1 13079460 740. Calculamos la matriz de correlación. matrix_cor&lt;-cor(data3) matrix_cor |&gt; round(2) ## lifeExp pop gdpPercap ## lifeExp 1.00 0.06 0.58 ## pop 0.06 1.00 -0.03 ## gdpPercap 0.58 -0.03 1.00 Podemos visualizarlo de forma conjunta: library(corrplot) ## corrplot 0.92 loaded corrplot(matrix_cor, method=&quot;color&quot;, type=&quot;upper&quot;) #Prueba cambiando el método. F1 Ten en cuenta que la forma de solicitar el gráfico con el paquete corrplot puede variar. Puedes personalizar la matriz de la forma en que más se te acomode. Para mayor detalle, visita la página: https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html 10.8 Toma nota 10.8.1 Correlación no implica causalidad El principio de “correlación no implica causalidad” es un concepto fundamental en estadística y metodología de investigación que establece que el hecho de que dos variables estén correlacionadas entre sí no significa necesariamente que exista una relación causal directa entre ellas. En otras palabras, solo porque dos variables muestren una asociación estadística, no se puede concluir automáticamente que una variable cause los cambios en la otra. Un ejemplo comúnmente citado para ilustrar este principio es la relación entre el consumo de helado y el número de casos de ahogamiento en piscinas. Estos dos fenómenos pueden estar correlacionados, es decir, puede haber una asociación estadística entre ellos. Durante los meses de verano, tanto el consumo de helado como el uso de piscinas aumentan. Por lo tanto, si se analizan los datos, es posible encontrar una correlación positiva entre la cantidad de helado consumido y el número de casos de ahogamiento en piscinas. Sin embargo, sería incorrecto concluir que el consumo de helado causa los casos de ahogamiento en piscinas. En realidad, ambos fenómenos están influenciados por un factor común, que es la temporada de verano. El aumento en el consumo de helado y el uso de piscinas se debe a las altas temperaturas y el clima cálido propios del verano, y no a una relación causal directa entre ambas variables. Este ejemplo ilustra cómo dos variables pueden estar correlacionadas sin que exista una relación causal entre ellas. Para establecer una relación causal, es necesario realizar estudios más rigurosos que consideren otros factores y utilicen diseños de investigación adecuados, como experimentos controlados o análisis de series temporales. Te invito a que visites esta página web con un conjunto de correlaciones espurias interesantes: https://www.tylervigen.com/spurious-correlations Las correlaciones espurias pueden surgir cuando dos variables están relacionadas indirectamente a través de un tercer factor común, lo que crea la ilusión de una asociación directa entre ellas. Estas asociaciones pueden ser engañosas si no se consideran cuidadosamente los factores confusores o variables de control en el análisis. 10.8.2 Cuidado: Scatterplot como primer paso SIEMPRE Vamos a crear la siguiente data: # Sólo con fines de ejemplo. peq = function(x) x^3+2*x^2+5 x = seq(-0.99, 1, by = .01) y = peq(x) + runif(200) df = data.frame(x = x, y = y) head(df,5) ## x y ## 1 -0.99 6.362188 ## 2 -0.98 6.857719 ## 3 -0.97 6.618252 ## 4 -0.96 6.639328 ## 5 -0.95 6.486032 Ahora, vamos a generar nuestro cor.test entre las dos variables. Qué concluimos? cor.test(df$x,df$y) ## ## Pearson&#39;s product-moment correlation ## ## data: df$x and df$y ## t = 7.4609, df = 198, p-value = 2.649e-12 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3526249 0.5701341 ## sample estimates: ## cor ## 0.4684478 library(tidyverse) # df |&gt; # ggplot()+ # aes(x=x, y=y)+ # geom_point() Mientras que una prueba de correlación lineal, como el coeficiente de correlación de Pearson, mide la relación lineal entre variables, puede haber relaciones no lineales entre las variables. Un diagrama de dispersión permite observar patrones más complejos y no lineales, como relaciones parabólicas, curvas en forma de S u otras formas. Si aplicamos un cor.test sin analizar previamente las variables podríamos llegar a conclusiones equivocadas El coeficiente de correlación de Pearson es una medida estadística que evalúa la relación lineal entre dos variables. Sin embargo, puede haber situaciones en las que exista una relación no lineal entre las variables, pero el coeficiente de Pearson aún pueda ser significativo y mostrar una correlación fuerte. Un escenario es el ejemplo mostrado en el que existe una relación monótona. Una relación monótona es aquella en la que las variables aumentan o disminuyen juntas, pero no necesariamente de manera lineal. En estos casos, el coeficiente de Pearson aún puede mostrar una correlación fuerte porque refleja la tendencia general de las variables, a pesar de que la relación subyacente no sea lineal. 10.8.3 Valores extremos La presencia de valores extremos en los datos puede afectar el resultado del cor.test y, en algunos casos, cambiar la interpretación de la correlación. El coeficiente de correlación de Pearson, que se calcula mediante cor.test en R, es sensible a los valores extremos debido a su influencia en la covarianza entre las variables. Los valores extremos pueden distorsionar la relación entre las variables y afectar la magnitud y significancia del coeficiente de correlación. Cuando se presentan valores extremos, existen algunas consideraciones a tener en cuenta: Influencia en la magnitud de la correlación: Un valor extremo atípico puede tener un impacto desproporcionado en la correlación, especialmente si está alejado de la tendencia general de los datos. Dependiendo de la dirección y magnitud del valor atípico, puede aumentar o disminuir la correlación observada. Por lo tanto, es importante tener en cuenta que una correlación fuerte obtenida a partir de un cor.test puede ser influenciada por valores extremos. Influencia en la significancia estadística: Los valores extremos pueden aumentar la varianza y la covarianza en los datos, lo que a su vez puede afectar la significancia estadística del coeficiente de correlación. En presencia de valores atípicos, el p-valor asociado al cor.test puede cambiar y volverse más o menos significativo, dependiendo del efecto de los valores extremos en la correlación. Por lo tanto, es importante identificar y evaluar la influencia de los valores extremos en el análisis de correlación y considerarlos al interpretar los resultados. Si los valores extremos son atípicos y no representativos de la población o el fenómeno en estudio, se pueden considerar métodos alternativos de análisis, como el uso de técnicas robustas o el análisis de subgrupos sin los valores extremos. 10.8.4 Tamaño de la muestra El tamaño de la muestra puede afectar la interpretación de la correlación. Las correlaciones pueden ser más confiables y representativas cuando se basan en muestras grandes en lugar de muestras pequeñas. Es importante considerar la confiabilidad estadística de la correlación al interpretar los resultados. "],["regresión-lineal-simple-explicar-y-predecir.html", "Sesión 11 Regresión lineal simple: explicar y predecir 11.1 Objetivo de la sesión 11.2 Pregunta de investigación 11.3 Recordando 11.4 Introducción a la Regresión Lineal Simple 11.5 Matrícula escolar: un modelo utilizando RLS", " Sesión 11 Regresión lineal simple: explicar y predecir 11.1 Objetivo de la sesión Introducir a los estudiantes en el concepto y la aplicación de la regresión lineal, que es una técnica estadística utilizada para modelar y analizar la relación entre una variable dependiente y una variable independiente. Se abordarán los siguientes temas: Concepto de regresión lineal Modelado Interpretación de los resultados 11.2 Pregunta de investigación Qué factores explican la variabilidad en el porcentaje anual de matrícula escolar a nivel secundaria por país? Contamos con las siguientes variables: Porcentaje de inscripción escolar neta por país PBI per cápita por país Porcentaje de la población que se encuentra por debajo de la línea de pobreza por país Porcentaje de población urbana por país Porcentaje de población femenina por país Porcentaje de inversión del Estado en Educación por país Proporción de estudiantes con relación al número de maestros por país Porcentaje de jóvenes entre 15-19 años que son consumidores de alcohol por país Tasa de natalidad adolescente (por 1000 mujeres de 15 a 19 años) pos país library(haven) library(tidyverse) library(rio) matricula&lt;-import(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/DATA_internacional/matricula.xlsx&quot;) colnames(matricula)&lt;-c(&quot;pais&quot;,&quot;matricula&quot;,&quot;pbiPC&quot;,&quot;pobreza&quot;,&quot;urbano&quot;,&quot;gastoeducacion&quot;,&quot;ratio_prof&quot;,&quot;alcohol&quot;,&quot;natalidad_ado&quot;) 11.3 Recordando 11.3.1 Coeficiente de Correlación de Pearson Habíamos visto que la correlación es una medida estadística que describe la relación o asociación entre dos variables. Indica la fuerza y la dirección de la relación lineal entre las variables y se mide a través de un coeficiente denominado coeficiente de correlación de Pearson. El coeficiente puede tomar los valores en el rango de -1 a 1. Con este podemos identificar dos caracterìsticas de la relación: FUERZA: Mientras el valor del coeficiente se aleje más del 0 (sea más grande como valor absoluto) ello indicará una mayor correlación entre las dos variables numéricas. DIRECCIÓN: Cuando el coeficiente tiene signo positivo, ello indicará que la relación tiene sentido directo, es decir, mientras una variable aumenta, la otra aumenta. Si el signo es negativo, mientras una variable aumenta la otra disminuye. Es decir, para nuestro caso, aplicaríamos lo siguiente: 11.3.2 ¡Ahora hazlo tú! En base a la data presentada vamos a considerar que deseamos explicar el fenómeno de la “Porcentaje de inscripción escolar neta por país”. head(matricula,5) ## pais matricula pbiPC pobreza urbano gastoeducacion ratio_prof alcohol natalidad_ado ## 1 Afghanistan 50.14 551.87 54.5 26.0 4.06 33.50124 0.3 62.0 ## 2 Albania 86.61 5223.81 14.3 62.1 2.47 11.19927 37.7 15.9 ## 3 Angola 11.29 3437.30 36.6 66.8 3.42 26.76619 33.5 163.0 ## 4 Argentina 90.80 11687.60 25.7 92.1 5.46 9.54885 54.5 54.4 ## 5 Armenia 87.74 4212.13 32.0 63.3 2.71 8.01689 16.3 21.2 Para ello vamos a: Calcular las correlaciones individuales entre “matricula” y el resto de variables, utilizando cor.test(). Rápidamente mencionen si encontraron correlaciones significativas. # cor.test(matricula$matricula, matricula$pbiPC) # cor.test(matricula$matricula, matricula$pobreza) # cor.test(matricula$matricula, matricula$gastoeducacion) # cor.test(matricula$matricula, matricula$ratio_prof) # cor.test(matricula$matricula, matricula$alcohol) # cor.test(matricula$matricula, matricula$natalidad_ado) Calcule la matriz de correlaciones y genere un gráfico. Qué podría afirmar a nivel general de la data presentada? Realizamos la matríz de correlaciones: matricula2&lt;-matricula %&gt;% select(2:9) # Creamos subdata matriz&lt;- cor(matricula2) #Calculamos la matriz matriz %&gt;% round(2) # La solicitamos con un redondeo a 2 decimales ## matricula pbiPC pobreza urbano gastoeducacion ratio_prof alcohol natalidad_ado ## matricula 1.00 0.54 -0.73 0.60 0.29 -0.67 0.66 -0.84 ## pbiPC 0.54 1.00 -0.45 0.56 0.36 -0.41 0.77 -0.53 ## pobreza -0.73 -0.45 1.00 -0.53 -0.18 0.49 -0.49 0.70 ## urbano 0.60 0.56 -0.53 1.00 0.29 -0.55 0.62 -0.49 ## gastoeducacion 0.29 0.36 -0.18 0.29 1.00 -0.16 0.32 -0.26 ## ratio_prof -0.67 -0.41 0.49 -0.55 -0.16 1.00 -0.53 0.58 ## alcohol 0.66 0.77 -0.49 0.62 0.32 -0.53 1.00 -0.53 ## natalidad_ado -0.84 -0.53 0.70 -0.49 -0.26 0.58 -0.53 1.00 Como puede ser un poco tedioso, podemos calcular un gráfico. library(corrplot) corrplot(matriz, method=&quot;number&quot;,insig = &quot;blank&quot;) 11.4 Introducción a la Regresión Lineal Simple La regresión lineal simple es una técnica estadística que busca modelar la relación entre dos variables: una variable dependiente y una variable independiente. La idea principal es encontrar una línea recta que mejor se ajuste a los datos observados, de manera que se pueda predecir o estimar el valor de la variable dependiente en función del valor de la variable independiente. Vamos por partes: 11.4.1 Variable dependiente y variable independiente A diferencia de la correlación, en este caso sí afirmamos que existe dependencia entre las variables. En estadística, los términos “variable dependiente” y “variable independiente” se utilizan para describir las diferentes categorías de variables en un estudio o análisis. Estas categorías se basan en la relación y el papel que desempeñan en el análisis estadístico. La variable dependiente es aquella que se estudia o analiza para determinar cómo es afectada o influenciada por otras variables. También se conoce como variable de respuesta o variable de interés. En un análisis de regresión, por ejemplo, la variable dependiente es la que se trata de predecir o explicar a partir de otras variables. Es común representarla en el eje vertical de un gráfico de dispersión o en la fórmula del modelo estadístico con el símbolo Y. Por otro lado, las variables independientes son aquellas que se consideran como posibles explicaciones o factores que pueden influir en la variable dependiente. También se conocen como variables predictoras o variables explicativas. Estas variables se manipulan o se observan en el estudio con el propósito de analizar su relación con la variable dependiente. En un análisis de regresión, las variables independientes son utilizadas para predecir o explicar los cambios en la variable dependiente. Se suelen representar en el eje horizontal de un gráfico o en la fórmula del modelo estadístico con la letra X. RECUERDA La variable dependiente es la que se estudia o analiza para determinar su relación con otras variables, mientras que las variables independientes son las que se utilizan para explicar o predecir los cambios en la variable dependiente. 11.4.2 Flashback: Ecuación de la recta En la regresión lineal simple, se asume que la relación entre las variables puede ser aproximada por una línea recta en el espacio bidimensional. Para ello debemos acordarnos algunos elementos básicos: \\[ Y = \\beta_0 + \\beta_1X \\] Donde: Y es la variable dependiente que se quiere predecir o estimar. X es la variable independiente que se utiliza para predecir Y. β₀ es la intersección de la línea (ordenada al origen). β₁ es la pendiente de la línea (indica cuánto varía Y por cada unidad de X). Tener en cuenta que si: Si β₁ es positivo, Y aumenta cuando X aumenta.Es una relación directa / positiva. Si β₁ es negativo, Y aumenta cuando X disminuye.Es una relación inversa / negativa. Si β₁ es cero.Y no cambia cuando X varía. No existe relación entre las variables. EJEMPLO Para calcular la ecuación nos preguntamos: ¿En qué punto la línea cruza el eje Y? (intercepto) Vemos que la recta atraviesa el eje Y en el número 1 Por cada unidad de X, ¿cuánto varía el valor de Y? (pendiente) Cada vez que X aumenta en una unidad, Y varía en dos unidades Entonces nuestra ecuación será: \\[ Y = 1 + 2X \\] Realicemos algunos ejercicios!!! 11.4.3 La mejor recta para la nube de puntos Ahora bien, utilizando el concepto de la ecuación lineal de la recta, podemos entender mejor lo que hace una regresión. El propósito de la regresión lineal es encontrar una línea que se ajuste mejor a la nube de puntos que habíamos identificado anteriormente en el gráfico de dispersión. Para ello definimos un modelo que agrega un elemento más a la ecuación de la recta que ya habíamos visto antes: En este caso vemos que al último ha aparecido un nuevo elemento, el error aleatorio. Formalmente debemos decir que ε es el término de error que representa la variación no explicada por el modelo. En otras palabras, el ε es la acumulación de las distancias de los puntos a la línea recta construida. 11.4.4 La Regresión Lineal El objetivo de la regresión lineal simple es estimar los coeficientes β₀ y β₁ a partir de los datos observados, de manera que la línea ajustada se aproxime lo mejor posible a los valores reales de la variable dependiente. Esta estimación se realiza utilizando el método de mínimos cuadrados, que busca minimizar la suma de los errores al cuadrado entre los valores observados y los valores predichos por el modelo (dónde antes habías visto la necesidad de elevar al cuadrado una resta? Por qué y para qué?). Una vez que se ha ajustado el modelo de regresión lineal simple, se pueden realizar inferencias y hacer predicciones sobre los valores de la variable dependiente para diferentes valores de la variable independiente. Regresión Lineal Simple Técnica que permite modelar y analizar la relación lineal entre dos variables, proporcionando una forma de estimar y predecir valores de la variable dependiente en función de la variable independiente. 11.4.5 Calidad de ajuste: R2 El coeficiente de determinación (R²) en regresión es una medida estadística que indica la proporción de la varianza de la variable dependiente (Y) que puede explicarse por la variable independiente (X) en el modelo de regresión. En otras palabras, el R² representa la cantidad de variabilidad en los valores de Y que es capturada por el modelo de regresión. El valor de R² varía entre 0 y 1. Un R² de 0 significa que el modelo no explica ninguna variabilidad en Y, mientras que un R² de 1 indica que el modelo explica toda la variabilidad en Y. En general, un valor de R² más cercano a 1 indica un mejor ajuste del modelo y una mayor capacidad de explicar la variabilidad observada en la variable dependiente. Para nuestros fines seguiremos utilizando de referencia la escala de Cohen: Sin embargo, es importante tener en cuenta que el R² por sí solo no indica la validez o la relevancia del modelo de regresión. Es necesario considerar otros aspectos, como la significancia estadística de los coeficientes, los supuestos del modelo y el contexto del problema, para evaluar completamente la calidad del ajuste del modelo. 11.4.6 Significancia estadística de los coeficientes La significancia estadística de los coeficientes en un modelo de regresión se refiere a la pregunta de si los coeficientes estimados son estadísticamente diferentes de cero o no. En otras palabras, se evalúa si hay evidencia suficiente en los datos para afirmar que la relación entre la variable independiente y la variable dependiente es real y no se debe simplemente al azar. Para ello, se realizan pruebas de hipótesis por cada uno de los coeficientes calculados. En este caso, al ser regresión lineal sólo tenemos un coeficiente (β₁). Por ello, planteamos las siguientes hipótesis: Hipótesis Descripción Hipótesis nula La variable X1 no aporta al modelo propuesto Hipótesis alterna La variable X1 sí aporta al modelo propuesto Para determinar la significancia estadística, se utiliza un valor de p, que indica la probabilidad de obtener un resultado igual o más extremo que el observado, asumiendo que la hipótesis nula es verdadera (la hipótesis nula generalmente establece que no hay relación entre las variables). Si el valor de p es menor que un umbral predeterminado (por ejemplo, 0.05), se considera que el coeficiente es estadísticamente significativo, lo que significa que hay suficiente evidencia para rechazar la hipótesis nula y concluir que el coeficiente es diferente de cero. En resumen, la significancia estadística de los coeficientes nos permite determinar si los efectos estimados en el modelo son estadísticamente confiables y si podemos inferir que existe una relación real entre las variables independientes y la variable dependiente en la población de interés. 11.4.7 La RLS como herramienta predictiva La regresión lineal no solo se utiliza para explicar la relación entre una variable dependiente y una o más variables independientes, sino que también puede ser una herramienta poderosa para la predicción. A través del modelo de regresión lineal ajustado, es posible utilizar las variables independientes para predecir el valor de la variable dependiente en nuevos casos o situaciones. Una vez que se ha ajustado el modelo de regresión lineal con datos de entrenamiento, se puede utilizar este modelo para realizar predicciones en datos no vistos. Para ello, se ingresan los valores de las variables independientes en el modelo y se obtiene una estimación del valor de la variable dependiente correspondiente. Esta capacidad predictiva puede ser especialmente útil en diversas aplicaciones, como pronósticos económicos, estimación de ventas, predicción de precios, análisis de tendencias, entre otros. Sin embargo, es importante tener en cuenta las limitaciones y supuestos del modelo de regresión lineal, así como la calidad de los datos utilizados para el ajuste del modelo, ya que esto puede afectar la precisión y validez de las predicciones. Es fundamental realizar una evaluación adecuada de la calidad del modelo y validar las predicciones utilizando técnicas como la validación cruzada o la división de datos en conjuntos de entrenamiento y prueba. Además, en algunos casos, puede ser necesario utilizar técnicas más avanzadas, como modelos de regresión no lineal o modelos de regresión con variables transformadas, para obtener predicciones más precisas en situaciones donde la relación entre las variables no es lineal. 11.5 Matrícula escolar: un modelo utilizando RLS 11.5.1 Paso 0: Identificar claramente la variable dependiente y la variable independiente DEPENDIENTE INDEPENDIENTE Matrícula escolar Pobreza 11.5.2 Paso 1: Explorar previamente la relación de las variables Veamos nuevamente lo encontrado en la exploración realizada hace unos momentos. En este caso centrémonos en la relación entre Matrícula Escolar y Nivel de Pobreza. matricula |&gt; ggplot()+ aes(x=pobreza, y=matricula)+ geom_point() cor.test(matricula$matricula, matricula$pobreza) ## ## Pearson&#39;s product-moment correlation ## ## data: matricula$matricula and matricula$pobreza ## t = -10.16, df = 93, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8086310 -0.6134162 ## sample estimates: ## cor ## -0.7252916 Interpretemos de acuerdo a lo visto en la clase anterior. 11.5.3 Paso 2: Planteamos el modelo y verificamos si es válido con ANOVA Para generar el modelo podemos utilizar la función lm(). Te sugiero que revises la documentación del R para que sepas qué argumentos utiliza esta función. modelo1 &lt;- lm(matricula~pobreza, data=matricula) Ahora probamos su validez con la técnica ANOVA. El análisis de varianza (ANOVA) se utiliza en la regresión lineal para evaluar la significancia estadística del modelo en su conjunto y determinar si hay una relación significativa entre las variables independientes y la variable dependiente. En el contexto de la regresión lineal, el ANOVA compara la variabilidad explicada por el modelo (la suma de cuadrados de la regresión) con la variabilidad no explicada (la suma de cuadrados del error). Si la variabilidad explicada es significativamente mayor que la variabilidad no explicada, esto indica que el modelo es estadísticamente significativo y que las variables independientes tienen un impacto significativo en la variable dependiente. El ANOVA proporciona información sobre la calidad general del modelo, indicando si el modelo en su conjunto es útil para predecir o explicar la variable dependiente. Hipótesis Descripción Hipótesis nula El modelo de regresión no es válido Hipótesis alterna El modelo de regresión es válido anova(modelo1) ## Analysis of Variance Table ## ## Response: matricula ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## pobreza 1 33036 33036 103.22 &lt; 2.2e-16 *** ## Residuals 93 29765 320 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 En este caso, al obtener un p-valor menor al alpha (0.05) podemos rechazar la hipótesis nula, concluyendo que nuestro modelo sí es válido. Ojo, si revisas la bibliografía estadística es probable que no encuentres la hipótesis nula planteada de la manera expuesta, sino más bien como “los coeficientes son distintos de 0”. 11.5.4 Paso 3: Nivel explicativo del modelo Habíamos dicho que el R2 indica la proporción de la varianza de la variable dependiente (Y) que puede explicarse por la variable independiente (X) en el modelo de regresión. summary(modelo1) ## ## Call: ## lm(formula = matricula ~ pobreza, data = matricula) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.873 -12.695 3.438 12.483 34.519 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 98.8995 3.5680 27.72 &lt;2e-16 *** ## pobreza -1.1130 0.1096 -10.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.89 on 93 degrees of freedom ## Multiple R-squared: 0.526, Adjusted R-squared: 0.521 ## F-statistic: 103.2 on 1 and 93 DF, p-value: &lt; 2.2e-16 Hemos obtenido un R2 de 0.526 por lo que concluimos: Un coeficiente de determinación (r cuadrado) de 0.52 indica que aproximadamente el 52% de la variabilidad de la variable dependiente puede ser explicada por las variables independientes incluidas en el modelo de regresión. Esto significa que alrededor del 52% de las fluctuaciones en los valores observados de la variable dependiente pueden ser atribuidas a las variables independientes utilizadas en el modelo. Cuanto más cercano esté el valor de r cuadrado a 1, mayor será la proporción de la variabilidad explicada por el modelo y mejor será el ajuste del modelo a los datos. Es importante tener en cuenta que un r cuadrado de 0.52 también implica que aproximadamente el 48% de la variabilidad de la variable dependiente no está explicada por las variables independientes incluidas en el modelo. Esta variabilidad restante puede ser atribuida a otros factores no considerados en el modelo o al azar!! Siguiendo nuestro intervalo, diremos que el modelo tiene un ALTO nivel explicativo. 11.5.5 Paso 4: La variable x aporta al modelo En este paso exploramos la significancia de los coeficientes de las X. En este caso al tener sólo una independiente (Pobreza), sólo tenemos un coeficiente. Hipótesis Descripción Hipótesis nula La variable X1 no aporta al modelo propuesto Hipótesis alterna La variable X1 sí aporta al modelo propuesto En este caso, para la variable Pobreza, al obtener un p-valor menor al alpha (0.05) podemos rechazar la hipótesis nula, concluyendo que efectivamente SÍ aporta al modelo para explicar el nivel de matrícula escolar en el mundo. 11.5.6 Paso 5: Construimos la ecuación del modelo creado \\[ Matrícula = 98.8995 + -1.1130 * Pobreza \\] La ecuación de regresión permite predecir los valores de la variable dependiente para nuevos valores de la variable independiente. Puedes usar la ecuación para estimar cómo cambiará la variable dependiente en función de los cambios en la variable independiente. Esto es especialmente útil en problemas de pronóstico o en la estimación de resultados futuros Podemos generar un gráfico interesante utilizando la extensión de ggplot2 ggpmisc. library(ggpmisc) ## Loading required package: ggpp ## ## Attaching package: &#39;ggpp&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## annotate ## Registered S3 method overwritten by &#39;ggpmisc&#39;: ## method from ## as.character.polynomial polynom matricula |&gt; ggplot()+ aes(x=pobreza, y=matricula) + stat_poly_line(se = FALSE) + stat_poly_eq(use_label(c(&quot;eq&quot;, &quot;R2&quot;)),label.x = &quot;right&quot;) + stat_fit_deviations(colour = &quot;red&quot;)+ geom_point() https://cran.r-project.org/web/packages/ggpmisc/readme/README.html "],["regresión-lineal-múltiple-más-de-una-independiente.html", "Sesión 12 Regresión lineal múltiple: más de una independiente 12.1 Objetivo de la sesión 12.2 Regresión Lineal Múltiple 12.3 Pregunta de investigación 12.4 Explicando y prediciendo la desigualdad: un modelo utilizando RLM 12.5 Comparando modelos: Tabla 12.6 Ahora hazlo tú!", " Sesión 12 Regresión lineal múltiple: más de una independiente 12.1 Objetivo de la sesión Se abordarán los siguientes temas: Concepto de regresión lineal múltiple Modelado Interpretación de los resultados Predicción 12.2 Regresión Lineal Múltiple La regresión lineal múltiple es una técnica estadística que busca modelar la relación entre una variable de interés (variable dependiente) y varias variables predictoras (variables independientes). Se basa en la suposición de que la variable dependiente puede ser aproximada por una combinación lineal de las variables predictoras, con un término de error aleatorio. 12.2.1 Ecuación En la regresión lineal múltiple, se estima una ecuación de la forma: \\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\epsilon \\] Donde: Y es la variable dependiente que se desea predecir. X₁, X₂, …, Xₚ son las variables predictoras. β₀, β₁, β₂, …, βₚ son los coeficientes de regresión que representan la contribución de cada variable predictora. ε es el término de error aleatorio, que captura la variabilidad no explicada por las variables predictoras. El objetivo de la regresión lineal múltiple es encontrar los valores óptimos para los coeficientes de regresión que minimicen la diferencia entre los valores observados y los valores predichos. Esto permite obtener un modelo que pueda utilizarse para realizar predicciones y entender la relación entre las variables involucradas. 12.2.2 Mayor complejidad para visualizar la relación La visualización de una regresión lineal múltiple puede ser más compleja debido a la presencia de múltiples variables predictoras. En un gráfico bidimensional, solo se pueden representar dos variables a la vez, por lo que se pierde la capacidad de mostrar las relaciones entre todas las variables predictoras y la variable dependiente simultáneamente. Además, las representaciones gráficas tradicionales, como los diagramas de dispersión, se vuelven más complicadas cuando hay más de dos variables, ya que no se puede representar fácilmente en un plano. Para abordar esta complejidad, se pueden utilizar enfoques alternativos como gráficos de pares (scatterplot matrix) para visualizar las relaciones entre pares de variables predictoras y la variable dependiente. También se pueden utilizar técnicas de visualización más avanzadas, como gráficos tridimensionales o visualizaciones interactivas, que permiten explorar las relaciones entre múltiples variables de manera más compleja. 12.3 Pregunta de investigación ¿Qué factores explican la desigualdad de género en el país? Contamos con las siguientes variables: library(tidyverse) library(rio) data&lt;-import(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/DATA_internacional/desigualdad.xlsx&quot;) 12.4 Explicando y prediciendo la desigualdad: un modelo utilizando RLM 12.4.1 Paso 0: Identificar claramente la variable dependiente y laS variables independientes DEPENDIENTE INDEPENDIENTES Desigualdad de género Voz Política y Libertad de Movimiento 12.4.2 Paso 1: Explorar previamente la relación de las variables Entre Desigualdad de género y Voz Política data |&gt; ggplot()+ aes(x=VozPolitica, y=DesigualdadGenero)+ geom_point() cor.test(data$DesigualdadGenero, data$VozPolitica) ## ## Pearson&#39;s product-moment correlation ## ## data: data$DesigualdadGenero and data$VozPolitica ## t = -4.7629, df = 119, p-value = 5.434e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.5400608 -0.2386883 ## sample estimates: ## cor ## -0.400137 Ahora con Desigualdad de género y Libertad de Movimiento data |&gt; ggplot()+ aes(x=LibertadMov, y=DesigualdadGenero)+ geom_point() cor.test(data$DesigualdadGenero, data$LibertadMov) ## ## Pearson&#39;s product-moment correlation ## ## data: data$DesigualdadGenero and data$LibertadMov ## t = -5.9398, df = 119, p-value = 2.902e-08 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.6050563 -0.3276799 ## sample estimates: ## cor ## -0.4782063 Te acuerdas que te había dicho que las relaciones entre más de 2 variables se volvía más compleja de analizar y visualizar? Realicemos un gráfico 3d de la relación de las tres variables escogidas. library(plotly) plot_ly(x = data$DesigualdadGenero, y = data$VozPolitica, z = data$LibertadMov, text=rownames(data),type = &quot;scatter3d&quot;, mode = &quot;markers&quot;) Este gráfico sólo se puede aplicar con tres variables! 12.4.3 Paso 2: Planteamos el modelo y verificamos si es válido con ANOVA Para generar el modelo podemos utilizar la función lm(). modelo1 &lt;- lm(DesigualdadGenero~VozPolitica + LibertadMov, data=data) Ahora probamos su validez con la técnica ANOVA. El ANOVA proporciona información sobre la calidad general del modelo, indicando si el modelo en su conjunto es útil para predecir o explicar la variable dependiente. Hipótesis Descripción Hipótesis nula El modelo de regresión no es válido Hipótesis alterna El modelo de regresión es válido #summary(modelo1) En este caso, al obtener un p-valor menor al alpha (0.05) podemos rechazar la hipótesis nula, concluyendo que nuestro modelo sí es válido. Ojo, si revisas la bibliografía estadística es probable que no encuentres la hipótesis nula planteada de la manera expuesta, sino más bien como “los coeficientes son distintos de 0”. 12.4.4 Paso 3: Nivel explicativo del modelo Habíamos dicho que el R2 indica la proporción de la varianza de la variable dependiente (Y) que puede explicarse por la variable independiente (X) en el modelo de regresión. Ello lo podemos visualizar en el mismo output: Habíamos aprendido a ver el R2 Múltiple, en ese caso nos indicaría que un 31% de la variabilidad de la Desigualdad de Género es explicada por la Voz Política y la Libertad de Movimiento. Sin embargo, en este caso se utiliza el R2 ajustado: Siguiendo nuestro intervalo, diremos que el modelo tiene un BAJO nivel explicativo, daod que el R2 ajustado es de 0.2995 (29.95% de la variabilidad de Y es explicada por nuestras X´s) 12.4.5 Paso 4: La variable x aporta al modelo En este paso exploramos la significancia de los coeficientes de las X. En este caso al tener dos independientes (Voz Política y Libertad de Movimiento), sólo tenemos un coeficiente. Hipótesis Descripción Hipótesis nula La variable X1 no aporta al modelo propuesto Hipótesis alterna La variable X1 sí aporta al modelo propuesto En este caso, tanto para la variable Voz Política como Libertad de Movimiento, al obtener un p-valor menor al alpha (0.05) podemos rechazar la hipótesis nula, concluyendo que efectivamente que SÍ aportan poder explicativo al modelo para explicar el nivel de desigualdad de género en el mundo. 12.4.6 Paso 5: Construimos la ecuación del modelo creado Entonces siguiendo la fórmula de la Regresión Lineal Múltiple: \\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 \\] Reemplazamos con los coeficientes calculados: Entonces siguiendo la fórmula de la Regresión Lineal Múltiple: \\[ DesigualdadGenero = 0.96 + (-0.005*VozPolitica) + (-0.007*LibertadMov) \\] \\[ DesigualdadGenero = 0.96 - (0.005*VozPolitica) - (0.007*LibertadMov) \\] La ecuación de regresión permite predecir los valores de la variable dependiente para nuevos valores de la variable independiente. 12.4.7 Paso 6: Predecir Calculemos un vector para generar nuestro valor predicho utilizando la ecuación. Compáralo con el vector de fitted.values que es calculado en el objeto “modelo1”. Son iguales? 12.5 Comparando modelos: Tabla El paquete stargazer en R es una herramienta útil para comparar modelos de regresión lineal múltiple de manera concisa y organizada. Proporciona una forma conveniente de generar tablas resumidas que muestran los resultados de varios modelos de regresión en una presentación fácil de leer. Algunas de las utilidades del paquete “stargazer” son: Comparación de modelos: Permite comparar los resultados de diferentes modelos de regresión lineal múltiple en una sola tabla. Esto facilita la identificación de las diferencias en los coeficientes, errores estándar, estadísticas de ajuste y otros estadísticos de interés entre los modelos. Personalización de la presentación: El paquete ofrece varias opciones de formato y estilo para personalizar la apariencia de la tabla generada. Es posible ajustar el alineamiento, agregar títulos, etiquetas y notas a las tablas, lo que facilita la presentación de los resultados de manera clara y ordenada. Exportación a diferentes formatos: El resultado de “stargazer” se puede exportar a diferentes formatos, como HTML, LaTeX, texto o incluso Excel, lo que facilita su inclusión en informes, presentaciones o documentos científicos. También lo puedes incluir en tu trabajo final! Para ello primero creamos nuestros modelos, en este caso, una RLS y dos RLM: modelo1 &lt;- lm(DesigualdadGenero~VozPolitica, data=data) modelo2 &lt;- lm(DesigualdadGenero~VozPolitica + LibertadMov, data=data) modelo3 &lt;- lm(DesigualdadGenero~VozPolitica + LibertadMov + CuentaF, data=data) library(stargazer) stargazer(modelo1,modelo2,modelo3, type=&quot;text&quot;) ## ## ============================================================================================ ## Dependent variable: ## ------------------------------------------------------------------------ ## DesigualdadGenero ## (1) (2) (3) ## -------------------------------------------------------------------------------------------- ## VozPolitica -0.007*** -0.005*** -0.003*** ## (0.001) (0.001) (0.001) ## ## LibertadMov -0.008*** -0.002** ## (0.002) (0.001) ## ## CuentaF -0.005*** ## (0.0004) ## ## Constant 0.516*** 0.960*** 0.867*** ## (0.039) (0.094) (0.057) ## ## -------------------------------------------------------------------------------------------- ## Observations 121 121 121 ## R2 0.160 0.311 0.753 ## Adjusted R2 0.153 0.299 0.747 ## Residual Std. Error 0.186 (df = 119) 0.169 (df = 118) 0.101 (df = 117) ## F Statistic 22.685*** (df = 1; 119) 26.649*** (df = 2; 118) 118.950*** (df = 3; 117) ## ============================================================================================ ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Cuál es el mejor modelo? Por qué? Ahora bien, si lo que deseamos es incluirlo en nuestro paper o artículo de investigación, debemos importarlo con un mejor formato. Para ello, convertimos dicha tabla en código HTML. En nuestro caso, lo leemos con otro paquete especializado para ello: library(htmltools) HTML(stargazer(modelo1,modelo2,modelo3, type=&quot;html&quot;)) ## ## &lt;table style=&quot;text-align:center&quot;&gt;&lt;tr&gt;&lt;td colspan=&quot;4&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td colspan=&quot;3&quot;&gt;&lt;em&gt;Dependent variable:&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan=&quot;3&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td colspan=&quot;3&quot;&gt;DesigualdadGenero&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;(1)&lt;/td&gt;&lt;td&gt;(2)&lt;/td&gt;&lt;td&gt;(3)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;4&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;VozPolitica&lt;/td&gt;&lt;td&gt;-0.007&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;-0.005&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;-0.003&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;(0.001)&lt;/td&gt;&lt;td&gt;(0.001)&lt;/td&gt;&lt;td&gt;(0.001)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;LibertadMov&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;-0.008&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;-0.002&lt;sup&gt;**&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.002)&lt;/td&gt;&lt;td&gt;(0.001)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;CuentaF&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;-0.005&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.0004)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Constant&lt;/td&gt;&lt;td&gt;0.516&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.960&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.867&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;(0.039)&lt;/td&gt;&lt;td&gt;(0.094)&lt;/td&gt;&lt;td&gt;(0.057)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;4&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Observations&lt;/td&gt;&lt;td&gt;121&lt;/td&gt;&lt;td&gt;121&lt;/td&gt;&lt;td&gt;121&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.160&lt;/td&gt;&lt;td&gt;0.311&lt;/td&gt;&lt;td&gt;0.753&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Adjusted R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.153&lt;/td&gt;&lt;td&gt;0.299&lt;/td&gt;&lt;td&gt;0.747&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Residual Std. Error&lt;/td&gt;&lt;td&gt;0.186 (df = 119)&lt;/td&gt;&lt;td&gt;0.169 (df = 118)&lt;/td&gt;&lt;td&gt;0.101 (df = 117)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;F Statistic&lt;/td&gt;&lt;td&gt;22.685&lt;sup&gt;***&lt;/sup&gt; (df = 1; 119)&lt;/td&gt;&lt;td&gt;26.649&lt;sup&gt;***&lt;/sup&gt; (df = 2; 118)&lt;/td&gt;&lt;td&gt;118.950&lt;sup&gt;***&lt;/sup&gt; (df = 3; 117)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;4&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/td&gt;&lt;td colspan=&quot;3&quot; style=&quot;text-align:right&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;p&lt;0.1; &lt;sup&gt;**&lt;/sup&gt;p&lt;0.05; &lt;sup&gt;***&lt;/sup&gt;p&lt;0.01&lt;/td&gt;&lt;/tr&gt; ## &lt;/table&gt; Dependent variable: DesigualdadGenero (1)(2)(3) VozPolitica-0.007***-0.005***-0.003*** (0.001)(0.001)(0.001) LibertadMov-0.008***-0.002** (0.002)(0.001) CuentaF-0.005*** (0.0004) Constant0.516***0.960***0.867*** (0.039)(0.094)(0.057) Observations121121121 R20.1600.3110.753 Adjusted R20.1530.2990.747 Residual Std. Error0.186 (df = 119)0.169 (df = 118)0.101 (df = 117) F Statistic22.685*** (df = 1; 119)26.649*** (df = 2; 118)118.950*** (df = 3; 117) Note:*p**p***p Listo para incluir en tu trabajo final! 12.6 Ahora hazlo tú! 12.6.1 Ejercicio 1 Con la data de Desigualdad de Género realiza lo siguiente: Con una matriz de correlación identifica las cuatro variables con mayor correlación. Genera tres modelos con alguna combinación de estas cuatro variables. A tu elección. Presenta la tabla comparativa utilizando la librería stargazer. 12.6.2 Ejercicio 2 Ahora vamos a replicar lo que se realiza en la vida cotidiana en las técnicas de machine learning. data_original&lt;-import(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/DATA_internacional/desigualdad.xlsx&quot;) Crea una copia de la data original que NO contenga los casos de “Peru”, “Italy” y “Japan”. Llama a este subset TRAIN. Crea una copia de la data original que SÓLO contenga los casos de “Peru”, “Italy” y “Japan”. Llama a este subset TEST. Entrena al modelo tal como lo hemos realizado en la parte anterior con la data TRAIN (la data original menos “Peru”, “Italy” y “Japan”). Construye la ecuación, predice el Valor de la Desigualdad de Género en “Peru”, “Italy” y “Japan” y compáralo con el valor real. Se acercan? "],["regresión-lineal-múltiple-elementos-adicionales.html", "Sesión 13 Regresión lineal múltiple: elementos adicionales 13.1 Objetivo de la sesión 13.2 Identificar la variable que aporta más al modelo 13.3 Sobre variables categóricas en la RLM", " Sesión 13 Regresión lineal múltiple: elementos adicionales 13.1 Objetivo de la sesión El objetivo de esta clase es ampliar nuestro conocimiento sobre la regresión lineal múltiple al introducir dos componentes clave dentro del modelamiento: Identificar cuál es la variable que aporta más al modelo explicativo. Introducir variables variables categóricas como variables independientes en nuestros modelos. 13.2 Identificar la variable que aporta más al modelo Los coeficientes en un modelo de regresión lineal representan las relaciones y las contribuciones relativas entre la variable dependiente (Y) y las variables independientes (X). Estos coeficientes cuantifican el cambio promedio en la variable dependiente por cada unidad de cambio en la variable independiente correspondiente, manteniendo constantes todas las demás variables. En una regresión lineal simple, donde se tiene una única variable independiente, el coeficiente de regresión (también conocido como pendiente) indica la variación promedio en la variable dependiente por cada unidad de cambio en la variable independiente. Por ejemplo, si el coeficiente es 0.5, significa que, en promedio, la variable dependiente aumentará en 0.5 unidades por cada unidad de cambio en la variable independiente. En una regresión lineal múltiple, donde se tienen múltiples variables independientes, los coeficientes de regresión se interpretan de manera similar. Cada coeficiente representa el cambio promedio en la variable dependiente por cada unidad de cambio en la variable independiente correspondiente, manteniendo constantes todas las demás variables. Se puede considerar que los coeficientes reflejan la contribución individual de cada variable independiente en la variabilidad de la variable dependiente. Es importante tener en cuenta que los coeficientes no indican necesariamente una relación causal entre las variables, sino que reflejan una asociación estadística. Además, los coeficientes están influenciados por la escala de las variables, por lo que no se pueden comparar directamente si están en diferentes escalas de medición. 13.2.1 El problema de la unidad de medida Las escalas de medición diferentes pueden llevar a conclusiones erróneas al explicar el efecto de las variables independientes (X) sobre la variable dependiente (Y) en una regresión por varias razones. Aquí se presenta un ejemplo ilustrativo: Supongamos que se está realizando un estudio para investigar el impacto del nivel de ingresos (medido en dólares) y la edad (medida en años) en el gasto mensual en alimentos (medido en dólares) de los hogares. Se ajusta un modelo de regresión lineal múltiple y se obtienen los siguientes coeficientes estimados: Coeficiente de ingresos: 0.02 Coeficiente de edad: 10 Si se interpreta únicamente basándose en los coeficientes, se podría concluir que la edad tiene un efecto mucho más fuerte en el gasto mensual en alimentos que los ingresos. Sin embargo, esta conclusión sería errónea debido a las escalas de medición diferentes. El coeficiente de ingresos de 0.02 indica que, en promedio, por cada aumento de 1 unidad en los ingresos (en dólares), el gasto mensual en alimentos aumenta en 0.02 unidades (en dólares). Por otro lado, el coeficiente de edad de 10 indica que, en promedio, por cada aumento de 1 año en la edad, el gasto mensual en alimentos aumenta en 10 unidades (en dólares). La razón por la cual la conclusión sería errónea es que los ingresos y la edad no están en la misma escala de medición. Un aumento de 1 unidad en los ingresos (por ejemplo, de 1000 dólares a 1001 dólares) es mucho más pequeño en magnitud que un aumento de 1 año en la edad. Por lo tanto, comparar directamente los coeficientes sin considerar las escalas de medición puede llevar a una interpretación incorrecta. Para evitar este tipo de errores, es importante estandarizar o normalizar las variables para que estén en una escala comparable antes de realizar comparaciones directas. Además, es fundamental tener en cuenta el contexto y la interpretación sustantiva de las variables para comprender el efecto relativo de cada variable independiente sobre la variable dependiente en un modelo de regresión. 13.2.2 Solución: Uso de coeficientes estandarizados El paquete “lm.beta” en R es una herramienta útil para analizar la importancia relativa de las variables independientes en un modelo de regresión lineal. Proporciona una medida llamada “beta estandarizada” que permite comparar directamente los efectos relativos de las variables independientes en la variable dependiente, independientemente de sus escalas de medición. Estos valores de beta estandarizados representan la cantidad de desviación estándar en la variable dependiente asociada con un incremento de una desviación estándar en la variable independiente, manteniendo constantes las demás variables. Esto permite identificar las variables que tienen el mayor impacto relativo en el modelo, lo cual es especialmente útil cuando se trabaja con variables de diferentes escalas. Probemos: library(tidyverse) library(rio) data&lt;-import(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/trabajadores.sav&quot;) Hacemos un modelo de regresión múltiple: modelo1 &lt;- lm(salario_actual~salario_inicial + antiguedad, data=data) Si vemos los coeficientes no estandarizados vemos lo siguientes: modelo1$coefficients ## (Intercept) salario_inicial antiguedad ## -12120.8133 1.9138 172.2974 Sin embargo, esto nos puede llevar al error pues las variables están en escalas distintas: range(data$salario_inicial) ## [1] 9000 79980 range(data$antiguedad) ## [1] 63 98 Para ello, utilizamos los coeficientes estandarizados: library(lm.beta) lm.beta(modelo1) ## ## Call: ## lm(formula = salario_actual ~ salario_inicial + antiguedad, data = data) ## ## Standardized Coefficients:: ## (Intercept) salario_inicial antiguedad ## NA 0.8821228 0.1015173 Vemos que la variable salario inicial aporta mucho más que la variable antiguedad si ambas variables están estandarizadas. 13.2.3 Ejercicio Con la data utilizada la clase pasada: library(tidyverse) library(rio) data&lt;-import(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/DATA_internacional/desigualdad.xlsx&quot;) Cree un modelo para explicar la DesigualdadGenero que utilice las siguientes variables como predictoras: VozPolitica, LibertadMov, CuentaF. Cuál variable aporta más al modelo explicativo de la variable dependiente? 13.3 Sobre variables categóricas en la RLM Las variables categóricas, también conocidas como variables cualitativas o nominales, son aquellas que representan características no numéricas, como género, estado civil, nivel de educación, ubicación geográfica, entre otros. Estas variables son de gran importancia en el análisis de datos en ciencias sociales, ya que nos permiten examinar el impacto de factores que no pueden expresarse en términos de magnitudes o cantidades. A diferencia de las variables numéricas que hemos utilizado anteriormente, las variables categóricas requieren un enfoque distinto en el análisis estadístico. No podemos simplemente ingresarlas como están en nuestro modelo de regresión lineal múltiple, ya que los algoritmos de regresión se basan en operaciones matemáticas que no pueden aplicarse directamente a variables no numéricas. Para eso, es necesario entender qué es una variable dummy. 13.3.1 Variables dummy (1-0): Definición Una variable dummy, también conocida como variable ficticia, es una variable binaria que se utiliza para representar una característica cualitativa o categórica en un análisis estadístico. En lugar de tomar valores continuos, una variable dummy toma solo dos valores posibles, generalmente codificados como 0 o 1. Estos valores indican la presencia o ausencia de una determinada categoría o condición. En el contexto de la regresión lineal, las variables dummy se utilizan para incorporar características categóricas en el modelo. Se crea una variable dummy para cada categoría o condición distinta de la variable categórica que se está analizando. Se asigna el valor 1 a la variable dummy correspondiente a la categoría de interés y se asigna el valor 0 a las demás variables dummy y a la categoría de referencia. Ejemplos: Variable dummy de género: En un estudio sociológico sobre la participación política, se puede utilizar una variable dummy para representar el género de los individuos. Se podría asignar el valor de 1 para la categoría “Mujer” y el valor de 0 para la categoría “Hombre”. De esta manera, la variable dummy de género permite analizar cómo el género influye en la participación política, al incluirlo como una variable independiente en un modelo de regresión lineal. Variable dummy de nivel educativo: En un análisis económico sobre los salarios, se puede utilizar una variable dummy para representar el nivel educativo de los trabajadores. Por ejemplo, se podría crear una variable dummy para la categoría “Título universitario” y asignarle el valor de 1 si un individuo tiene un título universitario, y el valor de 0 si no tiene dicho título. Esto permitiría examinar cómo el nivel educativo afecta los salarios, al incluir la variable dummy de nivel educativo como una variable independiente en el modelo de regresión lineal. En ambos ejemplos, las variables dummy permiten capturar características categóricas relevantes y representarlas de manera numérica en los modelos de regresión lineal. Esto facilita la interpretación del impacto de estas características en la variable dependiente y proporciona una forma efectiva de analizar cómo los factores cualitativos influyen en los fenómenos estudiados. 13.3.2 Variables dummy (1-0): Utilidad en la RLM Representación de variables categóricas: Las variables dummy proporcionan una forma de representar variables categóricas en modelos de regresión. Mientras que las variables categóricas no pueden ser introducidas directamente en el modelo debido a la naturaleza no numérica, las variables dummy asignan valores numéricos (por ejemplo, 0 y 1) para indicar la presencia o ausencia de una categoría particular. Esto permite que las características cualitativas se incluyan en el análisis y se examinen sus efectos en la variable dependiente. Comparación relativa de categorías: Al crear variables dummy, se establece una categoría de referencia, generalmente la más común o la que se considera la base de comparación. Las variables dummy para las otras categorías se comparan con la categoría de referencia. Los coeficientes estimados asociados con las variables dummy reflejan las diferencias en el efecto de cada categoría en comparación con la categoría de referencia. Esto permite interpretar y cuantificar el impacto relativo de cada categoría sobre la variable dependiente. Así, las variables dummy permiten realizar comparaciones entre grupos categóricos y analizar cómo influyen en el fenómeno de estudio. Control de confusión y variables omitidas: Al incluir variables dummy en el modelo de regresión, se evita el problema de omitir variables relevantes y se controla la influencia de las características categóricas en el efecto de otras variables independientes. Si no se utilizan variables dummy y se omiten las variables categóricas, se corre el riesgo de obtener coeficientes sesgados y resultados incorrectos. Al incluir variables dummy en el modelo, se captura el efecto específico de cada categoría y se evitan suposiciones erróneas sobre las relaciones entre las variables. Análisis de interacciones y modificadores de efecto: Las variables dummy también pueden ser utilizadas en el análisis de interacciones y efectos modificadores en los modelos de regresión. Al combinar una variable dummy con otra variable independiente, se puede examinar cómo el efecto de una variable varía según las diferentes categorías de la variable dummy. Esto permite comprender cómo ciertos factores pueden influir de manera diferente en grupos categóricos específicos, lo cual es fundamental para el análisis diferencial en ciencias sociales. Las variables dummy desempeñan un papel fundamental en los modelos de regresión al permitir la inclusión de variables categóricas en un marco analítico basado en regresión lineal. Proporcionan una representación numérica de características cualitativas, permiten la comparación relativa de categorías, controlan variables omitidas y facilitan el análisis de interacciones y efectos modificadores. Su uso adecuado y su interpretación correcta son esenciales para comprender cómo las características categóricas influyen en los fenómenos estudiados en las ciencias sociales 13.3.3 Ejercicio data&lt;-import(&quot;https://github.com/ChristianChiroqueR/banco_de_datos/raw/main/trabajadores.sav&quot;) names(data) ## [1] &quot;id&quot; &quot;sexo&quot; &quot;fechnac&quot; &quot;educ&quot; &quot;catlab&quot; &quot;salario_actual&quot; ## [7] &quot;salario_inicial&quot; &quot;antiguedad&quot; &quot;experiencia&quot; &quot;minoría&quot; &quot;directivo&quot; Ahora realizaremos unos modelos que tengan como variable explicativa la categoría laboral. str(data$catlab) ## num [1:474] 3 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;label&quot;)= chr &quot;Categoría laboral&quot; ## - attr(*, &quot;format.spss&quot;)= chr &quot;F1.0&quot; ## - attr(*, &quot;labels&quot;)= Named num [1:4] 0 1 2 3 ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;0 (Ausente)&quot; &quot;Administrativo&quot; &quot;Seguridad&quot; &quot;Directivo&quot; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
